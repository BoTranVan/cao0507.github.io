<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Vultr CentOS搭建shadowsocks服务端并开启BBR加速]]></title>
    <url>%2F2018%2F08%2F21%2FVPS%E6%90%AD%E5%BB%BAshadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B9%B6%E5%BC%80%E5%90%AFBBR%E5%8A%A0%E9%80%9F%2F</url>
    <content type="text"><![CDATA[前言最近教研室很多同学来问有没有公用的 VPN，教研室以前有师兄去买过一些 VPN，但现在师兄也毕业了就用不了了。为了同学们的方便，作为网管应该尽力满足大家日常查阅资料的需求，于是向老师申请了经费去购买了一台 VPS 来搭建教研室公用的 VPN。 购买 VPS我这里购买的是 vultr 的 VPS，在新加坡的节点，每个月5美元，其实是按小时计费，每小时0.007美元，如果出问题了可以方便的停止购买，不像其他厂商按年一次性付费的话，万一不能用了就很亏。操作系统选择 CentOS 7 64位。 安装过程安装 Shadowsocks 服务shadowsocks有很多版本，如Python，node.js，libev，Go等，Python版本用的人是最多的，但很久没有更新了，这里选择 Go 版本的shadowsocks。 在安装shadowsocks之前需要先安装 Go 语言的环境： 从官网下载 Linux 平台的源码包 1# wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz 解压到指定目录 1# sudo tar zxvf go1.10.3.linux-amd64.tar.gz -C /usr/local/ 配置环境变量 123# vi .bashrc添加export PATH=$PATH:/usr/local/go/bin 保存并使生效： 1# source .bashrc 安装 shadowsocks，使用一键安装脚本（https://github.com/iMeiji/shadowsocks_install） 123# wget --no-check-certificate https://raw.githubusercontent.com/iMeiji/shadowsocks_install/master/shadowsocks-go.sh# chmod +x shadowsocks-go.sh# ./shadowsocks-go.sh 2&gt;&amp;1 | tee shadowsocks-go.log 卸载 shadowsocks 方法： 1# ./shadowsocks-go.sh uninstall shadowsocks 常用命令： 启动：/etc/init.d/shadowsocks start 停止：/etc/init.d/shadowsocks stop 重启：/etc/init.d/shadowsocks restart 状态：/etc/init.d/shadowsocks status 执行完前面的安装脚本后，查看 shadowsocks 的运行状态： 12# /etc/init.d/shadowsocks statusshadowsocks-go running with PID 1629 能看到进程 ID 说明 shadowsocks 服务已经在运行了。 配置 shadowsocks 开机自启动： 123# vi /etc/rc.local添加/etc/init.d/shadowsocks restart 这样在系统重启后就可以自动加载 shadowsocks 服务了。 配置防火墙： 检查防火墙是否允许你设定的端口进行通信 1# iptables -vnL | grep 8989 如果没有信息的话，就是防火墙不允许该端口进行通信。 需设置： 12# firewall-cmd --zone=public --add-port=8989/tcp --permanent# firewall-cmd --reload 由于 CentOS 7 默认安装的是 firewalld，并没有安装 iptables-services，不能使用 iptables-save 来保存iptables 规则并在下次启动时自动加载，所以上面使用是 firewalld 来配置永久规则，这样在关机重启后规则也不会消失。 到这里，shadowsocks 服务器就基本上已经配置好了，你可以使用客户端来上外网了。但这时候的网络连接的速度可能只能够保证查查网页，如果要下载或者看 YouTube 速度很慢，所以后将进行配置TCP 加速。 TCP 加速在后面会升级系统内核，所以先查看一下服务器的内核版本： 12# uname -aLinux vultr.guest 3.10.0-862.3.2.el7.x86_64 #1 SMP Mon May 21 23:36:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 加速有锐速加速和 Google BBR 加速，这里使用 BBR 加速。TCP BBR是谷歌出品的 TCP 拥塞控制算法，目的是要尽量跑满带宽，并且尽量不要有排队的情况。BBR 可以起到单边加速 TCP 连接的效果。 Google提交到Linux主线并发表在ACM queue期刊上的TCP-BBR拥塞控制算法。继承了Google“先在生产环境上部署，再开源和发论文”的研究传统。TCP-BBR已经再YouTube服务器和Google跨数据中心的内部广域网(B4)上部署。由此可见出该算法的前途。 TCP-BBR的目标就是最大化利用网络上瓶颈链路的带宽。一条网络链路就像一条水管，要想最大化利用这条水管，最好的办法就是给这跟水管灌满水。 BBR解决了两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟，高带宽的网络链路。 降低网络链路上的buffer占用率，从而降低延迟。非常适合慢速接入网络的用户。 Google 在 2016年9月份开源了他们的优化网络拥堵算法BBR，最新版本的 Linux内核(4.9-rc8)中已经集成了该算法。 一键安装脚本： 123# wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh# chmod +x bbr.sh# ./bbr.sh 安装完成后会提示重启，重启完成后，查看内核： 12# uname -r4.18.3-1.el7.elrepo.x86_64 高于 4.9 就可以了 检查是否开启 BBR： 123456789101112# sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = reno cubic bbr# sysctl net.ipv4.tcp_congestion_controlnet.ipv4.tcp_congestion_control = bbr# sysctl net.core.default_qdiscnet.core.default_qdisc = fq# lsmod | grep bbrtcp_bbr 20480 7#返回值有 tcp_bbr 则说明已经启动 完成以上步骤，则 TCP 加速也已经配置好了，接下来就可以体验飞快的下载速度以及 1080p 的高清视屏啦！ shadowsocks 客户端下载连接：Shadowsocks - Clients]]></content>
      <categories>
        <category>VPN</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>shadowsocks</tag>
        <tag>BBR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟机磁盘扩容]]></title>
    <url>%2F2018%2F07%2F24%2FKVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[概述在 KVM 环境中，通常通过创建一个各种格式的磁盘来安装虚拟机，可能创建时你觉得大小够用，但是可能用着用着到了某一天你发现你的磁盘容量不够用了，很多程序都打不开，你可能会很慌，其实并不需要慌，因为有很多方法可以给你的虚拟机扩容，哈哈哈，如： 通过virsh attach-disk添加一个新的磁盘 通过virsh attach-device添加一个新的存储设备 直接给原来用的磁盘扩容 上面的三种方法都能实现给你的虚拟机扩容，但是本文想介绍的是第三种方法，直接给安装了虚拟机的磁盘来扩容，这样的好处是在你的主机上看来一个虚拟机就是一个磁盘，管理方便，而且只通过这一个磁盘来分享迁移你的虚拟机，也是更方便。 #创建虚拟机 创建磁盘 1$ qemu-img create -f qcow2 centos.qcow2 50G 这里创建了一个50G的磁盘，格式为qcow2，这种格式的特点是分配给虚拟机的实际使用的磁盘的大小是动态增长的，并不是一下子把所有的空间都给配给虚拟机，可以通过命令查看磁盘信息： 1234567891011$ qemu-img info centos.qcow2image: centos.qcow2file format: qcow2virtual size: 50G (53687091200 bytes)disk size: 1.2Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false 可以看到50G的磁盘目前的实际大小只有1.2个G。 安装虚拟机 可以通过 virt-manager 通过图形界面安装，但是我一般都是通过远程连接服务器的，所以常用命令行安装虚拟机，命令如下： 123456$ virt-install --virt-type kvm --name test-centos --ram 2048 --vcpus=1 \--cdrom CentOS-7-x86_64-Minimal-1804.iso \--disk path=test-centos.qcow2,format=qcow2 \--network network=default,model=virtio \--graphics vnc,listen=0.0.0.0 --noautoconsole \--os-type=linux 系统安装的过程就不介绍了，但在安装系统的过程中，对于磁盘分区部分一般分两个区，根目录 / 和 交换分区 swap 。 查看虚拟机磁盘 查看磁盘空间信息 123456789$ df -lhFilesystem Size Used Avail Use% Mounted on/dev/sda2 46G 1.1G 45G 3% /devtmpfs 909M 0 909M 0% /devtmpfs 920M 0 920M 0% /dev/shmtmpfs 920M 8.5M 911M 1% /runtmpfs 920M 0 920M 0% /sys/fs/cgrouptmpfs 184M 0 184M 0% /run/user/1000tmpfs 184M 0 184M 0% /run/user/0 可以看到 /dev/sda2 被挂载到了系统根目录 / 查看系统磁盘分区 1234567891011# fdisk -lDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux 我这里系统磁盘被成了两个区：/dev/sda1 和 /dev/sda2，就是我们前面说的一个用于交换分区，另一个用于挂载到根目录。 注：磁盘分区信息中可以看到原来我们 50个G 的磁盘到这里怎么变成 53.7GB 了呢？其实只是单位不一样而已，前面我们用的是 GiB 为单位，这两个单位的计算方式不同，1 GiB = 1024*1024*1024 Bytes，而 1GB = 1000*1000*1000 Bytes 。 到这里，假如我们发现磁盘空间不够用了，接下来就来看看如何给虚拟机的磁盘扩容吧。 磁盘扩容 首先将我们的虚拟机关机，然后进行磁盘扩容，在服务器主机上操作： 12# qemu-img resize centos.qcow2 60GImage resized. 上面我们把磁盘大小扩成了 60G 重启虚拟机，在虚拟机内，使用 fdisk 指令对磁盘进行分区： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# fdisk /dev/sda #我们的整块磁盘是sda，对整个磁盘进行分区Command (m for help): pDisk /dev/sda: 64.4 GB, 64424509440 bytes, 125829120 sectors #磁盘大小变成64.4GB了，扩容成功Units = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux #结束sector号小于磁盘的总数Command (m for help): nPartition type: p primary (2 primary, 0 extended, 2 free) e extendedSelect (default p): e #添加扩展分区Partition number (3,4, default 3): 3 #分区号，填写默认的3First sector (104857600-125829119, default 104857600): #直接回车选择默认Using default value 104857600Last sector, +sectors or +size&#123;K,M,G&#125; (104857600-125829119, default 125829119): #回车选择默认 Using default value 125829119Partition 3 of type Extended and of size 10 GiB is setCommand (m for help): p #再次查看分区表Disk /dev/sda: 64.4 GB, 64424509440 bytes, 125829120 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux/dev/sda3 104857600 125829119 10485760 5 Extended #新添加的扩展分区Command (m for help): n #再添加逻辑分区，因为扩展分区是不能格式化后挂载的Partition type: p primary (2 primary, 1 extended, 1 free) l logical (numbered from 5)Select (default p): l #逻辑分区Adding logical partition 5First sector (104859648-125829119, default 104859648): #回车选择默认Using default value 104859648Last sector, +sectors or +size&#123;K,M,G&#125; (104859648-125829119, default 125829119): #回车选择默认Using default value 125829119Partition 5 of type Linux and of size 10 GiB is setCommand (m for help): pDisk /dev/sda: 64.4 GB, 64424509440 bytes, 125829120 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux/dev/sda3 104857600 125829119 10485760 5 Extended/dev/sda5 104859648 125829119 10484736 83 Linux #新添加的逻辑分区Command (m for help): w #保存分区配置，并退出 The partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks. #需要重启或者重新获取分区表# 重新获取分区表： 1# partprobe 格式化分区： 可以先查看一下其他分区是什么格式的文件系统： 12# mount |grep /dev/sda/dev/sda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) /dev/sda2 是 xfs 格式的，CentOS 7默认文件系统格式是xfs，在CentOS 6以及之前的版本，使用的是ext文件系统格式，CentOS 6是ext4格式、CentOS 5是ext3格式。那我们也把 /dev/sda5 格式化成一样格式的： 12345678910# mkfs -t xfs /dev/sda5 #-t 指定格式 meta-data=/dev/sda5 isize=512 agcount=4, agsize=655296 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0data = bsize=4096 blocks=2621184, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 挂载磁盘： 1# mount /dev/sda5 /mnt/newdisk 将 /dev/sda5 挂载到 /mnt/newdisk，需要注意的是，newdisk 最好是一个空文件夹，不然挂载之后文件夹内的文件将会暂时隐藏，直到你卸载到磁盘后才能恢复。 查看磁盘空间信息： 123456789# df -lhFilesystem Size Used Avail Use% Mounted on/dev/sda2 46G 1.1G 45G 3% /devtmpfs 909M 0 909M 0% /devtmpfs 920M 0 920M 0% /dev/shmtmpfs 920M 8.5M 911M 1% /runtmpfs 920M 0 920M 0% /sys/fs/cgrouptmpfs 184M 0 184M 0% /run/user/1000/dev/sda5 10G 33M 10G 1% /mnt/newdisk 到这里就基本上结束了，你的磁盘又多了 10个G 的容量，可以正常读写磁盘啦。 命令附录这里先介绍几个会用到的关于Linux磁盘操作指令： df 指令：磁盘信息命令 语法： 1$ df [OPTION]... [FILE]... 功能说明： 使用 df 指令查看磁盘空间的信息。指令可以查看指定文件系统的占用情况。如果指令中未指定文件名，将显示当前所有挂载的文件系统的可用空间。 参数说明： | 参数 | 说明 || :—————–: | :—————————————: || -a | 显示包括0区块在内的所有文件系统的情况 || -h | 以可读性较高的方式显示信息 || -H | 相当于“-h”但在计算时，1K=1000，而不是1024 || -i | 显示 inode 节点信息 || -k | 区块大小为1024字节 || -l | 尽显示本地文件系统 || –no-sync | 取得磁盘信息前，忽略 sync 指令 || -P | 输出 POSIX 格式 || –sync | 在取得磁盘信息前，先执行 sync 指令 || -T | 显示文件系统类型 || –block-size= | 指定区块大小 || -t filesystem-type | 只显示选定文件系统的磁盘信息 || -x filesystem-type | 不显示选定文件系统的磁盘信息 || –help | 帮助信息 || –version | 版本信息 | 常用命令：$ df -lh fdisk 指令：Linux磁盘分区命令 语法： 123fdisk [options] &lt;disk&gt; change partition tablefdisk [options] -l &lt;disk&gt; list partition table(s)fdisk -s &lt;partition&gt; give partition size(s) in blocks 功能说明： fdisk 指令是Linux下管理分区的程序。应用该程序不仅可以创建磁盘分区，还可以对磁盘进行维护，改变分区类型。 参数说明： | 参数 | 说明 || :———: | :—————————-: || -b | 指定各分区大小 || -l | 列出分区表情况 || -s | 输出指定分区大小到标准输出设备 || -u | 与“-l”参数搭配，显示分区数目 || -v | 版本信息 | 【fdisk 程序指令】 a：设置/删除可引导分区标记 d：删除指定分区 l：列出分区类型 m：显示 fdisk 程序指令 n：新建分区 p：列出当前分区信息 q：退出 fdisk 分区，对更改不保存 t：改变分区 ID v：检测当前分区信息 w：退出 fdisk 分区，保存更改 注：不同的版本略差别 mkfs 指令：建立各种文件系统 语法： 1# mkfs [options] [-t &lt;type&gt;] [fs-options] &lt;device&gt; [&lt;size&gt;] 功能说明： mkfs 指令可用来在指定的设备上建立各种文件系统，它通过调用相关的程序来执行文件系统的构建，本身并不执行系统构建。 参数说明： | 参数 | 说明 || :——————: | :—————————————————-: || -c | 在创建文件系统之前，检查是否有损坏的区块 || fs | 指定建立文件系统时的参数，针对不同的文件系统的参数不同 || -t | 指定要创建的文件系统的类型，默认为ext2 || -v | 显示详细的处理信息 || -V | 版本信息 | 常用命令 1# mkfs -t ext3 /dev/sda3 mount 指令：挂载文件系统 语法： 1# mount [options] &lt;source&gt; &lt;directory&gt; 功能说明： 使用 mount 指令可将指定设备挂载到已存在的目录。当文件系统挂载完成后，用户可通过该目录进行操作，来实现对指定设备的文件读写等操作。 必要参数说明： | 参数 | 说明 || :———: | :————–: || | 指定要挂载的设备 || | 指定要挂载的目录 | 常用示例： 1# mount /dev/sda3 /mnt]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>磁盘扩容</tag>
        <tag>df</tag>
        <tag>fdisk</tag>
        <tag>mkfs</tag>
        <tag>mount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Linux Shell新的收获]]></title>
    <url>%2F2018%2F07%2F21%2F%E5%85%B3%E4%BA%8EShell%E6%96%B0%E7%9A%84%E6%94%B6%E8%8E%B7%2F</url>
    <content type="text"><![CDATA[最近又翻阅了几本Linux相关的工具书，在这里记录一些关于Linux shell的新的小小的收获。 查看正在使用的shell有很多方法可以查看当前正在使用的shell： 使用特殊参数$0 12$ echo $0-bash 对于这里有点疑问的是有资料说在本地系统执行可能的结果会是/bin/bash，但是我尝试了一下发现结果还是bash或者-bash。不知道资料中说的本地系统指的是什么意思。 输入不存在的命令： 12$ asdf-bash: asdf: command not found 从shell提示中可以看出当前正在使用的shell是bash。 查看用户登录默认的shell 通过查看/etc/passwd文件找到用户对应的那一行，最后一列就是用户登录的shell 12user1@ubuntu:~$ cat /etc/passwd | grep user1user1:x:1000:1000:user1,,,:/home/user1:/bin/bash 最常用的方法： 12$ echo $SHELL/bin/bash 从环境变量中查看： 12$ env |grep SHELLSHELL=/bin/bash 切换用户正在使用的shell直接输入对应shell的命令即可切换： 12345$ echo $0-bash &lt;==当前正在使用的shell$ zsh &lt;==运行zsh$ echo $0zsh &lt;==当前正在使用的shell已经变成zsh 修改用户的登录shell 首先可以查看用户合法的shell： 123456789$ chsh -l/bin/sh/bin/bash/sbin/nologin &lt;==合法不可登录的shell/bin/tcsh/bin/csh/bin/mksh/bin/ksh/bin/zsh chsh -l命令并不总是可用，对于某些Linux发行版，chsh命令没有-l的选项，如ubuntu16.04，但是可以直接查看/etc/shells文件 123456789$ cat /etc/shells/bin/sh/bin/bash/sbin/nologin &lt;==合法不可登录的shell/bin/tcsh/bin/csh/bin/mksh/bin/ksh/bin/zsh 修改用户登录shell 123456$ chsh -s /bin/zshPassword:$ cat /etc/passwd | grep user1user1:x:1000:1000:user1,,,:/home/user1:/bin/zsh &lt;== /etc/passwd文件中已经发生变化$ echo $SHELL/bin/bash &lt;==但是再次查看$SHELL，发现并没有变，其实退出shell再次登录就会看到变化了]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
        <tag>chsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化脚本与程序实现]]></title>
    <url>%2F2018%2F07%2F09%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC%E4%B8%8E%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[概述最近的项目中有远程登录虚拟机并执行相关命令的需求，所以尝试了远程免密码登录虚拟机（在另一篇博客中有介绍链接)。但是发现这还不够，因为登录远程虚拟机后执行的脚本可能会有需要交互的操作，比如sudo命令需要输入密码，所以就想实现一个完全自动化的脚本，包括登录时的密码自动输入以及登录后执行命令的自动交互。查阅相关资料后学习到，在Linux中可以用expect来实现自动化的交互，且在python中也有相应的一个模块pexpect具有此功能。本文将先介绍shell自动化交互脚本的实现，然后介绍其python程序实现。 shell脚本Expect介绍 简要介绍： Expect 是由Don Libes基于Tcl（Tool Command Language）语言开发的，主要应用于自动化交互式操作的场景，借助expect处理交互的命令，可以将交互过程如：ssh登录，ftp登录等写在一个脚本上，使之自动化完成。尤其适用于需要对多台服务器执行相同操作的环境中，可以大大提高系统管理人员的工作效率 。 主要命令： spawn：启动新的进程 spawn命令会fork一个子进程去执行command命令，然后在此子进程中执行后面的命令；spawn后的send和expect命令都是和spawn打开的进程进行交互的。如果没有spawn语句，整个Expect就无法进行下去，当然，如果真的不要spawn过程也没有关系，虽然这样就没有办法单独执行，但是这个脚本可以与任何调用它的进程进行交互。 使用方法：ssh自动登录脚本中，通过spawn ssh user_name@ip_addr，fork一个子进程执行ssh登录命令；连接远程ftp服务器，spawn ftp ftp.server.com。 expect：从进程接收字符串 expect命令是Expect解释器的关键命令，它的一般用法为 expect “string”，即期望获取到string字符串，可在在string字符串里使用 * 等通配符。 使用方法：在执行spawn命令ssh登录时，子进程会要求输入密码，因此可以使用expect命令检查子进程中的输出中是否包含“password”子字符串，命令为expect &quot;password&quot;。 send：用于向进程发送字符串 send命令的一般用法为 send “string”，它们会我们平常输入命令一样向命令行输入一条信息，当然不要忘了在string后面添加上 \r 表示输入回车 。 使用方法：在使用expect命令接收到字符串“password”后，就需要使用send命令来发送“PASSWORD”。 interact：允许用户交互 interact命令很简单，执行到此命令时，脚本fork的子进程会将操作权交给用户，允许用户与当前shell进行交互，让人在适当的时候干预这个过程了 。 使用方法：直接在脚本适当的位置加入一行interact。 安装方法： $ sudo apt-get install expect 脚本编写我需要实现一个脚本，其功能是ssh登录虚拟机，并在远程虚拟机用户目录下记录远程登录的日志文件，然后修改其iptables规则，禁止转发tcp 22号端口的报文。 ssh登录 12345spawn ssh $user@$ipexpect &#123; &quot;(yes/no)&quot; &#123;send &quot;yes\r&quot;; exp_continue&#125; &quot;password:&quot; &#123;send &quot;$password\r&quot;&#125;&#125; expect中可能会接收到两种字符串，”(yes/no)”表示你的主机还未登录过远程虚拟机，即你的用户目录下的文件~/.ssh/know_hosts中还未记录该远程虚拟机，问你是否需要将其添加到know_hosts中，回复”yes”，下次再登录就不会出现这个提醒了；然后就会收到&quot;password:&quot;，这时就需要将密码发送过去，这样就已经登录到远程虚拟机。 记录日志文件 1expect &quot;*$&quot; &#123;send &quot;echo &apos;login +1&apos; &gt;&gt; ~/remote_login.log\r&quot;&#125; 登录之后会收到”$”或”#”的命令行提示符，然后就可以发送需要执行的命令了。 修改iptables规则 12expect &quot;*$&quot; &#123;send &quot;sudo iptables -A FORWARD -p tcp --dport 22 -j REJECT\r&quot;&#125;expect &quot;password&quot; &#123;send &quot;$password\r&quot;&#125; 退出子程序 1expect &quot;*$&quot; &#123;send exit\r&#125; 完整的脚本： 12345678910111213141516171819#!/usr/bin/expectset ip 192.168.1.75set user openstackset passwd 123456set timeout 5spawn ssh $user@$ipexpect &#123; &quot;(yes/no)&quot; &#123;send &quot;yes\r&quot;; exp_continue&#125; &quot;password:&quot; &#123;send &quot;$password\r&quot;&#125;&#125;expect &quot;*$&quot; &#123;send &quot;echo &apos;login +1&apos; &gt;&gt; ~/remote_login.log\r&quot;&#125;;expect &quot;*$&quot; &#123;send &quot;sudo iptables -A FORWARD -p tcp --dport 22 -j REJECT\r&quot;&#125;;expect &quot;password&quot; &#123;send &quot;$passwd\r&quot;&#125;;expect &quot;*$&quot; &#123;send exit\r&#125;;expect eof; python程序Pexpect介绍Pexpect 是Expect 的一个 Python 实现，是一个用来启动子程序，并使用正则表达式对程序输出做出特定响应，以此实现与其自动交互的 Python 模块。 Pexpect 的使用范围很广，可以用来实现与 ssh、ftp 、telnet 等程序的自动交互；可以用来自动复制软件安装包并在不同机器自动安装；还可以用来实现软件测试中与命令行交互的自动化。 其依赖 pty module ，所以 Pexpect 还不能在 Windows 的标准 python 环境中执行，如果想在 Windows 平台使用，可以使用在 Windows 中运行 Cygwin 做为替代方案。 pexpect主要包含两个接口，一个是run()函数，另一个是spawn类。spawn类的功能很强大，run()函数要更简单，更适用于快速调用程序。 spawn class 使用这个类来开始和控制子程序。 spawn的构造函数 123class spawn: def __init__(self,command,args=[],timeout=30,maxread=2000,\ searchwindowsize=None, logfile=None, cwd=None, env=None) spawn是Pexpect模块主要的类，用以实现启动子程序，它有丰富的方法与子程序交互从而实现用户对子程序的控制。它主要使用 pty.fork() 生成子进程，并调用 exec() 系列函数执行 command 参数的内容。 使用示例： 12child = pexpect.spawn(&apos;/usr/bin/ftp&apos;)child = pexpect.spawn(&apos;/usr/bin/ssh user@example.com&apos;) 由于需要实现不断匹配子程序输出， searchwindowsize 指定了从输入缓冲区中进行模式匹配的位置，默认从开始匹配。 使用pexpect控制子程序 expect()定义 expect(self, pattern, timeout=-1, searchwindowsize=None) 在参数中： pattern 可以是正则表达式， pexpect.EOF ， pexpect.TIMEOUT ，或者由这些元素组成的列表。需要注意的是，当 pattern 的类型是一个列表时，且子程序输出结果中不止一个被匹配成功，则匹配返回的结果是缓冲区中最先出现的那个元素，或者是列表中最左边的元素。使用 timeout 可以指定等待结果的超时时间 ，该时间以秒为单位。当超过预订时间时， expect 匹配到pexpect.TIMEOUT。 expect 不断从读入缓冲区中匹配目标正则表达式，当匹配结束时 pexpect 的 before 成员中保存了缓冲区中匹配成功处之前的内容， pexpect 的 after 成员保存的是缓冲区中与目标正则表达式相匹配的内容。 12print child.beforeprint child.after send系列函数 123send(self, s) sendline(self, s=&apos;&apos;) sendcontrol(self, char) 这些方法用来向子程序发送命令，模拟输入命令的行为。 与 send() 不同的是 sendline() 会额外输入一个回车符 ，更加适合用来模拟对子程序进行输入命令的操作。 当需要模拟发送 “Ctrl+c” 的行为时，还可以使用 sendcontrol() 发送控制字符。 child.sendcontrol(&#39;c&#39;) 由于 send() 系列函数向子程序发送的命令会在终端显示，所以也会在子程序的输入缓冲区中出现，因此不建议使用 expect 匹配最近一次 sendline() 中包含的字符。否则可能会在造成不希望的匹配结果。 interact()定义 interact(self, escape_character = chr(29), input_filter = None, output_filter = None) Pexpect还可以调用interact() 让出控制权，用户可以继续当前的会话控制子程序。用户可以敲入特定的退出字符跳出，其默认值为“^]” 。 run() function run()的定义 12run(command,timeout=-1,withexitstatus=False,events=None,\ extra_args=None,logfile=None, cwd=None, env=None) 函数 run 可以用来运行命令，其作用与 Python os 模块中 system() 函数相似。run() 是通过 Pexpect spawn类实现的。 使用run()执行命令svn命令 12from pexpect import *run (&quot;svn ci -m &apos;automatic commit&apos; my_file.py&quot;) 与 os.system() 不同的是，使用 run() 可以方便地同时获得命令的输出结果与命令的退出状态 。 run()的返回值 12from pexpect import *(command_output, exitstatus) = run (&apos;ls -l /bin&apos;, withexitstatus=1) command_out 中保存的就是 /bin 目录下的内容。 更多关于pexpect的内容请看pexpect. 安装python pexpect模块 sudo pip install pexpect 程序编写还是一样的实现一个程序，其功能是ssh登录虚拟机，并在远程虚拟机用户目录下记录远程登录的日志文件，然后修改其iptables规则，禁止转发tcp 22号端口的报文。 完整程序： 12345678910111213141516171819202122232425262728import pexpectip = &quot;192.168.1.75&quot;user = &quot;chl&quot;passwd = &quot;123456&quot; ssh_newkey = &quot;Y|yes/no&quot;child = pexpect.spawn(&apos;ssh %s@%s&apos; % (user, ip))index = child.expect([pexpect.EOF, pexpect.TIMEOUT, ssh_newkey, &quot;password:&quot;])if index == 1: print &quot;TimeoutError!&quot;if index == 2: child.sendline(&quot;yes&quot;) child.expect(&quot;password&quot;) child.sendline(passwd)if index == 3: child.sendline(passwd)child.expect(&quot;chl@&quot;)child.sendline(&quot;echo &apos;login +1&apos; &gt;&gt; ~/remote_login.log&quot;)child.expect(&quot;chl@&quot;)child.sendline(&quot;sudo iptables -A FORWARD -p tcp --dport 22 -j REJECT&quot;)child.expect(&quot;password&quot;)child.sendline(passwd)child.expect(&quot;chl@&quot;)child.sendline(&quot;exit&quot;) 注：需要注意的是python中正则表达式与Linux中的通配符是有区别的，不能直接用通配符来编写python正则表达式。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>python</tag>
        <tag>Expect</tag>
        <tag>pexpect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程免密码登录Openstack实例]]></title>
    <url>%2F2018%2F07%2F07%2F%E8%BF%9C%E7%A8%8B%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95Openstack%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[背景一般情况下，可以通过Openstack Dashboard的控制台来访问用户创建的实例Instance，对于管理员来说，通过这种方法来访问会觉得很不方便，因为每次都需要打开浏览器来输入网址，每次点击都需要等待响应，登录到实例后控制台的响应也不是很及时且有卡顿。因此，本文介绍如何通过命名空间来实现免密码登录Openstack实例。 命名空间在Linux中，网络命名空间可以被认为是隔离的拥有单独网络栈（网卡、路由转发表、iptables）的环境。网络命名空间经常用来隔离网络设备和服务，只有拥有同样网络命名空间的设备，才能看到彼此。openstack中就采用命名空间来实现不同网络的隔离。 使用ip netns来查看已经存在的命名空间： 123$ ip netnsqrouter-e94975e8-4688-4858-8f64-86a18eea81edqdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 qdhcp开头的名字空间是dhcp服务器使用的，qrouter开头的则是router服务使用的。 查看openstack的网络： 1234567$ openstack network list+--------------------------------------+---------+--------------------------------------+| ID | Name | Subnets |+--------------------------------------+---------+--------------------------------------+| 34ba192f-ceea-4c86-addc-a5d14c6a34a8 | private | 74dbc6f4-ae59-4af5-b941-1f4d04918607 || fc9b502a-d472-46f5-8570-b0d3915759cf | public | de903618-fb31-412a-b46d-6ab593985b03 |+--------------------------------------+---------+--------------------------------------+ 可以看到private网络的dhcp服务器对应的命名空间qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8的名字中包含了private网络的ID。而本次测试的远程实例就是创建在private网络下的。 通过 ip netns exec namespace_id command 来在指定的网络名字空间中执行网络命令，记得加上sudo权限，例如 123456789101112131415161718$ sudo ip netns exec qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 ifconfiglo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:190 errors:0 dropped:0 overruns:0 frame:0 TX packets:190 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:65824 (65.8 KB) TX bytes:65824 (65.8 KB)tap4e2f68bb-84 Link encap:Ethernet HWaddr fa:16:3e:32:19:6b inet addr:10.0.0.2 Bcast:10.0.0.63 Mask:255.255.255.192 inet6 addr: fe80::f816:3eff:fe32:196b/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:110210 errors:0 dropped:0 overruns:0 frame:0 TX packets:107493 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:21335986 (21.3 MB) TX bytes:8426397 (8.4 MB) ssh远程登录实例： sudo ip netns exec namespace_id ssh $username@ip 通过该命令来实现从控制节点通过ssh服务远程访问Openstack实例。 发送公共秘钥要实现免密码远程登录实例，首先需要将控制节点root用户的ssh的公共秘钥发送到远程实例，也就是/root/.ssh/id_rsa.pub文件中的内容，远程实例收到后会将公共密钥保存到登录用户的.ssh/authorized_keys文件中，这样下次登录远程实例时就不再需要密码。 发送公共密钥： 1234567891011$ sudo ip netns exec qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 ssh-copy-id -i /root/.ssh/id_rsa.pub openstack@10.0.0.9/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysopenstack@10.0.0.9&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &apos;openstack@10.0.0.9&apos;&quot;and check to make sure that only the key(s) you wanted were added. 注意：发送的公共秘钥必须是控制节点root用户的，因为在进入命名空间执行命令时需要加上sudo权限，而sudo是用来以其他身份来执行命令的，预设的身份为root，这样在ssh登录远程实例时是以控制节点的root用户来登录远程实例的openstack用户，因此需要将控制节点root用户的公共密钥发送给远程实例，root用户的公共密钥的路径是/root/.ssh/id_rsa.pub。 查看远程实例的authorized_keys： 12$ vi ~/.ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCjXQQUtIcLLcvBXVudZDBbQFK8BT/hB67oOrs792sfCuMhxvxvFRbma5UmnxwxOhXUIRjdz4u7tWhR3VVhqqnlHGDKOQVje/t2QtTlXXcBI3kGnc0Epem2NRMgRKp/h/Y1EOwtPNHRDVfr8C2znilXpWW1ueigHuJF4TWT7vEjgbApmWhopZcOXKbLkSu5dxLGUO3TzGqkASgpLG2XyuUJVqoREr5wbAZytq7R2p5KCxUZ6T7sDUQG+xmFPsfPg3MUHQmatTvtSf+mImotTkNSqOp2Itct9afX7SPkRncrXVWJ0qutbrRrkjRJm1l/sCjFBOD0x6txcFBX30nPvkDx root@controller 查看控制节点root用户公共密钥： 12# vi /root/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCjXQQUtIcLLcvBXVudZDBbQFK8BT/hB67oOrs792sfCuMhxvxvFRbma5UmnxwxOhXUIRjdz4u7tWhR3VVhqqnlHGDKOQVje/t2QtTlXXcBI3kGnc0Epem2NRMgRKp/h/Y1EOwtPNHRDVfr8C2znilXpWW1ueigHuJF4TWT7vEjgbApmWhopZcOXKbLkSu5dxLGUO3TzGqkASgpLG2XyuUJVqoREr5wbAZytq7R2p5KCxUZ6T7sDUQG+xmFPsfPg3MUHQmatTvtSf+mImotTkNSqOp2Itct9afX7SPkRncrXVWJ0qutbrRrkjRJm1l/sCjFBOD0x6txcFBX30nPvkDx root@controller 可以看到，两者是一样的，说明控制节点的root用户已经被授权通过公共密钥来访问远程实例。 免密码登录 免密码登录远程实例： 1$ sudo ip netns exec qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 ssh openstack@10.0.0.9 这样就通过ssh服务免密码远程登录Openstack实例，而不需要通过Dashboard的控制台来登录实例。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Namespace</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAT网关设计]]></title>
    <url>%2F2018%2F05%2F21%2FNAT%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[目的本文的目的是将Linux配置成NAT网关，具备NAT、DHCP、DNS功能，实现内网访问外网的通信、实现外网对特定主机的访问、实现内网主机的IP地址动态分配以及域名解析的功能。 环境实验环境介绍： 一台搭建了KVM环境的物理服务器，命名为Server，并在该服务器上创建两台虚拟机VM； 第一台虚拟机用作NAT网关，命名为Gateway，分配两张网卡，eth0连接内网，eth1连接外网； 第二台虚拟机作为内网主机，命名为Host，分配一张网卡eth0连接内网。 内网网段为：10.0.0.0/24 外网网段为：192.168.1.0/24 注：本实验其实也可以在VMware workstation或者virtualBox环境下进行，但是由于博主的电脑配置不高，不想在个人电脑上搭建该环境，且身边的服务器刚好有空闲的资源，所以就在服务器上搭建了该环境，至于如何在服务器上搭建KVM环境可以参照我的另一篇博客的部分内容 快速链接。 搭建实验环境： 创建内部网桥：服务器Server上创建一个虚拟网桥br-int，作为连接内网的交换机。 1234567$ sudo brctl addbr br-int$ brctl showbridge name bridge id STP enabled interfacesbr-int 8000.000000000000 no$ sudo ifconfig br-int up 注：在VMware workstation中搭建环境时需要在虚拟网络编辑器中将使用本地DHCP服务将IP分配给虚拟机选项取消勾选，因为我们需要实现NAT网关来给虚拟机分配IP。另外在创建虚拟网桥后一定要开启该网桥。 创建NAT网关虚拟机 其网络配置信息如下： eth0 eth1 通过ifconfig查看网络信息： 1234567891011121314151617$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:1c:83:53 inet6 addr: fe80::5054:ff:fe1c:8353/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...eth1 Link encap:Ethernet HWaddr 52:54:00:de:e8:51 inet addr:192.168.1.123 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::5054:ff:fede:e851/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 ... 可见NAT网关的eth0网卡因为没有DHCP服务器所以没有分配到IP，但外网网卡eth1已经有一个外网IP。 创建内部主机虚拟机 其网络配置信息如下： eth0 通过ifconfig查看网络信息： 1234567891011$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:1a:a1:78 inet6 addr: fe80::5054:ff:fe1a:a178/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 ... 可见其内网网卡eth0没有分配IP。 其实，在启动这两个虚拟机时，因为内部网络没有DHCP服务器，他们的内网网卡会一直等待DHCP服务器给它们分配IP，等待的时间大概是5分钟，最终还是没有获得IP。 修改两个虚拟机的网络配置文件 NAT网关：$ sudo vi /etc/network/interfaces 123456789101112131415source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet staticaddress 10.0.0.1netmask 255.255.255.0gateway 10.0.0.1auto eth1iface eth1 inet dhcp 因为NAT网关的eth0将做内网的网关，故将其IP设置为静态IP：10.0.0.1，这样更容易识别，且不会因为重启而改变。 内部主机：$ sudo vi /etc/network/interfaces 123456789source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet dhcp 在服务器Server上查看虚拟网桥的信息： 1234$ brctl show br-intbridge name bridge id STP enabled interfacesbr-int 8000.fe54001aa178 no vnet1 vnet3 可以看到虚拟网桥br-int上面已经有两个网络接口，对应两个虚拟机的内部网卡。 NAT 功能实现NAT网关的功能是通过Linux中自带的iptables来实现，NAT功能包括SNAT和DNAT。SNAT是源地址转换，在内网主机访问外网时发挥作用，可以将内网主机的ip地址转换为网关的ip地址。DNAT是目的地址转换，在外网的主机通过NAT网关的ip和端口对内网主机发起访问时发挥作用，可以将NAT网关的ip地址与端口转换为对应内网主机的ip，从而实现从外网对内网中某一特定主机的访问。当然要将Linux虚拟机配置成NAT网关，首先得要开启Linux虚拟机的网络转发功能。下面将介绍NAT功能的实现过程： 开启网络转发功能 临时开启网络转发功能，需要切换到root用户，命令如下： # echo 1 &gt; /proc/sys/net/ipv4/ip_forward 这样在下次虚拟机重启后该功能会自动关闭，因此可以修改配置文件的方式来开启该功能并永久生效。 永久开启网络转发功能： # vi /etc/sysctl.conf 在文件里面添加一行net.ipv4.ip_foward=1，在下次重启之后就不会还原了。 SNAT的实现 在iptables的nat表的POSTROUTING规则链中添加规则，使得从10.0.0.0/24网络发到NAT网关的数据包，从eth1转发出去，并将数据包的源ip地址修改为NAT网关的外网地址，命令如下 ： sudo iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -o eth1 -j SNAT --to-source 192.168.1.123 192.168.1.123是NAT网关的外网网卡eth1的IP。 DNAT的实现 在iptables的nat表的PREROUTING规则链中添加规则，使得从外网发往NAT网关固定端口（如：8080）的TCP（这里也可以添加其他网络协议）数据包转发到内网的某固定主机上，命令如下： sudo iptables -t nat -A PREROUTING -i eth1 -d 192.168.1.67 -p tcp --dport 8080 -j DNAT --to-destination 10.0.0.107:22 10.0.0.107是内网某主机的IP，这里说明一下：实际实现的过程中，最好先实现DHCP与DNS的功能。 保存iptables规则 sudo iptables-save | sudo tee /etc/iptables.sav 编辑/etc/rc.local文件，将下面的一行添加到&quot;exit 0&quot;之前： iptables-restore &lt; /etc/iptables.sav 这样每次重启机器时都会自动加载NAT相关的iptables规则。 DHCP与DNS功能实现实现DHCP服务器可以使用isc-dhcp-server工具包，实现DNS服务器可以使用bind9工具包，但是本实验考虑使用DNSmasq工具来同时实现DHCP服务器与DNS服务器的功能。DNSmasq是一个小巧且方便地用于配置DHCP和DNS的工具，适用于小型网络。作为域名解析服务器（DNS），DNSmasq可以通过缓存DNS请求来提高对访问过的网址的连接速度。作为DHCP服务器，DNSmasq可以用于为局域网电脑分配内网ip地址和提供路由。 安装DNSmasq工具： $ sudo apt-get install dnsmasq 编辑DNSmasq的配置文件/etc/dnsmasq.conf，添加下面两行： 12interface=eth0dhcp-range=10.0.0.100,10.0.0.200,72h 其中interface是用作内网网关的网卡，也就是NAT网关的eth0网卡；dhcp-range是动态分配的IP地址池；72h表示分配的IP的有效时间是72个小时，到时间后需要重新分配IP。 重启DNSmasq服务： 12$ sudo /etc/init.d/dnsmasq restart[ ok ] Restarting dnsmasq (via systemctl): dnsmasq.service. 到这里就配置内容就完成了。然后需要重启这两个虚拟机。 测试结果 DHCP服务 在完成DHCP服务器配置重启虚拟机后，会发现在启动过程中不再需要等待5分钟来获取IP了，进入内部主机查看网络信息： 123456789101112131415161718$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:1a:a1:78 inet addr:10.0.0.176 Bcast:10.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::5054:ff:fe1a:a178/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:17 errors:0 dropped:0 overruns:0 frame:0 TX packets:21 errors:0 dropped:0 overruns:0 carrier:0 collisions:36 txqueuelen:1000 RX bytes:1642 (1.6 KB) TX bytes:2182 (2.1 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:164 errors:0 dropped:0 overruns:0 frame:0 TX packets:164 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:12200 (12.2 KB) TX bytes:12200 (12.2 KB) 可以看到eth0分配到了一个在DHCP地址池内的一个IP：10.0.0.176。 DNS与NAT服务 这里将同时对DNS服务与NAT服务进行测试 内网到外网 由于我所在的网络所有的ping外网的报文都会被拦截，所以没有办法通过ping来测试网络的连通性，这里采用wget命令在内部主机测试： 1234567891011$ wget http://www.baidu.com--2018-07-08 16:30:37-- http://www.baidu.com/Resolving www.baidu.com (www.baidu.com)... 180.97.33.108, 180.97.33.107Connecting to www.baidu.com (www.baidu.com)|180.97.33.108|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 2381 (2.3K) [text/html]Saving to: ‘index.html’index.html 100%[================================&gt;] 2.33K --.-KB/s in 0s 2018-07-08 16:30:37 (61.1 MB/s) - ‘index.html’ saved [2381/2381] 上面的测试说明内网主机到外网的连通性，并且也实现了域名解析的功能。 外网到内网 在博主的个人Windows电脑上去访问内网主机，以此来测试外网到内网的连通性： 12345678910111213141516171819202122232425262728293031323334[c:\~]$ ssh openstack-image@192.168.1.152 8080Connecting to 192.168.1.152:8080...Connection established.To escape to local shell, press &apos;Ctrl+Alt+]&apos;.Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage134 packages can be updated.55 updates are security updates.Last login: Sun Jul 8 16:13:54 2018openstack-image@ubuntu:~$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:d0:51:38 inet addr:10.0.0.176 Bcast:10.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::5054:ff:fed0:5138/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:96 errors:0 dropped:0 overruns:0 frame:0 TX packets:125 errors:0 dropped:0 overruns:0 carrier:0 collisions:516 txqueuelen:1000 RX bytes:17842 (17.8 KB) TX bytes:15515 (15.5 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:176 errors:0 dropped:0 overruns:0 frame:0 TX packets:176 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:13392 (13.3 KB) TX bytes:13392 (13.3 KB) 可以访问，外网到内网也是连通的。到此，需要实现的功能都已经实现了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>NAT</tag>
        <tag>DHCP</tag>
        <tag>DNS</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟化技术与云计算平台报告]]></title>
    <url>%2F2018%2F03%2F22%2F%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97%E5%B9%B3%E5%8F%B0%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[前言从2006年谷歌首次提出“云计算”的概念到现在，云计算已经经历的十多年的发展，有众多厂商、组织和学者投入其中。一些云服务厂商有自己的云计算平台，但只是为客户提供云服务，外部开发者无法对这些厂商的云平台进行开发。另外也有一些组织机构或厂商开发的云计算平台是开源的，吸引大量开发者在这些开源平台上开展自己的工作。 目前，主流的开源云计算平台有很多种，本问将对这些平台进行调研以选择最适合于自己需求的平台。另外，虚拟化技术又是云计算的核心支撑技术，是将各种计算及存储资源充分整合和高效利用的关键技术，当前虚拟化技术也是多种多样，并且开源云计算平台往往又支持多种底层的虚拟化技术，因此也有必要对虚拟化技术进行调研，以选择最适合云平台和上层应用的虚拟化技术。综上，本报告的主要目标就是对多种虚拟化技术和多种开源云计算平台进行对比，以选出最满足需求的技术与平台。 本文将首先简单对比虚拟化与云计算。其后，由于虚拟化技术是云计算的基础，本报告将先对虚拟化技术展开论述，然后再对云计算平台进行论述。 虚拟化与云计算借助虚拟化技术，用户可以单个物理硬件系统为基础创建多个模拟环境或专用资源。并使用一款名为“Hypervisor”(虚拟机监控程序)的软件直接连接到硬件，从而将一个系统划分为不同的、单独安全环境，即虚拟机 (VM)。Hypervisor 能够将计算机资源与硬件分离并适当分配资源，而虚拟机则依赖这些功能运行。 云计算则由多种规则和方法组合而成，可以跨任何网络向用户按需提供计算、网络和存储基础架构资源、服务、平台和应用。这些基础架构资源、服务和应用来源于云。 简单来讲，云就是一系列管理及自动化软件编排而成的虚拟资源池，旨在帮助用户通过支持自动扩展和动态资源分配的自助服务门户，按需对这些资源进行访问。 下面对虚拟化与云计算做一个简单的对比： 虚拟化 云 定义 技术 方法论 目的 从一个物理硬件系统创建多个虚拟环境 汇聚并自动化分配虚拟资源以供按需使用 用途 针对具体用途为特定用户提供打包资源 针对多种用途为用户群组提供不同资源 使用寿命 数年（长期） 数小时至数月 成本 资本支出（CAPEX）高运营支出（OPEX）低 共有云：CAPEX高、OPEX低私有云：CAPEX低、OPEX高 可扩展性 纵向扩展 横向扩展 工作负载 有状态 无状态 租赁 单一租户 多个租户 虚拟化技术按照虚拟化的对象分类，虚拟化可分为服务器虚拟化、操作系统虚拟化、存储虚拟化、网络虚拟化等。其中服务器虚拟化对CPU、内存、设备与I/O这三种硬件资源的虚拟化技术已经相当成熟，但对GPU的虚拟化却还有很大的提升空间。下面将分别介绍服务器虚拟化中CPU虚拟化及GPU虚拟化相关的技术，然后对现在主流的虚拟化平台做一些比较。 一、CPU虚拟化目前，为了解决x86体系结构下的CPU虚拟化问题，业界提出了全虚拟化和半虚拟化两种不同的软件方案。除了通过软件的方式实现CPU虚拟化外，业界还提出了在硬件层添加支持功能的硬件辅助虚拟化方案来处理那些敏感的高级别指令。 全虚拟化在宿主机底层物理硬件与VM之间增加一个软件层，即虚拟机监控器（VMM或hypervisor），此时，VMM充当主机操作系统，用来管理不同的虚拟机，如图1所示。它隐藏了特定计算平台的实际物理特性，为用户提供抽象的、统一的、模拟的计算环境（称为虚拟机）。在VMM平台上，可以模拟出多套虚拟机，实现了在单机上运行多个不同类型操作系统的虚拟机。全虚拟化的优点是不需要修改客户机操作系统，因此支持多种操作系统，缺点是VMM层工作负荷较大，并占用一定的宿主机资源，性能不如裸机。主要代表有VMware vSphere，Microsoft公司的Virtual PC、Redhat公司的RED HAT ENTERPRISE VIRTUALIZATION等。 半虚拟化与全虚拟化类似，不同之处是需要修改客户机操作系统的核心代码，即增加一个专门的虚拟化应用程序接口，优化客户操作系统发出的指令，与VMM能够协同工作，以减轻VMM和宿主机的负担，进一步提升了虚拟机的性能，如图2所示。缺点是需要修改客户操作系统，影响了技术的普及。主要代表有使用开源虚拟化技术的Citrix的Xenserver、Microsoft的Hyper-V 。 为了更好地实现全、半虚拟化技术，Intel与AMD对传统X86架构进行改进，分别设计了Intel-VT和AMD-V CPU硬件辅助虚拟化技术。将原来的特权等级Ring 0、1、2、3 定义为Non-Root mode，新增了一个Root mode 特权等级（或称为Ring -1），这种情况下，OS 即可运行在原来Ring0 的等级，而VMM 则调整到更底层的Root Mode 等级，其架构如图3。硬件辅助虚拟化有效地解决了虚拟机效率低的问题，它使虚拟机可以运行ring 0 的指令，不用再进行操作系统的ring 切换，提高了虚拟机的整体效率。 现在主流的半、全虚拟化产品都支持硬件辅助虚拟化，代表有Oracle公司的VirtualBox、RHEV、VMware vSphere和Xneserver。 二、GPU虚拟化GPU虚拟化相关技术还垄断在少数厂商手中，并没有像CPU、内存、存储一样在开源社区推广普及。下面将介绍三大显卡厂商GPU虚拟化的发展。 NVIDIA，早在2013年，NVIDIA就推出了行业内第一款GPU虚拟化显卡GRID K1/K2，同期联合Citrix推出了商用的vGPU虚拟桌面解决方案，这比AMD提前了近3年。GPU虚拟化技术的出现，给一直被诟病性能不足的桌面虚拟化带来了转机。在2016年，NVIDIA推出第二款GPU虚拟化显卡，Maxwell架构的Tesla M6/M10/M60，新版的GRID将使用授权分为 3 个不同版本，依据版本不同收取额外软件授权使用费。 在2017年8月份NVIDIA推出了最新版GRID 5.0，虚拟化显卡新增Pascal架构的Tesla P4/P6/P40/P100，其中Quadro Virtual Datacenter Workstation版的授权支持vGPU在图形渲染模式和高性能计算模式之间切换，这是硬件厂商首次在vGPU层面将图形渲染和高性能计算进行了统一，又一次引领了行业趋势和市场需求。NVIDIA的虚拟化显卡只是硬件，还需要相应的服务器虚拟化系统的支持，这和Intel的CPU需要操作系统Windows和Linux来配合一样。现在只有Xen和ESXi能够支持GRID virtual GPU solution，被大量第三方厂商采用的KVM虚拟化平台还没有出现在GRID的支持列表中，因此可以说GPU另外一只脚还没能踏进云计算时代。2017年7月份NVIDIA和Nutanix宣布将合作在年底推出AHV版本的GRID，这算是KVM虚拟化走出了第 一步。在解决GPU虚拟化后，如何将虚拟机的画面传送到客户端，这是KVM虚拟化可以商用的第二步。KVM上默认配置的Spice协议对3D的支持并不好，Nutanix以及其他KVM方案解决商还需自行开发出可用的桌面传输协议，这才算是彻底完成了GRID在KVM上的应用。 AMD，AMD在NVIDIA推出GRID K1/K2 的两年半后才推出了自己的GPU虚拟化产品MxGPU，算是姗姗来迟。共有三款FirePro S系列的GPU支持MxGPU，一块GPU最多可以支持 32 个用户。当MxGPU上的虚拟机比较少时，能够达到图形工作站的性能。随着虚拟机数量增多，每个虚拟机获得的GPU性能逐步降低。有别于NVIDIA GRID通过软件实现的显卡虚拟化方式，AMD MxGPU是“全球首款基于硬件的虚拟化GPU解决方案”。MxGPU每个虚拟机能分得一定数量的独享流处理器和显存空间，这样可以避免不同虚拟机对GPU资源的抢夺，造成用户噪音。这种噪音问题直到今年8月底推出NVIDIA GRID 5. 0 才得到解决。MxGPU在定价上采用的是更符合买方逻辑的营销方法，只向用户收取需付硬件的购买费用。不过遗憾的是，目前只有VMware的ESXi支持MxGPU，Xen暂时只有技术验证版。没能同时支持两种主流化的虚拟化系统，一定程度上阻碍了MxGPU在市场的普及速度。Citrix的用户还是可以用过vSphere+XenDesktop的方案用上MxGPU，相比使用免费的XenServer，要多支付vSphere的费用。 Intel，Intel官方将不同的Intel GPU虚拟化技术分别命名为Intel GVT-s，Intel GVT-d和Intel GVT-g，分布对应API转发，直通，完全虚拟化。Intel GVT-g和NVIDIA vGPU类似，支持Xen/KVM平台，每个GPU最多能分享给7个用户同时使用。其中XenGT在 2016 年最早实现了业界的vGPU在线迁移，NVIDIA GRID直到这个季度才和Citrix合作完成了vGPU在线迁移。 2016 年的 2017 年2月份，Linux 4.10中加入Intel GVT-g for KVM。这是三大GPU厂商中，第一个支持KVM平台的完全虚拟化方案，意味着第三方采用KVM的云计算厂商终于有了一个可用的vGPU方案。不过Intel GPU虚拟化，由于核显性能的原因，只能满足图像密集型的用户体验，不能像GRID vGPU和MxGPU一样满足图形渲染的重度使用场景。Intel GPU虽然支持了大多数虚拟化桌面厂商使用的Xen/KVM两大类服务器虚拟化系统，可是硬件却和VDI高密度的使用场景不太搭。首先Intel的完全虚拟化只支持Broadwell架构以后的核芯显卡，作为VDI服务器中常用的Xeon E5/E7 v4 系列，以及第 一个的Xeon Scalable处理器，都没有核芯显卡。这在很大程度上限定了Intel GPU虚拟化在VDI的使用规模，有种落入有枪无弹的尴尬境地。 三、虚拟化平台比较服务器虚拟化技术日益成熟，并具有广泛的应用前景，目前有很多厂商进行虚拟化技术产品的开发和生产，包括：VMware、Microsoft、Citrix、IBM和RedHat等，其各自产品都有不同的特点，产品功能日益强大。下面将比较一下四种主流服务器虚拟化平台，如下表 ： VMware Xen KVM Hyper-V 厂商 VMware Citrix Red Hat Microsoft 是否免费 付费 开源免费 开源免费 付费 宿主机系统 WindowsLinux NetBSDLinuxSolaris Linux Windows server 2008及以上系统 客户机系统 Windows 2003、Windows 2008、RedHat、Debian、Ubuntu、Centos Xen-PV：纯Linux；Xen-HVM：支持Windows、Linux Linux、Windows Windows系列、Linux 支持技术 硬件辅助虚拟化（全虚拟化） 硬件辅助虚拟化（HVM全虚拟化、PV半虚拟化） 硬件辅助虚拟化（全虚拟化） 硬件辅助虚拟化（半虚拟化） 支持的vGPU产品 NVIDIA GRIDAMD MxGPU NVIDIA GRIDAMD MxGPU（技术验证版） Intel GVT-g for KVM 优点 相对成熟的商业软件，市场占有率较大 性能较好，支持半虚拟化 是内核本身的一部分，因此可以利用内核的优化和改进；高性能，稳定，无需修改客户机系统 对Windows的支持较好 缺点 不开源，费用较高 操作复杂，维护成本较高，目前已被RedHat抛弃 虚拟机性能比Xen略低 对Linux的支持较差，性能损失大 开源云计算平台Openstack对这四种虚拟化平台都有支持，默认使用的是KVM，Openstack与KVM结合的方案也已经相当成熟。另外，考虑到KVM是开源免费的虚拟化技术；宿主机系统支持绝大多数Linux系统，对于使用Linux系统的服务器都有很好的支持；而客户机操作系统不仅支持Linux，还支持Windows，可以满足绝大多数用户的需求，因此选择KVM作为Openstack底层的虚拟化技术的理由是很充分的。不过，KVM对于虚拟GPU的支持不是很好，只有Intel GVT-g for KVM可以支持KVM平台的全虚拟化方案，但是Intel GPU虚拟化由于核显性能的原因，只能满足图像密集型的用户体验，不能满足图形渲染等重度使用的场景。 云计算平台目前已经有多个云计算平台的开源实现，主要的开源云计算项目有Openstack、Eucalyptus、CloudStack和OpenNebula等，现比较如下： Openstack Eucalyptus CloudStack OpenNebula 发布时间 2010年7月 2008年5月 2010年5月 2008年7月 最新版本 Queens 4.4 4.11 5.4 授权协议 Apache v2.0 GPL v3.0 Apache v2.0 Apache v2.0 基本架构 Nova、Glance、Neutron、Keystone、Horizon、swift、Tacker等 Cloud Controller、Cluster Controller、Node Controller、Walrus、 Storage Controller 主要包括管理服务、云基础设施和网络三大部分 主要包括接口与API、用户与组、主机、网络、存储、集群6个部分 虚拟化技术支持 KVM、LXC、QEMU、UML、Vmware ESX/ESXi、Xen、Hyper-V Xen、KVM、ESXi KVM、Xen、ESXi、OVM、Baremetal Xen、KVM、Vmware 用户界面 Dashboard，较简单 web界面 Web Console，功能较完善 web界面 社区活跃程度 人数多，活跃用户数最多 人数多，但活跃用户数较少 人数少，但活跃用户数较多 人数较少，活跃用户数也少 兼容云平台 Amazon EC2，S3 Amazon EC2，S3 Amazon EC2，S3 Amazon EC2，S3 开发主导 开源社区 Eucalyptus System Inc Citrix公司 开源社区 主要支持厂商 160家左右，包括NASA、Rackspace、HP、Dell、UnitedStack等 亚马逊、戴尔、惠普、Intel、Redhat、Vmware等 不到60家，包括诺基亚、日本电话电报公司、阿尔卡特、迪士尼等 IBM、Akamai、Blackberry、Fuze、Telefonica、Indigital 官方文档 非常详细 不够详细 详细 详细 检测和审计 Telemetry Service Accounting system Event/Audit logs Accounting system、periodically-Monitoring 数据库 PostareSQL、MySQL、SQLite HyperSQL Database MySQL SQLite、MySQL 部署 私有云、共有云、混合云 私有云、混合云 私有云、共有云、混合云 私有云、共有云、混合云 操作系统 Debian 7.0、openSUSE、SUSE、Red Hat、CentOS、Fedora、Ubuntu CentOS、RHEL CentOS、RHEL6.3+、Ubuntu Red Hat、Ubuntu、SUSE、CentOS、Debian 开发语言 Python Java、C/C++ Java C、Ruby、shell 开源市场部署比例 69% 3% 14% 无统计数据 这四种主流的开源云计算平台都经过了近十年的发展，更新迭代了很多版本，能够从众多云计算平台的竞争中存活下来，都有相应的支持厂商和用户，说明它们各有各的特点，如在开发语言上就各有特色，Openstack使用的是Python语言，Eucalyptus使用Java、C/C++，CloudStack仅使用Java，而OpenNebula却显得比较奇怪，使用的是C语言、Ruby和shell，多种语言混杂而成。但不同平台之间还是有较大差距，从结果来看，在开源市场的部署比例，Openstack 69%的比例占据了绝对统治地位，Openstack能占据这样的地位有多方面的原因，如Openstack支持绝大多数的虚拟化技术，支持的操作系统也很多，使得Openstack具有广阔的应用范围；Openstack的官方文档非常详细，也降低了其学习成本；Openstack具有一个充满活力的开源社区，开发者不断为Openstack的发展作出贡献；OpenStack的支持厂商有160家左右，有如此多的厂商支持，给OpenStack的发展提供了根本保障。综合以上多方面的原因，本项目采用Openstack作为底层的云计算平台为上层提供基础设施资源也是理所当然的。另外，在2018年2月28日发布的Openstack Queens最新版本中，新引入的Marquee功能正是为了提供对vGPU的内置支持能力，这意味着用户能够将GPU添加至虚拟机中，为本项目的上层应用，如深度学习等需要强大GPU运算能力的应用提供了支持。 总结通过以上的分析，能够清楚的了解各种虚拟化技术及各种云计算平台的差异，对于要选择满足自己需求的技术与平台会有一些帮助。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>Openstack</tag>
        <tag>云计算</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack环境下手动安装Mistral]]></title>
    <url>%2F2017%2F12%2F23%2FOpenstack%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Mistral%2F</url>
    <content type="text"><![CDATA[概述在openstack平台中能够成功安装Tacker，但是安装的Tacker并不能用，因为在tacker中创建VIM时需要调用Mistral工作流组件。因此本文就来介绍在openstack环境中手动安装Mistral的过程。 注： 安装的openstack是在Ubuntu 16.04系统下的Ocata版本；本文中涉及的密码都统一设置成 “openstack”。 参考官方文档 一、安装必要组件12$ apt-get install python-dev python-setuptools python-pip libffi-dev \ libxslt1-dev libxml2-dev libyaml-dev libssl-dev 二、安装Mistral server1、下载Mistral源码，并进入下载目录$ git clone https://github.com/openstack/mistral.git $ cd mistral 2、安装Mistral环境依赖包$ pip install -r requirements.txt 3、安装Mistral$ python setup.py install 4、生成配置文件$ oslo-config-generator --config-file tools/config/config-generator.mistral.conf --output-file etc/mistral.conf 5、创建Mistral日志文件和配置文件夹# mkdir -p /etc/mistral /var/log/mistral 6、复制配置文件到配置文件夹# cp etc/* /etc/mistral/ 7、修改配置文件123456789101112131415# vi /etc/mistral/mistral.conf [keystone_authtoken]auth_uri = http://controller:5000auth_version = 3identity_uri = http://controller:35357/admin_user = adminadmin_password = openstackadmin_tenant_name = admin[database]connection = mysql+pymysql://mistral:openstack@controller/mistral [DEFAULT]transport_url = rabbit://openstack:openstack@controller 8、创建数据库123456# mysqlMariaDB [(none)]&gt; CREATE DATABASE mistral;MariaDB [mistral]&gt; GRANT ALL PRIVILEGES ON mistral.* TO &apos;mistral&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [mistral]&gt; GRANT ALL PRIVILEGES ON mistral.* TO &apos;mistral&apos;@&apos;%&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [mistral]&gt; flush privileges;MariaDB [mistral]&gt; exit; 9、创建服务和endpoint1234$ openstack service create --name mistral --description &quot;Openstack Workflow service&quot; workflow$ openstack endpoint create --region RegionOne workflow public http://controller:8989/v2$ openstack endpoint create --region RegionOne workflow internal http://controller:8989/v2$ openstack endpoint create --region RegionOne workflow admin http://controller:8989/v2 10、初始化数据库信息12345678910111213141516171819202122232425root@controller:/home/openstack# mistral-db-manage --config-file /etc/mistral/mistral.conf upgrade headINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; 001, Kilo releaseINFO [alembic.runtime.migration] Running upgrade 001 -&gt; 002, KiloINFO [alembic.runtime.migration] Running upgrade 002 -&gt; 003, cron_trigger_constraintsINFO [alembic.runtime.migration] Running upgrade 003 -&gt; 004, add description for executionINFO [alembic.runtime.migration] Running upgrade 004 -&gt; 005, Increase executions_v2 column size from JsonDictType to JsonLongDictTypeINFO [alembic.runtime.migration] Running upgrade 005 -&gt; 006, add a Boolean column &apos;processed&apos; to the table delayed_calls_v2INFO [alembic.runtime.migration] Running upgrade 006 -&gt; 007, Move system flag to base definitionINFO [alembic.runtime.migration] Running upgrade 007 -&gt; 008, Increase size of state_info column from String to TextINFO [alembic.runtime.migration] Running upgrade 008 -&gt; 009, Add database indicesINFO [alembic.runtime.migration] Running upgrade 009 -&gt; 010, add_resource_members_v2_tableINFO [alembic.runtime.migration] Running upgrade 010 -&gt; 011, add workflow id for executionINFO [alembic.runtime.migration] Running upgrade 011 -&gt; 012, add event triggers tableINFO [alembic.runtime.migration] Running upgrade 012 -&gt; 013, split_execution_table_increase_namesINFO [alembic.runtime.migration] Running upgrade 013 -&gt; 014, fix_past_scripts_discrepanciesINFO [alembic.runtime.migration] Running upgrade 014 -&gt; 015, add_unique_keys_for_non_locking_modelINFO [alembic.runtime.migration] Running upgrade 015 -&gt; 016, Increase size of task_executions_v2.unique_keyINFO [alembic.runtime.migration] Running upgrade 016 -&gt; 017, Add named lock tableINFO [alembic.runtime.migration] Running upgrade 017 -&gt; 018, increate_task_execution_unique_key_sizeINFO [alembic.runtime.migration] Running upgrade 018 -&gt; 019, Change scheduler schema.INFO [alembic.runtime.migration] Running upgrade 019 -&gt; 020, add type to task executionINFO [alembic.runtime.migration] Running upgrade 020 -&gt; 021, Increase environments_v2 column size from JsonDictType to JsonLongDictType 11、添加自带的action123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172root@controller:/home/openstack# mistral-db-manage --config-file /etc/mistral/mistral.conf populate*输出结果可能为：*No handlers could be found for logger &quot;mistral.actions.openstack.action_generator.base&quot;*也可能会出错：*……2017-12-22 22:21:24.486 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.551 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.761 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.878 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.883 16802 WARNING oslo_config.cfg [-] Option &quot;auth_uri&quot; from group &quot;keystone_authtoken&quot; is deprecated for removal (The auth_uri option is deprecated in favor of www_authenticate_uri and will be removed in the S release.). Its value may be silently ignored in the future.^C2017-12-22 22:21:25.382 16802 CRITICAL Mistral [-] Unhandled error: KeyboardInterrupt2017-12-22 22:21:25.382 16802 ERROR Mistral Traceback (most recent call last):2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/bin/mistral-db-manage&quot;, line 10, in &lt;module&gt;2017-12-22 22:21:25.382 16802 ERROR Mistral sys.exit(main())2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/db/sqlalchemy/migration/cli.py&quot;, line 137, in main2017-12-22 22:21:25.382 16802 ERROR Mistral CONF.command.func(config, CONF.command.name)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/db/sqlalchemy/migration/cli.py&quot;, line 75, in do_populate2017-12-22 22:21:25.382 16802 ERROR Mistral action_manager.sync_db()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/services/action_manager.py&quot;, line 80, in sync_db2017-12-22 22:21:25.382 16802 ERROR Mistral register_action_classes()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/services/action_manager.py&quot;, line 126, in register_action_classes2017-12-22 22:21:25.382 16802 ERROR Mistral _register_dynamic_action_classes()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/services/action_manager.py&quot;, line 86, in _register_dynamic_action_classes2017-12-22 22:21:25.382 16802 ERROR Mistral actions = generator.create_actions()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/action_generator/base.py&quot;, line 143, in create_actions2017-12-22 22:21:25.382 16802 ERROR Mistral client_method = class_.get_fake_client_method()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/base.py&quot;, line 75, in get_fake_client_method2017-12-22 22:21:25.382 16802 ERROR Mistral return cls._get_client_method(cls._get_fake_client())2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/actions.py&quot;, line 380, in _get_fake_client2017-12-22 22:21:25.382 16802 ERROR Mistral return cls._get_client_class()(session=sess)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/ironic_inspector_client/v1.py&quot;, line 88, in __init__2017-12-22 22:21:25.382 16802 ERROR Mistral super(ClientV1, self).__init__(**kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/ironic_inspector_client/common/http.py&quot;, line 134, in __init__2017-12-22 22:21:25.382 16802 ERROR Mistral region_name=region_name)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 856, in get_endpoint2017-12-22 22:21:25.382 16802 ERROR Mistral return auth.get_endpoint(self, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/base.py&quot;, line 212, in get_endpoint2017-12-22 22:21:25.382 16802 ERROR Mistral service_catalog = self.get_access(session).service_catalog2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/base.py&quot;, line 136, in get_access2017-12-22 22:21:25.382 16802 ERROR Mistral self.auth_ref = self.get_auth_ref(session)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/generic/base.py&quot;, line 198, in get_auth_ref2017-12-22 22:21:25.382 16802 ERROR Mistral return self._plugin.get_auth_ref(session, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/v3/base.py&quot;, line 167, in get_auth_ref2017-12-22 22:21:25.382 16802 ERROR Mistral authenticated=False, log=False, **rkwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 766, in post2017-12-22 22:21:25.382 16802 ERROR Mistral return self.request(url, &apos;POST&apos;, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/positional/__init__.py&quot;, line 101, in inner2017-12-22 22:21:25.382 16802 ERROR Mistral return wrapped(*args, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 616, in request2017-12-22 22:21:25.382 16802 ERROR Mistral resp = send(**kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 674, in _send_request2017-12-22 22:21:25.382 16802 ERROR Mistral resp = self.session.request(method, url, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/requests/sessions.py&quot;, line 508, in request2017-12-22 22:21:25.382 16802 ERROR Mistral resp = self.send(prep, **send_kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/requests/sessions.py&quot;, line 618, in send2017-12-22 22:21:25.382 16802 ERROR Mistral r = adapter.send(request, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/requests/adapters.py&quot;, line 440, in send2017-12-22 22:21:25.382 16802 ERROR Mistral timeout=timeout2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py&quot;, line 601, in urlopen2017-12-22 22:21:25.382 16802 ERROR Mistral chunked=chunked)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py&quot;, line 380, in _make_request2017-12-22 22:21:25.382 16802 ERROR Mistral httplib_response = conn.getresponse(buffering=True)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/httplib.py&quot;, line 1136, in getresponse2017-12-22 22:21:25.382 16802 ERROR Mistral response.begin()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/httplib.py&quot;, line 453, in begin2017-12-22 22:21:25.382 16802 ERROR Mistral version, status, reason = self._read_status()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/httplib.py&quot;, line 409, in _read_status2017-12-22 22:21:25.382 16802 ERROR Mistral line = self.fp.readline(_MAXLINE + 1)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/socket.py&quot;, line 480, in readline2017-12-22 22:21:25.382 16802 ERROR Mistral data = self._sock.recv(self._rbufsize)2017-12-22 22:21:25.382 16802 ERROR Mistral KeyboardInterrupt2017-12-22 22:21:25.382 16802 ERROR Mistral 纠结了很久后发现这些都不用太在意，直接跳过，哈哈！ 三、安装Mistral client1、下载Mistral-client源码$ git clone git://git.openstack.org/openstack/python-mistralclient.git -b stable/ocata $ cd python-mistralclient 2、安装Mistral-client模块$ pip install -r requirements.txt $ python setup.py install 四、安装Mistral horizon1、下载Mistral-horizon源码$ git clone https://git.openstack.org/openstack/mistral-dashboard.git -b stable/ocata $ cd mistral-dashboard/ 2、安装Mistral-horizon模块$ pip install -r requirements.txt $ python setup.py install 3、复制一个文件# cp -b mistraldashboard/enabled/_50_mistral.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/_50_mistral.py 4、重启apache2服务# service apache2 restart 安装好Mistral-horizon后，admin用户登录dashboard界面就可以看到Mistral相关的workflow，如图： 五、运行Mistral server运行下面的第一条命令：12345678910111213141516171819202122232425262728root@controller:/home/openstack/mistral# python mistral/cmd/launch.py --server all --config-file /etc/mistral/mistral.conf|\\ //| || ||||\\ //|| __ || __ __ |||| \\// || || // |||||| || \\ // \\ |||| \/ || \\ || || || \\ |||| || || \\ || || || /\\ |||| || || __// ||_// || \\__// \\_ ||Mistral Workflow Service, version 6.0.0Launching server components [engine,event-engine,api,executor]...2017-12-22 22:42:58.373 16966 INFO mistral.event_engine.default_event_engine [-] Starting event notification task...2017-12-22 22:42:58.571 16966 INFO mistral.event_engine.default_event_engine [-] Found 0 event triggers./usr/local/lib/python2.7/dist-packages/oslo_messaging/server.py:341: FutureWarning: blocking executor is deprecated. Executor default will be removed. Use explicitly threading or eventlet instead in version &apos;pike&apos; and will be removed in version &apos;rocky&apos; category=FutureWarning)2017-12-22 22:42:58.913 16966 WARNING oslo_config.cfg [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Option &quot;auth_uri&quot; from group &quot;keystone_authtoken&quot; is deprecated for removal (The auth_uri option is deprecated in favor of www_authenticate_uri and will be removed in the S release.). Its value may be silently ignored in the future.2017-12-22 22:42:58.915 16966 WARNING oslo_config.cfg [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Option &quot;auth_uri&quot; from group &quot;keystone_authtoken&quot; is deprecated. Use option &quot;www_authenticate_uri&quot; from group &quot;keystone_authtoken&quot;.2017-12-22 22:42:58.925 16966 WARNING keystonemiddleware.auth_token [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Please set this to True.2017-12-22 22:42:58.926 16966 WARNING keystonemiddleware.auth_token [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Use of the auth_admin_prefix, auth_host, auth_port, auth_protocol, identity_uri, admin_token, admin_user, admin_password, and admin_tenant_name configuration options was deprecated in the Mitaka release in favor of an auth_plugin and its related options. This class may be removed in a future release.2017-12-22 22:42:58.931 16966 INFO oslo.service.wsgi [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] mistral_api listening on 0.0.0.0:89892017-12-22 22:42:58.932 16966 INFO oslo_service.service [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Starting 4 workersAPI server started.API server started.API server started.API server started.Event engine server started.Executor server started.Engine server started. 六、测试一下Mistral是否可用12345678910111213openstack@controller:~/mistral/etc$ mistral workbook-list+--------+--------+------------+------------+| Name | Tags | Created at | Updated at |+--------+--------+------------+------------+| &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; |+--------+--------+------------+------------+openstack@controller:~/mistral/etc$ mistral action-list+--------+--------+-----------+--------+-------------+--------+------------+------------+| ID | Name | Is system | Input | Description | Tags | Created at | Updated at |+--------+--------+-----------+--------+-------------+--------+------------+------------+| &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; |+--------+--------+-----------+--------+-------------+--------+------------+------------+ OK，成功了，开心！！！]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Mistral</tag>
        <tag>Tacker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack环境下手动安装Tacker]]></title>
    <url>%2F2017%2F12%2F23%2FOpenstack%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Tacker%2F</url>
    <content type="text"><![CDATA[概述本文参考官方文档，在现有的openstack平台上，手动安装Tacker。基础的openstack平台包含了最核心的keystone、glance、nova、neutron、horizon这5个组件，但是Tacker还需要预先安装好Mistral和Barbican这两个组件，在安装好这两个组件后就可以开始按照以下步骤安装Tacker了。参考：官方文档链接 注： 本文涉及到的密码都统一设置成openstack。 一、安装Tacker server1、创建数据库12345# mysqlMariaDB [(none)]&gt; CREATE DATABASE tacker;MariaDB [tacker]&gt; GRANT ALL PRIVILEGES ON tacker.* TO &apos;tacker&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [tacker]&gt; GRANT ALL PRIVILEGES ON tacker.* TO &apos;tacker&apos;@&apos;%&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [tacker]&gt; exit; 2、创建user、role、endpoints1)获得admin凭证# . admin-openrc 2)创建tacker用户，密码为openstack# openstack user create --domain default --password openstack tacker 3)给tacker用户赋予admin权限# openstack role add --project service --user tacker admin 4)创建tacker服务# openstack service create --name tacker \ --description &quot;Tacker Project&quot; nfv-orchestration 5)创建endpoints123456# openstack endpoint create --region RegionOne nfv-orchestration \ public http://controller:9890/# openstack endpoint create --region RegionOne nfv-orchestration \ internal http://controller:9890/# openstack endpoint create --region RegionOne nfv-orchestration \ admin http://controller:9890/ 3、下载Tacker源码# git clone https://github.com/openstack/tacker -b stable/ocata 4、安装Tacker环境依赖包# cd tacker # pip install -r requirements.txt 5、安装Tacker# python setup.py install 6、创建Tacker日志文件夹# mkdir -p /var/log/tacker 7、生成配置文件# ./tools/generate_config_file_sample.sh 这时生成的配置文件在etc/tacker/tacker.conf.sample，需要将其重命名为tacker.conf # mv etc/tacker/tacker.conf.sample etc/tacker/tacker.conf 8、修改配置文件1234567891011121314151617181920212223242526272829303132333435363738# vi etc/tacker/tacker.conf[DEFAULT]auth_strategy = keystonepolicy_file = /usr/local/etc/tacker/policy.jsondebug = Trueuse_syslog = Falsebind_host = 10.0.0.11bind_port = 9890service_plugins = nfvo,vnfmstate_path = /var/lib/tacker...[nfvo]vim_drivers = openstack[keystone_authtoken]memcached_servers = 11211region_name = RegionOneauth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultusername = tackerproject_name = servicepassword = openstackauth_url = http://controller:35357auth_uri = http://controller:5000...[agent]root_helper = sudo /usr/local/bin/tacker-rootwrap /usr/local/etc/tacker/rootwrap.conf[database]connection = mysql://tacker:openstack@controller:3306/tacker?charset=utf8[tacker]monitor_driver = ping,http_ping 9、复制配置文件到配置文件夹# cp etc/tacker/tacker.conf /usr/local/etc/tacker/ 10、初始化数据库信息# /usr/local/bin/tacker-db-manage --config-file /usr/local/etc/tacker/tacker.conf upgrade head 二、安装Tacker client1、下载Tacker-client源码# git clone https://github.com/openstack/python-tackerclient -b stable/ocata 2、安装Tacker-client模块# cd python-tackerclient # python setup.py install 三、安装Tacker horizon1、下载Tacker-horizon源码# git clone https://github.com/openstack/tacker-horizon -b stable/ocata 2、安装Tacker-horizon模块# cd tacker-horizon # python setup.py install 安装好tacker-horizon后，admin用户登录dashboard界面就可以看到Tacker相关的VNFM和NFVO，如图： 四、开启Tacker server打开一个新的终端，开启Tacker-server，因为Tacker-server的程序会独占这个终端。123sudo python /usr/local/bin/tacker-server \ --config-file /usr/local/etc/tacker/tacker.conf \ --log-file /var/log/tacker/tacker.log 需注意的一个问题： 在安装完Tacker而没有装Mistral时创建VIM的结果如下：1234root@controller:/home/openstack# tacker vim-register --is-default --config-file config.yaml test_vimThe resource could not be found.或者是这种错误：Expecting to find domain in project. The server could not comply with the request since it is either malformed or otherwise incorrect. The client is assumed to be in error. 经过查阅资料，知道这个问题是因为Tacker在创建VIM时要调用Mistral而造成的。所以在使用tacker之前需要先安装好Mistral（可以在安装tacker前安装Mistral，也可以在tacker安装之后安装Mistral，后续还需继续了解）。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Tacker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM虚拟机部署openstack的网络配置]]></title>
    <url>%2F2017%2F12%2F03%2FKVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%83%A8%E7%BD%B2openstack%E7%9A%84%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[概述这篇文章记录的是按照官方文档在KVM环境下部署双节点openstack过程中，前期准备KVM环境和网络配置相关的内容，在完成这篇博客涉及到的工作之后就可以按照官方文档手动安装openstack了。本文涉及的主要工作，首先是在服务器的ubuntu 16.04 desktop版系统上搭建kvm环境，然后在服务器上安装VNC远程桌面，最后在KVM环境中开启两台虚拟机，分别两张网卡，第一张网卡使用桥接模式，第二张网卡使用NAT模式。下面开始介绍一下这个过程。 服务器搭建KVM环境查看CPU是否支持KVM$ egrep -c &quot;(svm|vmx)&quot; /proc/cpuinfo 输出结果大于0证明CPU支持KVM虚拟化 安装KVM及相关依赖包$ sudo apt-get install qemu-kvm qemu virt-manager virt-viewer libvirt-bin bridge-utils 启用桥接网络在服务器上启用桥接网络需要配置一个桥接设备br0，配置br0有两种方式，通过手动配置和通过修改文件配置。 通过手动配置 创建br0网桥 # brctl addbr br0 将eth0网卡添加到br0上，此时可能会断网 # brctl addif br0 eth0 删除eth0上的IP地址 # ip addr del dev eth0 192.168.1.25/24 配置br0的IP地址并启动br0网桥设备 # ifconfig br0 192.168.1.25/24 up 重新加入默认网关 # route add default gw 192.168.1.1 查看配置是否生效 12345# route //查看默认网关，输出结果如下Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 192.168.1.1 0.0.0.0 UG 0 0 0 br0192.168.1.0 * 255.255.255.0 U 0 0 0 br0 123456789101112131415161718192021222324252627# ifconfig //查看eth0和br0的IP信息，输出结果如下，可以发现现在br0有IP而eth0没有IP了br0 Link encap:Ethernet HWaddr 00:e0:81:e2:3c:3d inet addr:192.168.1.25 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::2e0:81ff:fee2:3c3d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1316822 errors:0 dropped:5787 overruns:0 frame:0 TX packets:365475 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:581279124 (581.2 MB) TX bytes:562586852 (562.5 MB) eth0 Link encap:Ethernet HWaddr 00:e0:81:e2:3c:3d inet6 addr: fe80::2e0:81ff:fee2:3c3d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6671034 errors:0 dropped:9627 overruns:0 frame:0 TX packets:840972 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1346523816 (1.3 GB) TX bytes:614510541 (614.5 MB) Memory:dfb80000-dfbfffff lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:1450290 errors:0 dropped:0 overruns:0 frame:0 TX packets:1450290 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:24027042487 (24.0 GB) TX bytes:24027042487 (24.0 GB) 这就是通过手动来配置桥接设备br0的方法，这种方法在配置好之后马上就生效了，但是在系统重启之后这些配置信息都会被清除，要想使配置永久生效则需要修改网络配置文件，也就是下面的方法。 通过修改文件配置 修改前先将网络配置文件进行备份 # cp /etc/network/interfaces /etc/network/interfaces.bak 修改网络配置文件/etc/network/interfaces # vi /etc/network/interfaces //修改结果如下 1234567891011121314auto loiface lo inet loopback # Enabing Bridge networking br0 interfaceauto br0iface br0 inet staticaddress 192.168.1.25network 192.168.1.0netmask 255.255.255.0broadcast 192.168.1.255gateway 192.168.1.1dns-nameservers 223.5.5.5bridge_ports eth0bridge_stp off 保存后退出，关机重启中配置文件就生效了。这种方法只需要修改配置文件然后重启就可以，比较简单，而且是永久生效，比较符合我们的需求，因为我们的虚拟机通过桥接模式连接外网的话都是连接到br0上的。 修改virbr0的网段在服务器上安装好虚拟化软件后，KVM会自动生成一个virbr0的桥接设备，它的作用是为连接其上的虚拟网卡提供NAT访问外网的功能，并提供DHCP服务。virbr0默认分配的IP是192.168.122.1，使用 ifconfig 命令查看得virbr0的信息如下： 12345678910$ ifconfig…… virbr0 Link encap:Ethernet HWaddr 52:54:00:f8:70:e3 inet addr:192.168.122.1 Bcast:192.168.122.255 Mask:255.255.255.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 在这种情况下，连接到virbr0上的虚拟机的虚拟网卡也是在192.168.122.0网段上的，如果让连接到virbr0上的虚拟网卡在自定义的网段上就需要修改virbr0的网段，修改方法如下： # virsh net-edit default 123456789101112&lt;network&gt; &lt;name&gt;default&lt;/name&gt; &lt;uuid&gt;91cc230a-bf53-487c-b296-10323705d7e8&lt;/uuid&gt; &lt;forward mode=&apos;nat&apos;/&gt; &lt;bridge name=&apos;virbr0&apos; stp=&apos;on&apos; delay=&apos;0&apos;/&gt; &lt;mac address=&apos;52:54:00:f8:70:e3&apos;/&gt; &lt;ip address=&apos;10.0.0.1&apos; netmask=&apos;255.255.255.0&apos;&gt; &lt;dhcp&gt; &lt;range start=&apos;10.0.0.2&apos; end=&apos;10.0.0.254&apos;/&gt; &lt;/dhcp&gt; &lt;/ip&gt;&lt;/network&gt; 这样就将virbr0的网段改成10.0.0.0/24，连接到virbr0的虚拟网卡的IP将会在10.0.0.2/24 - 10.0.0.254/24范围内自动分配一个。如果有需要可以自己手动给虚拟网卡配置IP并写到配置文件中去。 服务器安装VNC远程桌面因为服务器上安装的Ubuntu 16.04 LTS desktop版的系统，在后续的工作中需要远程登录到服务器，虽然可以通过SSH远程管理服务器，但是可视化的界面往往会给新手用户提供很大的便利，所以可以在服务器上安装VNC。开始在服务器上安装VNC试过很多方法，VNC服务器端也有多种选择，如VNC4server、tigervncserver，感觉很麻烦，而且还不一定能安装成功，我安装的VNC服务器端是x11VNC，按照步骤可以很顺利地完成安装，步骤如下： 安装x11VNC软件包$ sudo apt-get install x11vnc 配置访问密码$ sudo x11vnc -storepasswd /etc/x11vnc.pass 创建服务# vi /lib/systemd/system/x11vnc.service //粘贴一下代码，最后:wq 保存，请使用root用户，否则没有权限 12345678[Unit]Description=Start x11vnc at startup.After=multi-user.target[Service]Type=simpleExecStart=/usr/bin/x11vnc -auth guess -forever -loop -noxdamage -repeat -rfbauth /etc/x11vnc.pass -rfbport 5900 -shared[Install]WantedBy=multi-user.target 配置防火墙，配置和启动服务# ufw allow 5900 # systemctl enable x11vnc.service # systemctl daemon-reload 完成这四个步骤然后重启就可以了。（这个VNC的安装过程可以参考http://blog.csdn.net/longhr/article/details/51657610） 最后在你自己的电脑需要有一个vnc viewer的软件，可以在这里下载（链接：https://pan.baidu.com/s/1o8kPqXG 密码：v5r2） 创建VM并配置相关信息在安装好VNC后就可以登录服务器的远程桌面，打开一个terminal，在terminal中输入下面的命令可以打开Virtual Machine Manager（注意，使用SSH远程登录服务器是无法打开virt-manager的界面的，一定要在登录了远程桌面后才能打开界面） 使用Virtual Machine Manager的界面可以很方便的创建虚拟机，当然也可以在命令行中使用命令创建虚拟机，这个我就不在这里说了。 按照Openstack官网安装文档的主机网络配置两台虚拟机Controller和Compute 控制节点和计算节点这两个虚拟机分别两张网卡，一张配置为桥接模式，另一张配置为NAT模式。创建虚拟机时默认是添加一张网卡的，后面可以在虚拟机的硬件信息中添加。两张虚拟网卡的配置信息如图： 上图显示的是桥接模式网卡的配置信息，Network source选择为Bridge br0：Host device eth0 上图显示的是NAT模式网卡的配置信息，Network source选择为Virtual network ‘default’：NAT 这样按照官方文档部署双节点Openstack的前期准备工作就已经做完，后面就可以按照官方文档开始安装openstack了，祝你成功。附上官方文档链接https://docs.openstack.org/ocata/zh_CN/install-guide-ubuntu/index.html （注：这个是在ubuntu系统下安装Ocata版本Openstack中文文档）]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>Openstack</tag>
        <tag>vnc</tag>
        <tag>ubuntu</tag>
        <tag>Virtual Machine</tag>
      </tags>
  </entry>
</search>
