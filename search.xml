<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[给新 Git 账户添加 ssh-key]]></title>
    <url>%2F2019%2F02%2F21%2F%E7%BB%99%E6%96%B0git%E8%B4%A6%E6%88%B7%E6%B7%BB%E5%8A%A0ssh-key%2F</url>
    <content type="text"><![CDATA[背景在使用 git 的时候，git 与远程服务器一般通过 https 进行传输，这种传输方式在每次 push 和 pull 时都需要输入账户和密码，比较麻烦。所以更好的方法是通过 ssh 进行传输，这需要在本机上创建 ssh-key 密钥对，并把其中的公钥添加到远程的 Git 服务器中。但有时又会使用到多个 git 账户登录不同的 git 服务器，所以就涉及到添加 ssh-key 密钥对了。 我的环境中最初是针对 GitHub 的账户设置了 ssh 的密钥对，然后我现在需要针对另一个 git 账户进行设置密钥对，比如牛客网的 git 服务器。 添加操作过程1. 新建 SSH key： 12345678910111213141516171819202122$ cd ~/.ssh$ ssh-keygen -t rsa -C &quot;xxx@email.com&quot;Generating public/private rsa key pair.Enter file in which to save the key (/home/chl/.ssh/id_rsa): id_rsa_nowcoderEnter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in id_rsa_nowcoder.Your public key has been saved in id_rsa_nowcoder.pub.The key fingerprint is:SHA256:hjOHuHTiXqCjTPCBf81owybsdflq/HVWcq4jBZjnCNs xxx@email.comThe key&apos;s randomart image is:+---[RSA 2048]----+| || . o ||o Oo o ||=...oBoo || =.+*+O.S o ||+.o=EB.=.= ||o.* +.o .o . ||.o + o.o. ||o. o ... |+----[SHA256]-----+ 上面设置名称为 id_rsa_nowcoder 2. 新秘钥添加到 SSH agent 中 因为默认只读取 id_rsa，为了让 SSH 识别新的私钥，需将其添加到 SSH agent 中： 12$ ssh-add ~/.ssh/id_rsa_nowcoderCould not open a connection to your authentication agent. 但是出现了 Could not open a connection to your authentication agent. 的错误，用一下方法解决： 123$ ssh-agent bash$ ssh-add ~/.ssh/id_rsa_nowcoderIdentity added: ~/.ssh/id_rsa_nowcoder (~/.ssh/id_rsa_nowcoder) 3. 在 git 账户中添加 SSH key 登录 git 账户中添加，完成之后，SSH key 就生效了。检测方法： 1$ git clone 你的仓库ssh地址 若这时不再询问密码，说明设置生效。 4. 更改远程仓库地址 修改命令： 1$ git remote set-url origin 你的仓库ssh地址 或者先删后加： 12$ git remote rm origin$ git remote add origin 你的仓库ssh地址 或者修改 config 文件 12$ git config -e修改 url 更改完成之后，再通过 git push 或 git pull 就不需要输入账号和密码了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 v2ray 实现科学上网]]></title>
    <url>%2F2019%2F02%2F15%2Fv2ray%E5%AE%9E%E7%8E%B0%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[前言原本在 vultr 上面购买的 VPS 上面搭建的 VPN 不能用了，具体原因就是 GTW 经过一波加强，可以通过 TCP 阻断来封锁一些用于搭建 VPN 的 VPS。TCP 阻断的结果就是在国内无法通过 tcp 来连接访问国外的 vps，从而在国内无法 ssh 登录 vps，但是使用 ping 工具却能 ping 通被 tcp 阻断的服务器，因为 ping 是基于 ICMP 的。这样给人一种 vps 没被封的错觉。 当 vps 被 TCP 阻断时，原本基于 shadowsocks 的科学上网方式就不能使用了，本文就是针对被 TCP 阻断的 vps，通过 v2ray 来实现科学上网。当然对于没有被 TCP 阻断的 vps 使用该方法来搭梯子也不容易被封。 实现过程1. 服务器安装在 Linux 操作系统， V2Ray 的安装有脚本安装、手动安装、编译安装 3 种方式，选择其中一种即可，本指南仅提供使用使用脚本安装的方法，并仅推荐使用脚本安装，该脚本由 V2Ray 官方提供。该脚本仅可以在 Debian 系列或者支持 Systemd 的 Linux 操作系统使用。 本文基于 CentOS 7 服务器来实现。 首先下载脚本： 1234567891011# wget https://install.direct/go.sh--2019-02-15 07:53:34-- https://install.direct/go.shResolving install.direct (install.direct)... 104.27.174.71, 104.27.175.71, 2606:4700:30::681b:af47, ...Connecting to install.direct (install.direct)|104.27.174.71|:443... connected.HTTP request sent, awaiting response... 200 OKLength: unspecified [text/plain]Saving to: ‘go.sh’ [ &lt;=&gt; ] 13,915 --.-K/s in 0s 2019-02-15 07:53:34 (58.3 MB/s) - ‘go.sh’ saved [13915] 然后执行安装脚本： 1234567891011121314151617181920212223242526272829303132333435363738# sudo bash go.shInstalling V2Ray v4.16.0 on x86_64Downloading V2Ray: https://github.com/v2ray/v2ray-core/releases/download/v4.16.0/v2ray-linux-64.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 608 0 608 0 0 1330 0 --:--:-- --:--:-- --:--:-- 1333100 10.2M 100 10.2M 0 0 1723k 0 0:00:06 0:00:06 --:--:-- 2356kUpdating software repohttp://mirrors.syringanetworks.net/fedora-epel/7/x86_64/repodata/5a7d69681e5cfd3ae41829a733077e717b512d3dee2d802edeeb206b7d8bda33-updateinfo.xml.bz2: [Errno 14] HTTP Error 404 - Not FoundTrying other mirror.To address this issue please refer to the below wiki article https://wiki.centos.org/yum-errorsIf above article doesn&apos;t help to resolve this issue please use https://bugs.centos.org/.Installing unzipExtracting V2Ray package to /tmp/v2ray.Archive: /tmp/v2ray/v2ray.zip inflating: /tmp/v2ray/config.json creating: /tmp/v2ray/doc/ inflating: /tmp/v2ray/doc/readme.md inflating: /tmp/v2ray/geoip.dat inflating: /tmp/v2ray/geosite.dat creating: /tmp/v2ray/systemd/ inflating: /tmp/v2ray/systemd/v2ray.service creating: /tmp/v2ray/systemv/ inflating: /tmp/v2ray/systemv/v2ray inflating: /tmp/v2ray/v2ctl extracting: /tmp/v2ray/v2ctl.sig inflating: /tmp/v2ray/v2ray extracting: /tmp/v2ray/v2ray.sig inflating: /tmp/v2ray/vpoint_socks_vmess.json inflating: /tmp/v2ray/vpoint_vmess_freedom.json PORT:40827UUID:505f001d-4aa8-4519-9c54-6b65749ee3fbCreated symlink from /etc/systemd/system/multi-user.target.wants/v2ray.service to /etc/systemd/system/v2ray.service.V2Ray v4.16.0 is installed. 看到类似于这样的提示就算安装成功了。如果安装不成功脚本会有红色的提示语句，这个时候你应当按照提示除错，除错后再重新执行一遍脚本安装 V2Ray。 在上面的提示中，有一行 “PORT:40827” 代表着端口号为 40827，还有一行 “UUID:505f001d-4aa8-4519-9c54-6b65749ee3fb” 代表着 id 为 505f001d-4aa8-4519-9c54-6b65749ee3fb。这两个都是随机生成的，不用担心跟别人撞上了。 安装完之后，使用以下命令启动 V2Ray： 1# systemctl start v2ray 在首次安装完成之后，V2Ray 不会自动启动，需要手动运行上述启动命令。 2. 服务端配置v2ray 的配置文件位于 /etc/v2ray/config.json。v2ray 相对于 shadowsocks 更复杂的地方就在于其有众多的配置选项，针对不同的应用场景有不同的配置方案，从而实现不同的功能，而 shadowsocks 则相对傻瓜一些。要详细讲述 v2ray 的所有配置选项是需要很长的内容的，本文针对被 TCP 阻断的 VPS 来实现 vpn 的场景。下面简单介绍一些配置内容： VMess VMess 协议是由 V2Ray 原创并使用于 V2Ray 的加密传输协议，如同 Shadowsocks 一样为了对抗墙的深度包检测而研发的。在 V2Ray 上客户端与服务器的通信主要是通过 VMess 协议通信。 V2Ray 使用 inbound(传入) 和 outbound(传出) 的结构，这样的结构非常清晰地体现了数据包的流动方向，同时也使得 V2Ray 功能强大复杂的同时而不混乱，清晰明了。形象地说，我们可以把 V2Ray 当作一个盒子，这个盒子有入口和出口(即 inbound 和 outbound)，我们将数据包通过某个入口放进这个盒子里，然后这个盒子以某种机制（这个机制其实就是路由，后面会讲到）决定这个数据包从哪个出口吐出来。以这样的角度理解的话，V2Ray 做客户端，则 inbound 接收来自浏览器数据，由 outbound 发出去(通常是发到 V2Ray 服务器)；V2Ray 做服务器，则 inbound 接收来自 V2Ray 客户端的数据，由 outbound 发出去(通常是如 Google 等想要访问的目标网站)。 mKCP V2Ray 引入了 KCP 传输协议，并且做了一些不同的优化，称为 mKCP。 mKCP 使用 UDP 来模拟 TCP 连接，这样即使 vps 被 TCP 阻断了，还是能够通过 UDP 来连接。mKCP 牺牲带宽来降低延迟，传输同样的内容，mKCP 一般比 TCP 消耗更多的流量，但是对于我购买的 vps 流量一般都用的很少，每个月用不完 十分之一，所以采用 mKCP 对流量消耗也没有太大的问题。 服务端采用 vmess + mKCP 的配置时，配置文件 /etc/v2ray/config.json 的内容如下： 123456789101112131415161718192021222324252627282930313233&#123; &quot;inbounds&quot;: [ &#123; &quot;port&quot;: 40827, &quot;protocol&quot;: &quot;vmess&quot;, &quot;settings&quot;: &#123; &quot;clients&quot;: [ &#123; &quot;id&quot;: &quot;505f001d-4aa8-4519-9c54-6b65749ee3fb&quot;, &quot;alterId&quot;: 64 &#125; ] &#125;, &quot;streamSettings&quot;: &#123; &quot;network&quot;: &quot;mkcp&quot;, &quot;kcpSettings&quot;: &#123; &quot;uplinkCapacity&quot;: 5, &quot;downlinkCapacity&quot;: 100, &quot;congestion&quot;: true, &quot;header&quot;: &#123; &quot;type&quot;: &quot;none&quot; &#125; &#125; &#125; &#125; ], &quot;outbounds&quot;: [ &#123; &quot;protocol&quot;: &quot;freedom&quot;, &quot;settings&quot;: &#123;&#125; &#125; ]&#125; 修改完配置文件后，需要重新启动 v2ray： 1# systemctl restart v2ray 最后，还需要修改 vps 的防火墙设置，将对应的 udp 端口放行： 12# firewall-cmd --zone=public --add-port=40827/udp --permanent# firewall-cmd --reload 再设置 v2ray 开机自启动，修改 /etc/rc.local 文件，添加： 1systemctl restart v2ray 服务端配置完毕。 3. 客户端安装下载客户端，windows 系统下有两种客户端，一种是不带图形界面的，另一种是在其基础上增加了图形界面的。为了方便，使用带有图形界面的版本。下载连接：v2rayN，下载 Lastest release 的 v2rayN.zip 压缩包。 解压后运行 v2rayN.exe 程序，双击任务栏图标打开界面： 点击上方检查更新，检查更新 v2rayCore，点击“是”。 更新完后，点击服务器，添加[VMess]服务器，如下配置： 配置完成点击确定，右键任务栏 v2rayN 图标，点击启用http代理，http 代理模式可选择 PAC模式 或者 全局模式。 这样，客户端的配置也完成，即可开始科学上网。 其他内容 非图形界面客户端配置： 如果客户端没有使用带有图形界面的 v2rayN，其配置稍微复杂一点，需要自己修改客户端配置文件 config.json，配置内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;inbounds&quot;: [ &#123; &quot;port&quot;: 1080, &quot;protocol&quot;: &quot;socks&quot;, &quot;sniffing&quot;: &#123; &quot;enabled&quot;: true, &quot;destOverride&quot;: [&quot;http&quot;, &quot;tls&quot;] &#125;, &quot;settings&quot;: &#123; &quot;auth&quot;: &quot;noauth&quot; &#125; &#125; ], &quot;outbounds&quot;: [ &#123; &quot;protocol&quot;: &quot;vmess&quot;, &quot;settings&quot;: &#123; &quot;vnext&quot;: [ &#123; &quot;address&quot;: &quot;xxx.xxx.xxx.xxx&quot;, &quot;port&quot;: 40827, &quot;users&quot;: [ &#123; &quot;id&quot;: &quot;505f001d-4aa8-4519-9c54-6b65749ee3fb&quot;, &quot;alterId&quot;: 64 &#125; ] &#125; ] &#125;, &quot;streamSettings&quot;: &#123; &quot;network&quot;: &quot;mkcp&quot;, &quot;kcpSettings&quot;: &#123; &quot;uplinkCapacity&quot;: 5, &quot;downlinkCapacity&quot;: 100, &quot;congestion&quot;: true, &quot;header&quot;: &#123; &quot;type&quot;: &quot;none&quot; &#125; &#125; &#125; &#125; ]&#125; 配置完 v2ray 并启动之后，还需要配合浏览器的一些代理插件来实现科学上网，如谷歌浏览器与火狐浏览器的 SwitchyOmega 插件，如何使用这里不介绍。 参考链接 v2ray 配置指南：https://toutyrater.github.io v2ray配置模板：https://github.com/KiriKira/vTemplate]]></content>
      <categories>
        <category>VPN</category>
      </categories>
      <tags>
        <tag>v2ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm join success but node not joined]]></title>
    <url>%2F2018%2F12%2F18%2Fkubernetes-node-not-joined%2F</url>
    <content type="text"><![CDATA[问题在搭建 Kubernetes 集群时，遇到这样一个问题，就是在 node 节点上使用 kubeadm join 时能够成功的加入节点，但是在 master 节点上却无法查看集群中的 node 节点。如下： 123456789101112131415node1$ sudo kubeadm join --token 1bc310.cb323487a828849e 10.2.7.114:6443 --discovery-token-ca-cert-hash sha256:3d39f8fe34a043ccef4821014fd6d3e0f222614d37d59a6e4944c74f257c6d4d[preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system path[discovery] Trying to connect to API Server &quot;10.2.7.114:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://10.2.7.114:6443&quot;[discovery] Requesting info from &quot;https://10.2.7.114:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;10.2.7.114:6443&quot;[discovery] Successfully established connection with API Server &quot;10.2.7.114:6443&quot;This node has joined the cluster:* Certificate signing request was sent to master and a response was received.* The Kubelet was informed of the new secure connection details.Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster. 123master$ kubectl get nodesNAME STATUS ROLES AGE VERSIONubuntu Ready master 4h v1.9.1 解决方法出现这个问题的原因是 node 节点的主机名与 master 节点的相同，因此需要给 所有的 node 节点取与 master 节点不同的主机名。 修改 /etc/hostname 以及 /etc/hosts 文件中的主机名，再通过命令临时设置主机名：sudo hostname 主机名。 配置完之后，在 node 节点上执行 kubeadm reset， 再重新执行 kubeadm join 。 最后在 master 节点上查看节点： 12345$ kubectl get nodesNAME STATUS ROLES AGE VERSIONnode1 Ready &lt;none&gt; 1h v1.9.1node2 Ready &lt;none&gt; 1h v1.9.1ubuntu Ready master 6h v1.9.1 可以看到有两个 node 节点。 参考：issue 61224]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Bugs</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 VPN 无法打开谷歌学术的问题]]></title>
    <url>%2F2018%2F12%2F17%2F%E8%A7%A3%E5%86%B3vpn%E6%97%A0%E6%B3%95%E6%89%93%E5%BC%80%E8%B0%B7%E6%AD%8C%E5%AD%A6%E6%9C%AF%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题之前搭好的 VPN 用的好好的，最近两天突然不能登录谷歌学术（Google Scholar），但是还能使用谷歌搜索等其他国外的网站。登录谷歌学术网站出现：We&#39;re sorry...... but your computer or network may be sending automated queries. To protect our users, we can&#39;t process your request right now. 原因出现这个问题是因为我使用的是 Vultr 的 VPS 来搭建 shadowsocks 服务端，而且有很多人都在这些 VPS 服务商（Vultr，搬瓦工，DigitalOcean，Linode等等），然后有人使用这些公网的 IP 段来做爬虫，所以 Google 把这些公网 IP 给封了。但是一般封的都是 IPv4 的地址，IPv6 的地址一般没有被封，所以可以考虑使用 IPv6 来访问谷歌学术的网站。 解决方法 首先，找到最新的 Google IPv6 地址，可以在这里查看：IPv6-hosts 找到 Google 学术对应的 IPv6 地址后，修改服务器的 hosts 文件，vim /etc/hosts，在文件的最后加入如下配置： 12345## Scholar 学术搜索2404:6800:4008:c06::be scholar.google.com2404:6800:4008:c06::be scholar.google.com.hk2404:6800:4008:c06::be scholar.google.com.tw2404:6800:4005:805::200e scholar.google.cn #www.google.cn 然后，重启 shadowsocks 服务端程序： 1/etc/init.d/shadowsocks restart 注：不同的搭建方式可能重启的方式不太一样。我的搭建方式请参考：shadowsocks服务端搭建 之后，就能够正常访问谷歌学术的网站了。 其他问题有些时候，到这里可能还是不能正常访问谷歌学术。这个可能是因为 Vultr 上 5 美元的 VPS 默认使用的是 IPv4 的地址，而没有启用 IPv6 的地址，所以需要给你的 VPS 分配一个 IPv6 的地址。 Server Information -&gt; Settings -&gt; IPv6：给你的 Server 分配（assign）一个 IPv6 地址，分配一个 IPv6 地址是不要钱的。这个过程会重启你的 VPS，重启之后，能看到有一个 IPv6 的地址，如下图： 我一开始就是因为没有给我的 VPS 分配 IPv6 地址，所以修改了服务器的 hosts 文件也还是没有解决这个问题。你在解决问题的时候注意要先分配 IPv6 地址哦！]]></content>
      <categories>
        <category>VPN</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>Bugs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger Caliper Disable TLS]]></title>
    <url>%2F2018%2F12%2F15%2Ffabric-disable-tls%2F</url>
    <content type="text"><![CDATA[前言最近在使用 Hyperledger Caliper 时，想通过 wireshark 抓包来分析 fabric 运行流程中各阶段的数据信息，但是发现 fabric 节点间的通信使用了传输层安全（Transport Layer Security，TLS）协议，使得通信的报文的内容在抓包后无法分析。因此考虑在测试环境中暂时关闭 TLS，从而能够直接查看报文中承载的数据内容。 实现过程1. 在 docker-compose 的配置文件中修改环境变量本实验是在 Hyperledger Caliper 的测试环境中进行的，Caliper 测试工具在运行初始阶段会调用 docker-compose 启动 fabric 的网络，启动的 fabric 默认启用了 TLS，可以在其 docker-compose 的启动配置文件 docker-compose.yaml 中看到环境变量： FABRIC_CA_SERVER_TLS_ENABLED=true ORDERER_GENERAL_TLS_ENABLED=true CORE_PEER_TLS_ENABLED=true 以上三个环境变量都设置为 true。如果要 disable TLS，则需在配置文件 docker-compose.yaml 中将这三个环境变量都注释掉，或者将它们设置为 false。即： FABRIC_CA_SERVER_TLS_ENABLED=false ORDERER_GENERAL_TLS_ENABLED=false CORE_PEER_TLS_ENABLED=false 2. 修改 benchmark 中的 fabric.json 文件在 benchmark 中的每个例子中，如 simple 的网络配置文件 fabric.json 中，client 与 peer 、orderer、ca 等节点都是通过 grpcs 或 https 来通信的，而这是在使用了 TLS 时的通信方式，因此需要将其改为 grpc 或 http 来通信。修改入下： orderer.url：grpcs://localhost:7050 ==&gt; grpc://localhost:7050 ca.url：https://localhost:7054 ==&gt; http://localhost:7054 peer1.requests：grpcs://localhost:7051 ==&gt; grpc://localhost:7051 peer1.events：grpcs://localhost:7053 ==&gt; grpc://localhost:7053 其他的都是如此修改。 如果用 vim 编辑器的话，可以快捷的使用全局替换功能，在 normal 模式下输入冒号： 12&gt; :1,$ s/grpcs/grpc/g #表示将第一行到最后一行间的所有grpcs替换成grpc&gt; 3. 错误记录如果仅仅修改 docker-compose.yaml 文件中的环境变量，没有修改 fabric.json 中的通信方式的话，则在运行测试时会出现如下错误： 123456789101112131415161718192021222324252627282930313233# create mychannel......E1215 12:26:25.877864366 9327 ssl_transport_security.cc:989] Handshake failed with fatal error SSL_ERROR_SSL: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number.E1215 12:26:25.879670836 9327 ssl_transport_security.cc:989] Handshake failed with fatal error SSL_ERROR_SSL: error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number.error: [Orderer.js]: sendBroadcast - on error: &quot;Error: 14 UNAVAILABLE: Connect Failed\n at createStatusError (/home/user1/caliper/node_modules/grpc/src/client.js:64:15)\n at ClientDuplexStream._emitStatusIfDone (/home/user1/caliper/node_modules/grpc/src/client.js:270:19)\n at ClientDuplexStream._readsDone (/home/user1/caliper/node_modules/grpc/src/client.js:236:8)\n at readCallback (/home/user1/caliper/node_modules/grpc/src/client.js:296:12)&quot;not ok 1 Failed to create channels Error: SERVICE_UNAVAILABLE at ClientDuplexStream.&lt;anonymous&gt; (/home/user1/caliper/node_modules/fabric-client/lib/Orderer.js:136:21) at emitOne (events.js:116:13) at ClientDuplexStream.emit (events.js:211:7) at ClientDuplexStream._emitStatusIfDone (/home/user1/caliper/node_modules/grpc/src/client.js:271:12) at ClientDuplexStream._readsDone (/home/user1/caliper/node_modules/grpc/src/client.js:236:8) at readCallback (/home/user1/caliper/node_modules/grpc/src/client.js:296:12) --- operator: fail at: channels.reduce.then.then.catch (/home/user1/caliper/src/fabric/create-channel.js:159:19) stack: |- Error: Failed to create channels Error: SERVICE_UNAVAILABLE at ClientDuplexStream.&lt;anonymous&gt; (/home/user1/caliper/node_modules/fabric-client/lib/Orderer.js:136:21) at emitOne (events.js:116:13) at ClientDuplexStream.emit (events.js:211:7) at ClientDuplexStream._emitStatusIfDone (/home/user1/caliper/node_modules/grpc/src/client.js:271:12) at ClientDuplexStream._readsDone (/home/user1/caliper/node_modules/grpc/src/client.js:236:8) at readCallback (/home/user1/caliper/node_modules/grpc/src/client.js:296:12) at Test.assert [as _assert] (/home/user1/caliper/node_modules/tape/lib/test.js:224:54) at Test.bound [as _assert] (/home/user1/caliper/node_modules/tape/lib/test.js:76:32) at Test.fail (/home/user1/caliper/node_modules/tape/lib/test.js:317:10) at Test.bound [as fail] (/home/user1/caliper/node_modules/tape/lib/test.js:76:32) at channels.reduce.then.then.catch (/home/user1/caliper/src/fabric/create-channel.js:159:19) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) ...fabric.init() failed, Error: Fabric: Create channel failed at channels.reduce.then.then.catch (/home/user1/caliper/src/fabric/create-channel.js:160:31) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7)[Transaction Info] - Submitted: 0 Succ: 0 Fail:0 Unfinished:0unexpected error, Error: Fabric: Create channel failed at channels.reduce.then.then.catch (/home/user1/caliper/src/fabric/create-channel.js:160:31) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) 这个问题需要注意。 结果在 TLS 被开启或关闭两种情况下，能够发现关闭 TLS 后，系统的吞吐率略有提升，这是可想而知的，毕竟减少了一层传输层安全协议的封装。结果图如下： Enable TLS Disable TLS 不过以上的结果在实际中的意义并不大，因为在实际应用中肯定需要进行传输层安全协议的封装，不然这区块链的安全从和谈起。 如下图可以看到关闭 TLS 后，抓包后能够查看数据内容： 太不安全了！所以以上内容均只能应用于测试。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Hyperledger fabric</tag>
        <tag>Hyperledger caliper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 容器启动时端口映射失败]]></title>
    <url>%2F2018%2F12%2F05%2Fdocker%E5%AE%B9%E5%99%A8%E5%90%AF%E5%8A%A8%E6%97%B6%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[介绍在一台虚拟机上创建容器时因为端口映射的问题而导致容器启动失败，并提示 docker: Error response from daemon: driver failed programming external connectivity on endpoint orderer.example.com (ae62c5d74521cc7ea21dc4d4c762cf09390839a1a21d8dcfdcb3784ecdc5e568): Bind for 0.0.0.0:7050 failed: port is already allocated. 1. 错误发现过程 通过 docker-compose 启动一个容器是提示无法启动容器，错误原因是 Bind for 0.0.0.0:7050 failed: port is already allocated&#39; 。 1234567$ docker-compose -f docker-compose-orderer.yaml up -dCreating orderer.example.com ... errorERROR: for orderer.example.com Cannot start service orderer.example.com: b&apos;driver failed programming external connectivity on endpoint orderer.example.com (b1253c6e3542219f989fb9f6508c738066aeeb2fcdebd1e13b9b85c63c2715dd): Bind for 0.0.0.0:7050 failed: port is already allocated&apos;ERROR: for orderer.example.com Cannot start service orderer.example.com: b&apos;driver failed programming external connectivity on endpoint orderer.example.com (b1253c6e3542219f989fb9f6508c738066aeeb2fcdebd1e13b9b85c63c2715dd): Bind for 0.0.0.0:7050 failed: port is already allocated&apos;ERROR: Encountered errors while bringing up the project. 接着直接使用 docker run 命令启动容器，还是提示一样的错误。 123$ docker run --name orderer.example.com -d -p 7050:7050 hyperledger/fabric-orderer:x86_64-1.1.0 07c97104c290f470588bf0cfe041f76771bfc8586b3ad0fe784a20f97c4e4a6fdocker: Error response from daemon: driver failed programming external connectivity on endpoint orderer.example.com (ae62c5d74521cc7ea21dc4d4c762cf09390839a1a21d8dcfdcb3784ecdc5e568): Bind for 0.0.0.0:7050 failed: port is already allocated. 查看主机的端口是否被占用： 12345678$ netstat -ntlp(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp6 0 0 :::22 :::* LISTEN - tcp6 0 0 :::2375 :::* LISTEN - 发现并没有 7050 端口并没有在使用。 启动另一个容器，绑定主机的另一个端口： 12$ docker run -d -p 8050:7050 hyperledger/fabric-orderer:x86_64-1.1.0 fba31d37ff4cc409740ce8cd045f4f4bc6a76f7c69c454a2d7e380327613acb0 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfba31d37ff4c hyperledger/fabric-orderer:x86_64-1.1.0 &quot;orderer&quot; 8 seconds ago Up 7 seconds 0.0.0.0:8050-&gt;7050/tcp confident_wescoff 可以启动。 查看 docker 服务状态： 1234567891011121314$ systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/docker.service.d └─override.conf Active: active (running) since Wed 2018-12-05 09:29:11 CST; 1h 14min ago Docs: https://docs.docker.com Main PID: 987 (dockerd) Tasks: 35 Memory: 161.2M CPU: 54.874s CGroup: /system.slice/docker.service ├─ 987 /usr/bin/dockerd └─1089 docker-containerd --config /var/run/docker/containerd/containerd.toml 服务显示是正常运行的。 2.解决方法网上查看了一些方法，可以通过重启 docker 服务来解决： 1$ sudo systemctl restart docker 真的解决了。 123456$ docker run --name orderer.example.com -d -p 7050:7050 hyperledger/fabric-orderer:x86_64-1.1.0 2f289e5225344e4e07230a0985e9f51f2ef6584af263be28388bdb7f6c80af35$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2f289e522534 hyperledger/fabric-orderer:x86_64-1.1.0 &quot;orderer&quot; 7 seconds ago Up 6 seconds 0.0.0.0:7050-&gt;7050/tcp orderer.example.com]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Bugs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UNP练习：确定主机字节序]]></title>
    <url>%2F2018%2F11%2F08%2F%E7%A1%AE%E5%AE%9A%E4%B8%BB%E6%9C%BA%E5%AD%97%E8%8A%82%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[确定主机字节序的程序记录 UNP 学习第三章的确定主机字节序的程序，使用 C++ 语言编写。 程序如下： 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#define CPU_VENDOR_OS "x86_64-unknown-linux-gnu"using namespace std;int main()&#123; union &#123; short s; char c[sizeof(short)]; &#125; un; un.s = 0x0102; cout &lt;&lt; CPU_VENDOR_OS &lt;&lt; ": "; if ( sizeof(short) == 2 )&#123; if ( un.c[0] == 1 &amp;&amp; un.c[1] == 2 ) cout &lt;&lt; "big-endian" &lt;&lt; endl; else if ( un.c[0] == 2 &amp;&amp; un.c[1] == 1 ) cout &lt;&lt; "little-endian" &lt;&lt; endl; else cout &lt;&lt; "unknown" &lt;&lt; endl; &#125; else cout &lt;&lt; "sizeof(short) = " &lt;&lt; sizeof(short) &lt;&lt; endl; return 0;&#125; 在我的 Ubuntu 16.04 虚拟机上运行上面的程序得到的结果是： 1x86_64-unknown-linux-gnu: little-endian 这说明我的虚拟机在内存中存储数据的字节序是 小端字节序。 上面定义的 CPU_VENDOR_OS 宏的值是可在 UNP 随书的源码得到的 config.h 头文件中查看。 大端模式与小端模式理解考虑一个 16 位整数，它由 2 个字节组成。内存中存储这两个字节有两种方法： 一种将低序字节存储在起始地址，这称为 小端字节序； 另一种将高序字节存储在起始地址，这称为 大端字节序。 在上面的程序中，定义了一个联合体变量 un，包含一个短整形变量 s 和一个包含 2 个字符的字符数组 c。首先给 un.s 赋值 0x0102，这样在字符串数组 c 中存放的字符串对应的 ASCII 值为 0x0102 。高序字节为 0x01，低序字节为 0x02 。 如果系统是小端模式： 低序字节存储在起始地址，也就是 0x02 存放在数组的起始地址 un.c；高序字节存储在起始地址+1，即 0x01 存放在 un.c+1 中。即有一下等式： 12un.c[0] == 0x02un.c[1] == 0x01 如果系统是大端模式： 高序字节存储在起始地址，也就是 0x01 存放在数组的起始地址 un.c；低序字节存储在起始地址+1，即 0x02 存放在 un.c+1 中。有以下等式： 12un.c[0] == 0x01un.c[1] == 0x02 对于一个二进制或十六进制的值，如 0x01020304，位于左边的 0x01 字节是高序字节，位于右边 0x04 的是低序字节。]]></content>
      <categories>
        <category>UNIX网络编程</category>
      </categories>
      <tags>
        <tag>大小端模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket编程之bind函数]]></title>
    <url>%2F2018%2F11%2F07%2Fsocket%E7%BC%96%E7%A8%8B%E4%B9%8Bbind%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[bind 函数：关联地址和套接字定义： 12#include &lt;sys/socket.h&gt;int bind(int sockfd, const struct sockaddr *addr, socklen_t len); 返回值：若成功，返回 0；若出错，返回 -1. 使用 bind 时遇到的错误在练习 UNP 代码 daytimetcpsrv.c 时遇到两个问题： Permission denied 这是因为地址中的端口号必须不小于 1024，除非该进程具有相应的特权（即 root 用户）。 Address already in use 这个问题有时会让人很疑问，明明已经结束了使用对应端口的进程，端口应该不是 in use 的啊，但却无法再次调用 bind 函数来绑定该端口到一个套接字端点（bind 函数返回 EADDRINUSE）。其实这是由 TCP 套接字状态 TIME_WAIT 引起的，该状态在套接字关闭后约保留 2 到 4 分钟，因此无法再次绑定刚刚使用的端口。在 TIME_WAIT 状态退出之后，套接字被删除，该地址才能被重新绑定而不出问题。 可以通过 netstat -ant 来查看这个端口还处于 TIME_WAIT 状态： 等待 TIME_WAIT 结束可能是令人恼火的一件事，特别是如果您正在开发一个套接字服务器，就需要停止服务器来做一些改动，然后重启。幸运的是，有方法可以避开 TIME_WAIT 状态。可以给套接字应用 SO_REUSEADDR 套接字选项，以便端口可以马上重用。 对于 daytimetcpsrv.c，可以加上以下代码： 1234567int reuse = 1;listenfd = Socket(AF_INET, SOCK_STREAM, 0);if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;reuse, sizeof(reuse)) &lt; 0) &#123; perror("setsockopet error\n"); return -1; &#125; 重新编译运行。 关于 TIME_WAIT 状态TCP 设计中之所以要让一个旧的连接处于 TIME_WAIT 状态是因为要防止旧连接的老的重复分组出现在新连接中。拿 UNP 上面的例子来说： 假设在 12.106.32.245 的端口 1500 和 206.168.112.219 的端口 21 之间有一个 TCP 连接。当我们关闭这个连接后，很快又重新建立一条相同 IP 和端口的 TCP 连接。在这种情况下，假如旧连接在网络中还存在没有被丢弃的重复分组，而且重复分组又出现在了新连接中了，TCP 将无法正确处理这个分组。为了防止这种情况的发生，TCP 将刚关闭的连接置于 TIME_WAIT 状态，不允许给处于该状态的连接启动新的化身，持续时间是 2MSL，如此将能保证该连接的老的重复分组都已在网络中消逝。 注：是主动执行关闭 TCP 连接的那端将处于 TIME_WAIT 状态。]]></content>
      <categories>
        <category>UNIX网络编程</category>
      </categories>
      <tags>
        <tag>bind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UNP练习：TCP时间获取程序]]></title>
    <url>%2F2018%2F11%2F07%2FTCP%E6%97%B6%E9%97%B4%E8%8E%B7%E5%8F%96%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[《UNIX 网络编程：卷1》学习记录1：书本上有两个程序： 1.2节：简单的 TCP 时间获取客户程序 1.5节：简单的 TCP 时间获取服务器程序 本文记录的是我在学习过程编写的 C++ 版本的程序（但其实还是 C 程序，hahaha…），为了熟悉 UNIX 函数库，这里并没有基于书中的 &lt;unp.h&gt; 头文件来编写，而是全部自己添加 C 函数库中的头文件。程序如下： 简单的 TCP 时间获取客户程序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;iostream&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;cstdlib&gt;#define MAXLINE 4096using namespace std;int main(int argc, char **argv)&#123; int sockfd, n; char recevline[MAXLINE + 1]; // sockaddr_in结构定义在&lt;netinet/in.h&gt;头文件中 struct sockaddr_in servaddr; if (argc != 3)&#123; cerr &lt;&lt; "usage: " &lt;&lt; argv[0] &lt;&lt; " &lt;IPaddress&gt; &lt;port&gt;" &lt;&lt; endl; return 0; &#125; //socket()在&lt;sys/socket.h&gt; if ( (sockfd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0 )&#123; cerr &lt;&lt; "socket error" &lt;&lt; endl; return 0; &#125; bzero(&amp;servaddr, sizeof(servaddr)); //定义在&lt;string.h&gt;头文件中 servaddr.sin_family = AF_INET; servaddr.sin_port = htons(atoi(argv[2])); //头文件&lt;arpa/inet.h&gt; //inet_pton()在头文件&lt;arpa/inet.h&gt; if ( inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr) &lt;= 0 )&#123; cerr &lt;&lt; "inet_pton error for " &lt;&lt; argv[1] &lt;&lt; endl; return 0; &#125; if (connect(sockfd, (struct sockaddr *) &amp;servaddr, sizeof(servaddr)) &lt; 0)&#123; cerr &lt;&lt; "connect error" &lt;&lt; endl; return 0; &#125; while ( (n = read(sockfd, recevline, MAXLINE)) &gt; 0 )&#123; //头文件&lt;unisted.h&gt; recevline[n] = 0; if ( fputs(recevline, stdout) == EOF )&#123; //头文件&lt;stdio.h&gt; cerr &lt;&lt; "fputs error" &lt;&lt; endl; return 0; &#125; &#125; if ( n &lt; 0 )&#123; cerr &lt;&lt; "read error" &lt;&lt; endl; return 0; &#125; return 0;&#125; 简单的 TCP 时间获取服务器程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;cstdlib&gt;#define MAXLINE 4096#define LISTENQ 1024using namespace std;int main(int argc, char **argv)&#123; int listenfd, connfd; struct sockaddr_in servaddr; // sockaddr_in结构定义在&lt;netinet/in.h&gt;头文件中 char buff[MAXLINE]; time_t ticks; if (argc != 2)&#123; cerr &lt;&lt; "usage: " &lt;&lt; argv[0] &lt;&lt; " &lt;listen_port&gt;" &lt;&lt; endl; return 0; &#125; if ( (listenfd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0 )&#123; cerr &lt;&lt; "socket error" &lt;&lt; endl; return 0; &#125; bzero(&amp;servaddr, sizeof(servaddr)); //定义在&lt;string.h&gt;头文件中 servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); //头文件&lt;arpa/inet.h&gt; servaddr.sin_port = htons(atoi(argv[1])); //头文件&lt;arpa/inet.h&gt; //bind()在头文件&lt;sys/socket.h&gt;中 if ( bind(listenfd, (struct sockaddr *) &amp;servaddr, sizeof(servaddr)) &lt; 0 )&#123; cerr &lt;&lt; "bind error" &lt;&lt; endl; return 0; &#125; if (listen(listenfd, LISTENQ) &lt; 0)&#123; //头文件&lt;sys/socket.h&gt; cerr &lt;&lt; "listen error" &lt;&lt; endl; return 0; &#125; for (;;)&#123; if ((connfd = accept(listenfd, (struct sockaddr *) NULL, NULL)) &lt; 0)&#123; cerr &lt;&lt; "accept error" &lt;&lt; endl; return 0; &#125; ticks = time(NULL); snprintf(buff, sizeof(buff), "%.24s\r\n", ctime(&amp;ticks));//c头文件&lt;stdio.h&gt; 或c++头文件&lt;cstdlib&gt; if ( write(connfd, buff, strlen(buff)) &lt; 0 )&#123; //头文件&lt;unisted&gt; cerr &lt;&lt; "write error" &lt;&lt; endl; return 0; &#125; if (close(connfd) &lt; 0)&#123; //头文件&lt;unisted&gt; cerr &lt;&lt; "close error" &lt;&lt; endl; return 0; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>UNIX网络编程</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode-141-142]]></title>
    <url>%2F2018%2F10%2F21%2Fleetcode-141-142%2F</url>
    <content type="text"><![CDATA[141. Linked List CycleGiven a linked list, determine if it has a cycle in it. Follow up:Can you solve it without using extra space? 题意理解给定一个链表，判断链表中是否有环。不要有额外的空间消耗。 解法思路双指针法： 最常用的方法，快慢指针，慢指针每次移动一步，快指针一次移动两步。如果链表中没有环，那么快指针将先到达链表结尾，并且指向 NULL；如果链表中有环，则快指针会在环中循环，直到慢指针进入环中，快指针将会追上慢指针，两者相等。 代码如下： 12345678910111213141516class Solution &#123;public: bool hasCycle(ListNode *head) &#123; if (head == NULL || head-&gt;next == NULL) return false; ListNode* slow = head; ListNode* fast = head-&gt;next; while(fast-&gt;next != NULL &amp;&amp; fast-&gt;next-&gt;next != NULL) &#123; if (slow == fast) return true; slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; &#125; return false; &#125;&#125;; 复杂度分析 时间复杂度：O(N)。如果链表中无环，则在快指针到达链表结尾时经过 N/2 步左右，所以复杂度为 O(N)；如果链表中有环，则最坏情况下，当慢指针几乎绕环一圈时快指针才追上它，所以是 N 步左右，所以复杂度为 O(N)。综上，时间复杂度为 O(N)。 空间复杂度：O(1)。只使用了两个指针。 142. Linked List Cycle IIGiven a linked list, return the node where the cycle begins. If there is no cycle, return null. Note: Do not modify the linked list. Follow up:Can you solve it without using extra space? 题意理解给定一个链表，如果链表中有环，返回环开始的节点；如果没有环，则返回 null。并且不能修改这个链表。 解法思路和上面那题一样，都是使用双指针法，但是要输出环开始的节点需要一些其他的计算。 假设经过 k 步，快慢指针相遇；假设环的长度是 r；所以有：2k - k = nr，即 k = nr （其中n表示快指针在环中循环的次数）。 假设链表表头到环的起点的距离为 s；假设环的起点到快慢指针相遇的节点的距离为 m；所以有：s = k - m。 s = nr - m = (n - 1) + (r - m)，n = 1,2,3,... 所以，可以再使用两个指针，慢指针从链表表头开始，每次移动一步；快指针从快慢指针相遇的节点开始，每次也移动一步，那么经过 s 步之后，快慢指针将会在环的起点相遇。 代码如下： 123456789101112131415161718192021222324class Solution &#123;public: ListNode *detectCycle(ListNode *head) &#123; if (head == NULL || head-&gt;next == NULL) return NULL; ListNode* slow = head; ListNode* fast = head; while (fast != NULL &amp;&amp;fast-&gt;next != NULL)&#123; slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; if (slow == fast)&#123; slow = head; fast = fast; while (slow != fast)&#123; slow = slow-&gt;next; fast = fast-&gt;next; &#125; return slow; &#125; &#125; return NULL; &#125;&#125;; 复杂度和上一题一样，都为 O(N).]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM 从虚拟机磁盘镜像创建虚拟机]]></title>
    <url>%2F2018%2F10%2F19%2F%E4%BB%8E%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E9%95%9C%E5%83%8F%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[概述作为教研室的网管，干了很多运维的工作，而一个高效的运维是必须少做重复性工作。教研室的网管做的最多的事就是在很多服务器上给很多同学分配很多虚拟机，由于同学们没法直接操作服务器，所以有时系统也是需要网管帮忙安装的，而安装系统的过程费时费力且没啥技术含量。所以最好的办法给各种系统保存一份模板镜像，如果同学需要一个虚拟机就给他复制一份，然后从镜像启动虚拟机，无需重复系统安装的过程，一人一个文件夹，方便管理。本文就将介绍如何在 KVM 环境下从磁盘镜像创建虚拟机。 实现过程这里假设保存的磁盘镜像格式为 qcow2 格式的，操作系统为 Linux ，并且后面的操作的磁盘名称为：ubuntu.qcow2。 命令行实现：在命令行下，使用 virt-install 命令来安装虚拟机，这命令有很多选项，这里不详细介绍，可以通过 man virt-install 仔细查看。我们通常安装虚拟机都是通过系统 ISO 文件来安装系统，使用的命令如下： 123456virt-install --virt-type kvm --name ubuntu --ram 4096 --vcpus=4 \ --cdrom=/home/kb310/test/ubuntu-16.04.4-desktop-amd64.iso \ --disk path=/home/kb310/test/ubuntu.qcow2,format=qcow2 \ --network network=br0,model=virtio \ --graphics vnc,listen=0.0.0.0, --noautoconsole \ --os-type=linux 但是以上方法每次都是重新安装系统，太麻烦。 下面介绍从虚拟机磁盘文件创建虚拟机的命令，但是这和 virt-instal 工具的版本有关系（可能和KVM的版本也有关系），不同版本的选项略微有差别，使用如下命令查看版本： 12345678$ virt-install --version1.3.2 或 0.600.4 （我只接触过这两个版本）$ kvm -versionQEMU emulator version 2.5.0 (Debian 1:2.5+dfsg-5ubuntu10.30), Copyright (c) 2003-2008 Fabrice Bellard或# kvm -versionQEMU emulator version 2.0.0 (Debian 2.0.0+dfsg-2ubuntu1.33), Copyright (c) 2003-2008 Fabrice Bellard 一种兼容前后版本的命令如下： 123456virt-install --virt-type kvm --name ubuntu --ram 4096 --vcpus=4 \ --import \ --disk path=/home/kb310/test/ubuntu.qcow2,format=qcow2 \ --network bridge=br0,model=virtio \ --graphics vnc,listen=0.0.0.0 --noautoconsole \ --os-type=linux 只适用于新版本的命令如下： 123456virt-install --virt-type kvm --name docker-$1 --ram 4096 --vcpus=4 \ --cdrom=/home/kb310/test/ubuntu.qcow2 \ --disk path=/home/kb310/test/ubuntu.qcow2,format=qcow2 \ --network bridge=br0,model=virtio \ --graphics vnc,listen=0.0.0.0 --noautoconsole \ --os-type=linux 主要的区别就在于 –import 选项是兼容的，而使用 –cdrom 只适用于新版本。 图形界面实现：对于图形界面就是通过 virt-manager 来实现，但是这种方法貌似只适用于较新版本KVM环境，步骤如下： 打开 virtual machine mananger，-&gt; 点击右上角新建虚拟机 选择 Import existing disk image，-&gt; Forward 点击 Browse 点击 Browse Local，找到虚拟机磁盘镜像文件，-&gt;Choose Volume 点击 Forward 配置合适的内存和 CPU 个数，-&gt; Forward 给虚拟机命名（但注意这不是虚拟机里面的 hostname），-&gt; Finish 最后就会进入虚拟机的系统启动过程 遇到的问题由于教研室有一些服务器的使用年限较长，有一些服务器的系统和软件版本较低，在使用 KVM 时碰到一些很烦人的问题，所以才写下这篇博客记录一下。如果在旧版本的环境下使用新环境才支持的命令选项，就可能会出现一些问题，如： 1234567891011121314$ virt-install --virt-type kvm --name docker-$1 --ram 4096 --vcpus=4 \ --cdrom=/home/kb310/test/ubuntu.qcow2 \ --disk path=/home/kb310/test/ubuntu.qcow2,format=qcow2 \ --network bridge=br0,model=virtio \ --graphics vnc,listen=0.0.0.0 --noautoconsole \ --os-type=linuxStarting install...ERROR internal error: process exited while connecting to monitor: qemu-system-x86_64: -drive file=/home/kb310/test/ubuntu.qcow2,if=none,id=drive-ide0-0-0,format=qcow2: could not open disk image /home/kb310/test/ubuntu.qcow2: Could not open &apos;/home/kb310/test/ubuntu.qcow2&apos;: Permission deniedDomain installation does not appear to have been successful.If it was, you can restart your domain by running: virsh --connect qemu:///system start hghotherwise, please restart your installation. 上面出现的错误就是由于软件版本的不同导致的，所以最好使用兼容的命令选项。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>virt-install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[110. Balanced Binary Tree]]></title>
    <url>%2F2018%2F10%2F18%2Fleetcode-110%2F</url>
    <content type="text"><![CDATA[110. Balanced Binary TreeGiven a binary tree, determine if it is height-balanced. For this problem, a height-balanced binary tree is defined as: a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Example 1: Given the following tree [3,9,20,null,null,15,7]: 12345 3 / \9 20 / \ 15 7 Return true.Example 2: Given the following tree [1,2,2,3,3,null,null,4,4]: 1234567 1 / \ 2 2 / \ 3 3 / \4 4 Return false. 题意理解 给定一个二叉树（是任意二叉树，而不是二叉搜索树），判断其是否是高度平衡的。高度平衡二叉树的定义就是： 对于二叉树的每一个节点，其左右子树的高度相差最大不超过 1 。 根据上面的定义，简单地认为只需要根节点的左右子树的高度相差不超过 1 是错误的，必须保证每一个节点都满足该条件。 解法思路 初始思路： 按照高度平衡二叉树的定义，可以想到通过遍历每一个节点，然后求每个节点的左右子树的高度，最后比较左右子树的高度差是否大于 1，当高度差大于时则返回 false，否则返回 true。实现如下： 123456789101112131415161718class Solution &#123;public: int getHeight(TreeNode* root) &#123; if (root == NULL) return 0; else return 1 + max(getHeight(root-&gt;left), getHeight(root-&gt;right)); &#125; bool isBalanced(TreeNode* root) &#123; if (root == NULL) return true; int left = getHeight(root-&gt;left); int right = getHeight(root-&gt;right); return abs(left - right) &lt;= 1 &amp;&amp; isBalanced(root-&gt;left) &amp;&amp; isBalanced(root-&gt;right); &#125;&#125;; 复杂度分析：求个节点的高度调用 getHeight() 花费的时间复杂度是 O(N)，遍历所有节点又需要 O(N)，因此总的时间复杂度是 O(N^2)。 改进思路： 对于上面的方法，是从根往下，在求每一个节点的高度时都会遍历其子节点，这些操作有很多时重复的。如果可以每个节点的高度都保存下来就好了。对于这个题目，节点的值是没有用的，因此可以用节点的值来存放其高度，这样就没有额外的空间消耗。 代码如下： 12345678910111213141516171819202122232425262728293031class Solution &#123;public: void storeHeight(TreeNode* root) &#123; if (root == NULL) return ; storeHeight(root-&gt;left); storeHeight(root-&gt;right); if (root-&gt;left != NULL &amp;&amp; root-&gt;right != NULL) root-&gt;val = 1 + max(root-&gt;left-&gt;val, root-&gt;right-&gt;val); else if (root-&gt;left != NULL) root-&gt;val = 1 + root-&gt;left-&gt;val; else if (root-&gt;right != NULL) root-&gt;val =1+root-&gt;right-&gt;val; else root-&gt;val = 1; &#125; bool helper(TreeNode* root) &#123; if (root == NULL) return true; int left = root-&gt;left == NULL? 0 : root-&gt;left-&gt;val; int right = root-&gt;right == NULL? 0 : root-&gt;right-&gt;val; return abs(left - right) &lt;= 1 &amp;&amp; helper(root-&gt;left) &amp;&amp; helper(root-&gt;right); &#125; bool isBalanced(TreeNode* root) &#123; storeHeight(root); return helper(root); &#125;&#125;; 复杂度分析：算法首先求出二叉树各节点的高度，时间复杂度为 O(N)；然后再递归的计算每个节点左右子树的高度差是否小于等于 1，时间复杂度也为 O(N)。所以总的时间复杂度为 O(N) + O(N) = O(N)。 最终解法： 在递归的计算二叉树中每个节点的高度时，如果两个子树的高度相差小于等于 1，则记录该节点真实的高度值，否则记录为 -1；并且在递归求某节点的高度时判断其左右孩子的高度值是否记录的是 -1，如果是，说明其左右子树中有非平衡树，则将该节点的高度值记为 -1。最后判断根节点的高度值是否为 -1，若为 -1，说明该二叉树不是高度平衡的；若不为 -1，说明该二叉树是高度平衡的。 代码如下： 123456789101112131415161718192021222324class Solution &#123;public: int dfsHeight(TreeNode* root)&#123; if (root == NULL) return 0; int left = dfsHeight(root-&gt;left); if (left == -1) return -1; int right = dfsHeight(root-&gt;right); if (right == -1) return -1; if (abs(left - right) &gt; 1) return -1; return 1 + max(left, right); &#125; bool isBalanced(TreeNode* root) &#123; if (dfsHeight(root) == -1) return false; else return true; &#125;&#125;; 复杂度分析：只需要遍历一遍所有节点即可，时间复杂度为 O(N)。]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[107. Binary Tree Level Order Traversal II]]></title>
    <url>%2F2018%2F10%2F17%2Fleetcode-107%2F</url>
    <content type="text"><![CDATA[107. Binary Tree Level Order Traversal IIGiven a binary tree, return the bottom-up level order traversal of its nodes’ values. (ie, from left to right, level by level from leaf to root). For example:Given binary tree [3,9,20,null,null,15,7], 12345 3 / \9 20 / \ 15 7 return its bottom-up level order traversal as: 12345[ [15,7], [9,20], [3]] 题意理解给定一个二叉树，要求自底向上，自左向右的按层输出二叉树各节点的值。输出保存在一个二维数组中，也就是一个数组的数组，外层数组的元素是二叉树每一层节点的值的集合。 解法思路解法一：DFS（Depth-First-Search）可以知道的是，输出的二维数组中用一个数组来保存二叉树中的每一层的节点的值，且该内层数组在外层数组中的下标索引刚好与这些节点在树中的高度相反。所以可以利用 DFS 来遍历二叉树，根据节点在树中的高度，找到数组中保存该层元素的内层数组，将节点的值插入，最后只需将整个数组反转即可。对于二叉树的深度优先搜索，可以考虑用递归或者栈来实现。 DFS with recursive： 123456789101112131415161718192021class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrderBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; levelRecursive(res, root, 0); reverse(res.begin(), res.end()); return res; &#125; void levelRecursive(vector&lt;vector&lt;int&gt;&gt; &amp;res, TreeNode* root, int level)&#123; if (root == NULL) return ; if (res.size() &lt; level+1)&#123; //判断是否已经有内层数组来存放该层节点的值 vector&lt;int&gt; vec; res.push_back(vec); &#125; res[level].push_back(root-&gt;val); levelRecursive(res, root-&gt;left, level+1); levelRecursive(res, root-&gt;right, level+1); &#125;&#125;; DFS with stack： 12345678910111213141516171819202122232425class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrderBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == NULL) return res; vector&lt;pair&lt;TreeNode*, int&gt;&gt; stack; //用一个pair来存放节点和节点在书中的深度 stack.push_back(&#123;root, 0&#125;); while (!stack.empty())&#123; pair&lt;TreeNode*, int&gt; p = stack.back(); stack.pop_back(); if (res.size() &lt; p.second + 1)&#123; vector&lt;int&gt; vec; res.push_back(vec); &#125; res[p.second].push_back(p.first-&gt;val); if (p.first-&gt;right != NULL) stack.push_back(&#123;p.first-&gt;right, p.second+1&#125;); if (p.first-&gt;left != NULL) stack.push_back(&#123;p.first-&gt;left, p.second+1&#125;); &#125; reverse(res.begin(), res.end()); return res; &#125;&#125;; 解法二：BFS（Breadth-First-Search）使用队列来存放每一层的节点，在 while 循环中又有一个 for 循环来遍历每一层的元素。 123456789101112131415161718192021222324252627class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrderBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; queue&lt;TreeNode* &gt; q; if (root == NULL) return res; q.push(root); while (!q.empty())&#123; vector&lt;int&gt; level; int len = q.size(); for (int i = 0; i &lt; len; ++i)&#123; level.push_back(q.front()-&gt;val); TreeNode* p = q.front(); q.pop(); if (p-&gt;left != NULL) q.push(p-&gt;left); if (p-&gt;right != NULL) q.push(p-&gt;right); &#125; res.push_back(level); &#125; reverse(res.begin(), res.end()); return res; &#125;&#125;; 复杂度：以上解法的时间复杂度及空间复杂度都为 O(N)。]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程访问 Docker Daemon]]></title>
    <url>%2F2018%2F10%2F13%2F%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEDocker%2F</url>
    <content type="text"><![CDATA[介绍最近的项目中涉及到监控远程主机上的 Docker 容器的运行状况，即通过本地 docker 客户端访问远程主机的 docker 服务端，以此来监控远程主机上的 Docker 容器。但是 Docker Daemon 默认情况下是只允许本地访问的，不允许远程访问。本文将首先介绍 Docker Daemon 的连接方式，然后说明如何配置远程访问。 Docker Daemon 的连接方式 UNIX 域套接字 默认就是这种方式, 会生成一个 /var/run/docker.sock 文件, UNIX 域套接字用于本地进程之间的通讯, 这种方式相比于网络套接字效率更高, 但局限性就是只能被本地的客户端访问。 tcp 端口监听 服务端开启端口监听 dockerd -H IP:PORT , 客户端通过指定IP和端口访问服务端 docker -H IP:PORT 。通过这种方式, 任何人只要知道了你暴露的ip和端口就能随意访问你的docker服务了, 这是一件很危险的事, 因为docker的权限很高, 不法分子可以从这突破取得服务端宿主机的最高权限。 配置 Docker 远程访问看到一些资料上面说可以通过修改 /etc/default/docker 文件来修改 Docker Daemon 的配置，也就是编辑docker的配置文件 /etc/default/docker 中的 DOCKER_OPTS 选项成同时监听本地 unix socket 和远程 http socket（2375） 1DOCKER_OPTS=&quot;-H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375&quot; 但是通过这种方法修改完成重新启动 docker 守护进程之后，Docker Daemon 并没有在监听 2375 端口。然后发现其实在 /etc/default/docker 文件也有说到： 1234# THIS FILE DOES NOT APPLY TO SYSTEMD## Please see the documentation for &quot;systemd drop-ins&quot;:# https://docs.docker.com/engine/admin/systemd/ 因为 Ubuntu 16.04 使用 systemd 来管理 docker 进程，/etc/default/docker 文件已经不再起作用了，该文件只对更老的系统起作用。正确的配置方式如下： 主要参考该链接：Adjustments to docker setting 1. Changes to default options安装好 Docker 后可能有一些选项需要修改，就像我们这里想要配置 Docker Daemon 允许远程访问一样。有两种方法： 修改 /etc/default/docker 和 /lib/systemd/system/docker.service 文件，但是这种方法对不同版本的 Docker 有不同的配置方法，不推荐。 创建 /etc/docker/daemon.json 来作为配置文件，Docker 推荐使用这种方法，这也是我们接下来介绍的。 注：这里使用的是 Ubuntu 16.04 LTS 系统，使用了 systemd 来管理 docker 进程。而更老版本的 Ubuntu 系统没有使用 systemd，所以只需要修改 /etc/default/docker 文件就够了。 开始之前，先关闭 docker 进程： 1$ sudo service docker stop 注意：在上面的原文链接的方法中，还会修改 docker 的默认目录，也就是将 /var/lib/docker 修改成了 /data/docker ，修改之后我们原来的容器镜像都会找不到的，所以我们就不修改默认目录，还是使用原来的 /var/lib/docker。可以使用以下命令查看 docker 默认目录： 123&gt; $ docker info | grep &quot;Docker Root Dir&quot;&gt; Docker Root Dir: /var/lib/docker&gt; 2. Create/modify files创建 /etc/docker/daemon.json 文件（如果已经存在则修改），加入以下内容： 123&#123; &quot;hosts&quot; : [&quot;unix:///var/run/docker.sock&quot;, &quot;tcp://0.0.0.0:2375&quot;]&#125; &quot;unix:///var/run/docker.sock&quot;：unix socket，本地客户端将通过这个来连接 Docker Daemon。 &quot;tcp://0.0.0.0:2375&quot;：tcp socket，表示允许任何远程客户端通过 2375 端口连接 Docker Daemon。 注意：这里也和原文链接有点不同。 3. Servers using systemd:可以使用 systemctl edit docker 来调用文本编辑器修改指定的单元或单元实例，ubuntu 默认调用的是 nano 编辑器，不是很好用，如果不熟悉 nano 编辑器的操作可以使用 vim 编辑器。 主要也就是新建或修改 /etc/systemd/system/docker.service.d/override.conf，其内容如下： 1234##Add this to the file for the docker daemon to use different ExecStart parameters (more things can be added here)[Service]ExecStart=ExecStart=/usr/bin/dockerd 解释一下： 默认情况下使用 systemd 时，docker.service 的设置为：ExecStart=/usr/bin/dockerd -H fd://，这将覆盖写到 daemon.json 中的任何 hosts 。通过在 override.conf 文件中将 ExecStart 仅仅定义为：ExecStart=/usr/bin/dockerd，这将会使用在 daemon.json 中设置的 hosts 。这个文件中的第一行ExecStart= 必须要有，因为它将用于清楚默认的 ExecStart 参数。如果是修改 docker.service 的文件而不是创建 override.conf，那么下次 systemd 重启时，docker.service 文件也会被重新创建。 重新加载 daemon 并重启 docker 服务： 12$ sudo systemctl daemon-reload$ sudo systemctl restart docker.service 检查端口监听： 12# netstat -ntlp |grep dockerdtcp6 0 0 :::2375 :::* LISTEN 6259/dockerd 如果检查端口监听时出现下面的提示，则说明需要使用 root 权限。 123&gt; (Not all processes could be identified, non-owned process info&gt; will not be shown, you would have to be root to see it all.)&gt; 在远程主机上面通过 tcp socket 来访问本机的 Docker Daemon 服务： 123$ docker -H 192.168.1.130:2375 images$ docker -H 192.168.1.130:2375 ps 其中 192.168.1.130 是开放了远程访问的主机的 IP。 另外需要注意的是这种操作是不太安全的，仅用于测试使用。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger caliper 安装记录]]></title>
    <url>%2F2018%2F10%2F10%2Fcaliper%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[介绍Caliper 是一个区块链性能测试框架，可用于测试不同的区块链实现，由华为公司开发并贡献给 Hyperledger。本文介绍如何在 ubuntu 系统下安装 caliper 并完成简单测试，最后会说明安装过程中的错误解决。 Pre-requisites需要安装的基础环境如下： make，g++ NodeJS 8.X node-gyp Docker Docker-compose 1. 安装 make，g++ 编译工具1sudo apt-get install make g++ 2. 安装node.jsubuntu 支持 nodesouce 的二进制安装脚本，命令如下： 12curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -sudo apt-get install -y nodejs 安装完成后查看 node 与 npm 的版本： 1234$ node -vv8.12.0$ npm -v6.4.1 3. 安装 node-gypnpm 全局安装 node-gyp： 1sudo npm install -g node-gyp 4. 安装 Docker 由于 apt 源使用HTTPS以确保软件下载过程中不被篡改。因此，我们首先需要添加使用HTTPS传输的软件包以及CA证书。 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 为了确认所下载软件包的合法性，需要添加软件源的 GPG 秘钥 1$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 然后，我们需要向 sources.list 中添加 Docker 软件源 1234$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 以上命令会添加稳定版本的Docker CE apt 镜像源。 更新 apt 软件包缓存，并安装 docker-ce： 123$ sudo apt-get update$ sudo apt-get install docker-ce 查看 Docker 版本： 12$ docker -vDocker version 18.06.0-ce, build 0ffa825 启动 Docker CE 12$ sudo systemctl enable docker$ sudo systemctl start docker 建立 docker 用户组 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 1$ sudo groupadd docker 其实一般按照上面的方法安装 Docker 后就已经创建好 docker 用户组了，可以使用 $ cat /etc/group | grep docker 命令来验证，所以就不需要再建立 docker 用户组了，再建立也会报错提示用户组已存在的。 将当前用户加入 docker 用户组： 1$ sudo usermod -aG docker $USER 下次登录时即可方便的使用 docker 命令。 测试 Docker 是否安装正确 123456789101112131415161718192021222324252627$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-world9db2ca6ccae0: Pull complete Digest: sha256:4b8ff392a12ed9ea17784bd3c9a8b1fa3299cac44aca35a85c90c5e3c7afacdcStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 配置镜像加速器 国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如： Docker 官方提供的中国 registry mirror https://registry.docker-cn.com 七牛云加速器 https://reg-mirror.qiniu.com/ 当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。 国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。 我们以 Docker 官方加速器 https://registry.docker-cn.com 为例进行介绍。 在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在则创建） 12345&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125; 之后重新启动服务 12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 5. 安装 Docker-compose 通过二进制包来安装，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。 123$ sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 查看 Docker compose 版本 12$ docker-compose --versiondocker-compose version 1.22.0, build f46880fe Clone caliper repository从 GitHub 克隆 caliper 代码仓库： 1git clone https://github.com/hyperledger/caliper.git 进入 caliper 目录并运行 npm install 安装依赖包： 12cd caliper/npm install Install fabric SDKs在 caliper 目录下本地安装 fabric SDKs： 1npm install grpc@1.10.1 fabric-ca-client fabric-client 以上命令默认安装 fabric 最新版本的 SDKs，但是由于 caliper 验证过的最新版本是 v1.1.0，所以我们最好安装 v1.1.0 版本： 1npm install fabric-ca-client@1.1.0 fabric-client@1.1.0 Run benchmark性能测试示例在benchmark目录下，用法如下： 1node benchmark/simple/main.js -c yourconfig.json -n yournetwork.json -c 用于指定区块链的配置文件，不指定的话默认为config.json； -n 用于指定区块链网络配置文件，不指定的话由-c指定的配置文件定义。 运行一个 simple 的实例： 1node benchmark/simple/main.js 生成的报告如下： Bugs运行测试遇到 REQUEST_TIMEOUT 的问题：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114$ node benchmark/simple/main.js TAP version 13# #######Caliper Test######docker-compose -f network/fabric/simplenetwork/docker-compose.yaml up -dCreating network &quot;simplenetwork_default&quot; with the default driverCreating ca_peerOrg1 ... doneCreating simplenetwork_peer_1 ... doneCreating orderer.example.com ... doneCreating ca_peerOrg2 ... doneCreating simplenetwork_ca_1 ... doneCreating peer1.org2.example.com ... doneCreating peer0.org1.example.com ... doneCreating peer1.org1.example.com ... doneCreating peer0.org2.example.com ... done# create mychannel......ok 1 created mychannel successfully# Sleep 5s......# Join channel......# join mychannelok 2 Successfully joined mychannel# install all chaincodes......info: [packager/Golang.js]: packaging GOLANG from contract/fabric/simple/goinfo: [packager/Golang.js]: packaging GOLANG from contract/fabric/simple/gook 3 Installed chaincode simple successfully in all peers# Instantiate chaincode......error: [Peer.js]: sendProposal - timed out after:120000error: [Peer.js]: sendProposal - timed out after:120000error: [Peer.js]: sendProposal - timed out after:120000error: [Peer.js]: sendProposal - timed out after:120000error: [client-utils.js]: sendPeersProposal - Promise is rejected: Error: REQUEST_TIMEOUT at Timeout._onTimeout (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/fabric-client/lib/Peer.js:124:19) at ontimeout (timers.js:498:11) at tryOnTimeout (timers.js:323:5) at Timer.listOnTimeout (timers.js:290:5)error: [client-utils.js]: sendPeersProposal - Promise is rejected: Error: REQUEST_TIMEOUT at Timeout._onTimeout (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/fabric-client/lib/Peer.js:124:19) at ontimeout (timers.js:498:11) at tryOnTimeout (timers.js:323:5) at Timer.listOnTimeout (timers.js:290:5)error: [client-utils.js]: sendPeersProposal - Promise is rejected: Error: REQUEST_TIMEOUT at Timeout._onTimeout (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/fabric-client/lib/Peer.js:124:19) at ontimeout (timers.js:498:11) at tryOnTimeout (timers.js:323:5) at Timer.listOnTimeout (timers.js:290:5)error: [client-utils.js]: sendPeersProposal - Promise is rejected: Error: REQUEST_TIMEOUT at Timeout._onTimeout (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/fabric-client/lib/Peer.js:124:19) at ontimeout (timers.js:498:11) at tryOnTimeout (timers.js:323:5) at Timer.listOnTimeout (timers.js:290:5)not ok 4 Failed to instantiate chaincodes, Error: Failed to send instantiate due to error: Error: Failed to send instantiate Proposal or receive valid response. Response null or status is not 200. exiting... at Client.newDefaultKeyValueStore.then.then.then.then (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/e2eUtils.js:372:19) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) at Client.newDefaultKeyValueStore.then.then.then.then.then (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/e2eUtils.js:385:15) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) --- operator: fail at: chaincodes.reduce.then.catch (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/instantiate-chaincode.js:60:19) stack: |- Error: Failed to instantiate chaincodes, Error: Failed to send instantiate due to error: Error: Failed to send instantiate Proposal or receive valid response. Response null or status is not 200. exiting... at Client.newDefaultKeyValueStore.then.then.then.then (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/e2eUtils.js:372:19) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) at Client.newDefaultKeyValueStore.then.then.then.then.then (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/e2eUtils.js:385:15) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) at Test.assert [as _assert] (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/tape/lib/test.js:224:54) at Test.bound [as _assert] (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/tape/lib/test.js:76:32) at Test.fail (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/tape/lib/test.js:317:10) at Test.bound [as fail] (/home/user1/go/src/github.com/hyperledger/caliper/node_modules/tape/lib/test.js:76:32) at chaincodes.reduce.then.catch (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/instantiate-chaincode.js:60:19) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7) ...fabric.installSmartContract() failed, Error: Fabric: instantiate chaincodes failed at chaincodes.reduce.then.catch (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/instantiate-chaincode.js:61:31) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7)[Transaction Info] - Submitted: 0 Succ: 0 Fail:0 Unfinished:0unexpected error, Error: Fabric: instantiate chaincodes failed at chaincodes.reduce.then.catch (/home/user1/go/src/github.com/hyperledger/caliper/src/fabric/instantiate-chaincode.js:61:31) at &lt;anonymous&gt; at process._tickCallback (internal/process/next_tick.js:189:7)docker-compose -f network/fabric/simplenetwork/docker-compose.yaml down;docker rm $(docker ps -aq);docker rmi $(docker images dev* -q)Stopping peer0.org1.example.com ... doneStopping peer0.org2.example.com ... doneStopping peer1.org1.example.com ... doneStopping peer1.org2.example.com ... doneStopping simplenetwork_ca_1 ... doneStopping ca_peerOrg2 ... doneStopping orderer.example.com ... doneStopping ca_peerOrg1 ... doneRemoving peer0.org1.example.com ... doneRemoving peer0.org2.example.com ... doneRemoving peer1.org1.example.com ... doneRemoving peer1.org2.example.com ... doneRemoving simplenetwork_ca_1 ... doneRemoving ca_peerOrg2 ... doneRemoving simplenetwork_peer_1 ... doneRemoving orderer.example.com ... doneRemoving ca_peerOrg1 ... doneRemoving network simplenetwork_default&quot;docker rm&quot; requires at least 1 argument.See &apos;docker rm --help&apos;.Usage: docker rm [OPTIONS] CONTAINER [CONTAINER...]Remove one or more containers&quot;docker rmi&quot; requires at least 1 argument.See &apos;docker rmi --help&apos;.Usage: docker rmi [OPTIONS] IMAGE [IMAGE...]Remove one or more images1..4# tests 4# pass 3# fail 1 问题解决： 首先需要检查环境依赖是否正确安装，尤其是 node-gyp 包。 然后查看 npm 包版本信息及 docker 镜像信息： 12345678910111213$ npm ls fabric-clientcaliper@0.1.0 /home/user1/go/src/github.com/hyperledger/caliper└── fabric-client@1.1.0$ npm ls fabric-ca-clientcaliper@0.1.0 /home/user1/go/src/github.com/hyperledger/caliper└── fabric-ca-client@1.1.0$ npm ls grpccaliper@0.1.0 /home/user1/go/src/github.com/hyperledger/caliper├─┬ fabric-client@1.1.0│ └── grpc@1.10.1 deduped└── grpc@1.10.1 需确认无误。因为 caliper 在创建 fabric 网络时默认是使用的 fabric v1.1.0 版本，docker 下载的 peer、Orderer、ca 镜像也是 x86_64-v1.1.0 版本，所以 fabric SDKs 的 版本必须一致。 12345$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhyperledger/fabric-ca x86_64-1.1.0 72617b4fa9b4 6 months ago 299MBhyperledger/fabric-orderer x86_64-1.1.0 ce0c810df36a 6 months ago 180MBhyperledger/fabric-peer x86_64-1.1.0 b023f9be0771 6 months ago 187MB 修改请求超时时间 在一台虚拟机里面安装的测试环境，搭建的 fabric 网络将会运行有7个 nodes/containers（1 orderer，2 CAs，4 peers），可能由于虚拟机性能的原因，原本 2 分钟的请求超时时间可能不足以 instantiate chaincode（需要构建一个 docker 镜像，并在每个 peer 中运行 chaincode Init 函数）。所以应该尝试增加 timeout 的值。修改 src/fabric/e2eUtils.js 第211行： 1Client.setConfigSetting(&apos;request-timeout&apos;, 120000); 将 120000 增加到 480000。 参考链接： Instantiate chaincode…… #118 instantiate chaincodes Error #137]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Hyperledger fabric</tag>
        <tag>Hyperledger caliper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云服务器实现 frp 内网穿透]]></title>
    <url>%2F2018%2F09%2F18%2F%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E7%8E%B0frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%2F</url>
    <content type="text"><![CDATA[前言前几天在一台具有公网IP的 vultr 云服务器上实现了 frp 内网穿透（参考链接），可以从寝室 ssh 登录到教研室的服务器，但是由于 vultr 的云服务器位于国外的节点，连接速度太慢了，导致连接 ssh 登录的体验很差。今天又弄了一台阿里云的云服务器 ECS，所以现在来介绍如何在阿里云上实现 frp 内网穿透。 配置 ssh 访问内网机器之前使用的 vultr 的VPS是 CentOS 系统的，这次阿里云服务器是 Ubuntu 的操作系统，但其实不同操作系统的云服务器在使用 frp 时操作都是一样的，就是下载，配置，运行。可能有区别的地方就是防火墙相关的配置，不同云服务厂商的镜像不同可能也有一些差别。下面来说明在阿里云的公网服务器的 frp 实现步骤： 下载在阿里云的公网服务器和内网机器上都要下载安装包并解压： 12$ wget https://github.com/fatedier/frp/releases/download/v0.21.0/frp_0.21.0_linux_amd64.tar.gz$ tar -xzvf frp_0.21.0_linux_amd64.tar.gz 解压之后的文件夹中既包含了服务端的文件又包括客户端的文件，所以可以分别在两个机器上删除掉不必要的文件，也可以不删，都没有影响。强迫症还是来删一下，在解压后的文件夹中： 在公网服务器上删除客户端相关的文件，只保留一下两个文件： 1frps frps.ini 在内网机器上删除服务端相关的文件，只保留以下两个文件： 1frpc frpc.ini 配置就是需要修改配置文件 frps.ini 及 frpc.ini。 修改公网服务器上的服务端配置文件 frps.ini，如下： 12[common]bind_port = 7000 #frp服务端端口（必须） 修改内网目标主机的客户端配置文件 frpc.ini，如下： 12345678910[common]server_addr = xxx.xxx.xxx.xxx #frp服务端地址，必须是公网ip或者域名，这里假设为xxx.xxx.xxx.xxxserver_port = 7000 #frp服务端端口，即填写服务端配置中的 bind_port[ssh]type = tcp #连接类型，填tcp或udplocal_ip = 127.0.0.1 #填127.0.0.1或内网ip都可以local_port = 22 #需要转发到的端口，ssh端口是22remote_port = 6000 #frp服务端的远程监听端口，即你访问服务端的remote_port就相当于访 #问客户端的 local_port，如果填0则会随机分配一个端口 运行 在公网服务器上运行服务端程序： 1$ nohup ./frps -c frps.ini &amp; 查看 nohup.out 的信息，success 123456$ tail -f nohup.out2018/09/17 21:34:01 [I] [service.go:130] frps tcp listen on 0.0.0.0:70002018/09/17 21:34:01 [I] [root.go:207] Start frps success2018/09/17 22:06:02 [I] [service.go:319] client login info: ip [125.71.229.32:60516] version [0.21.0] hostname [] os [linux] arch [amd64]2018/09/17 22:06:02 [I] [proxy.go:217] [7940291c148c2fca] [ssh] tcp proxy listen port [6000]2018/09/17 22:06:02 [I] [control.go:335] [7940291c148c2fca] new proxy [ssh] success 在内网目标主机上运行客户端程序： 1$ nohup ./frpc -c frpc.ini &amp; 查看 nohup.out 的信息，success 1234567$ tail -f nohup.out2018/09/17 22:42:22 [I] [proxy_manager.go:300] proxy removed: []2018/09/17 22:42:22 [I] [proxy_manager.go:310] proxy added: [ssh1]2018/09/17 22:42:22 [I] [proxy_manager.go:333] visitor removed: []2018/09/17 22:42:22 [I] [proxy_manager.go:342] visitor added: []2018/09/17 22:42:23 [I] [control.go:246] [0624b332c3465118] login to server success, get run id [0624b332c3465118], server udp port [0]2018/09/17 22:42:23 [I] [control.go:169] [0624b332c3465118] [ssh1] start proxy success 配置多个内网主机错误的多客户端配置使用一台阿里云的公网服务器，我们可以配置很多内网机器的 frp 内网穿透，公网服务器上只需要按照上述的配置一次即可，但是内网机器的配置稍有不同，如果使用了一样的配置则后添加的内网机器是无法连接上公网服务器的。这里假设另一台内网机器2的 frpc.ini 配置如下，来说明会遇到的问题： 12345678910$ vi frpc.ini[common]server_addr = xxx.xxx.xxx.xxx &lt;==这里还是按照上面的假设，公网服务器的ip为xxx.xxx.xxx.xxxserver_port = 7000[ssh]type = tcp local_ip = 127.0.0.1local_port = 22remote_port = 6001 &lt;==remote_port设置为另一个值 两个内网主机的配置除了 remote_port 不一样之外，都是一样的。但是在内网机器2上运行 frpc 后，公网服务器的 nohup.out 中会记录一下的错误： 1[W] [control.go:332] [280d36891a6ae0c7] new proxy [ssh] error: proxy name [ssh] is already in use 后来发现，frp 中是通过 [ssh] 这个名字来区分不同客户端的，所以不同的客户端要配置成不同的名字。 正确的多客户端配置内网机器1和内网机器2的配置应该区分如下： 12345678910111213内网机器1：[ssh] &lt;==不同点type = tcp local_ip = 127.0.0.1local_port = 22remote_port = 6000 &lt;==不同点内网机器2：[ssh1] &lt;==不同点type = tcp local_ip = 127.0.0.1local_port = 22remote_port = 6001 &lt;==不同点 在两个内网机器上分别运行 frpc 客户端程序后，一般就可以通过以下的方法 ssh 登录： 12345内网机器1：$ ssh -p 6000 user_name1@server_addr内网机器2：$ ssh -p 6001 user_name2@server_addr 以上参数中，server_addr是公网服务器的公网ip；user_name1、user_name2 分别是内网机器1、2的用户名，之后分别使用登录密码就可以登录。 connection timed out 解决但是有时候会发现按照以上的配置还是使 frp 的服务端与客户端建立连接，在客户端上会出现以下错误： 122018/09/17 22:02:23 [W] [control.go:113] login to server failed: dial tcp xxx.xxx.xxx.xxx:7000: connect: connection timed outdial tcp xxx.xxx.xxx.xxx:7000: connect: connection timed out 仔细检查了一下云服务器上面的防火墙设置，发现并没有任何过滤规则，那应该不是防火墙的问题 123456789# iptables -vnLChain INPUT (policy ACCEPT 46165 packets, 28M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 42223 packets, 3001K bytes) pkts bytes target prot opt in out source destination 然后又仔细想了一下我以前在做关于 OpenStack 的项目中也碰到了类似的问题，就是创建了云实例，发现无法通过 ssh 来登录，就是因为 OpenStack 有安全组规则相关的设置。 想到了这里就有一种熟悉的感觉，还是原来的配方，hahaha。。。 登录到阿里云对应云服务器的管理控制台，如下找到左上方的本实例安全组： 然后点击右上角的配置规则： 再点击右上角的添加安全组规则： 最后在添加安全组规则的界面添加相应的规则： 主要有两个配置项，端口范围和授权对象 端口范围就是你要开放的端口的范围，需要将 server_port 和 remote_port 都添加进去，可以分成两条规则分别添加； 授权对象是你要授权的对象，一般设为 0.0.0.0/0 表示允许所有IP的访问。 以上就是针对阿里云的云服务器的安全组设置，其他厂商如果有安全组规则也应该是类似的。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
        <tag>frp</tag>
        <tag>内网穿透</tag>
        <tag>aliyun</tag>
        <tag>安全组规则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 SSH 的个人总结]]></title>
    <url>%2F2018%2F09%2F15%2F%E5%85%B3%E4%BA%8ESSH%E7%9A%84%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目的在日常学习工作中，早已离不开远程的 Linux 服务器或虚拟机，每次都是借助 Xshell 中保存的用户密码方便的 ssh 登录，偶尔在配置环境中的时候用到了秘钥登录，但是从来没有总结一下相关的内容，每次有问题都是上网查，所以写下这篇个人经验总结的文章。本文将主要介绍 ssh 的安装， ssh 免密登录，允许 root 登录，ssh 登录的一些选项，ssh 登录本机等问题。 安装 ssh基于 Debian/Ubuntu 系统 安装客户端 1sudo apt-get install openssh-client 安装服务端 1sudo apt-get install openssh-server 基于 RedHat/CentOS 系统 安装客户端 1yum install openssh-clients 安装服务端 1yum install openssh-server 关于客户端与服务端的区别就是：客户端用于你去登录别人，而服务端则是别人来登录你。 ssh 免密登录快速实现步骤：12$ ssh-keygen -t rsa$ ssh-copy-id -i ~/.ssh/id_rsa.pub username@xxx.xxx.xxx.xxx 说明如下：Linux系统有一个钥匙环(keyring)的管理程序。钥匙环受到用户登录密码的保护。当你登录Linux系统时，会自动解开钥匙环的密码，从而可访问钥匙环。SSH的密钥和公钥也存储在钥匙环。所以初次使用SSH密钥登录远程Linux服务器时需要输入一次SSH密钥的密码。而将来使用SSH密钥登录时不再输入密码。Ubuntu的钥匙环程序是seahorse。 SSH密钥就好比是你的身份证明。远程Linux服务器用你生成的SSH公钥来加密一条消息，而只有你的SSH密钥可以解开这条消息。所以其他人如果没有你的SSH密钥，是无法解开加密消息的，从而也就无法登录你的Linux服务器。 12345678910111213141516171819202122232425$ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/home/user1/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user1/.ssh/id_rsa.Your public key has been saved in /home/user1/.ssh/id_rsa.pub.The key fingerprint is:SHA256:BQeyB4pwfE1L7NAi7hjrTl5X814QW5Gu4dOZx0gIdng user1@ubuntuThe key&apos;s randomart image is:+---[RSA 2048]----+|... == +.... || oo.+o+O E .. || ..o.++ =.+. ||. . .. ++o || = oSo= = ||o . . o+.= o ||.. . . .... ||o.. . . . ||.o . |+----[SHA256]-----+$ cd ~/.ssh/$ lsid_rsa id_rsa.pub ssh-copy-id 工具可以把本地主机的公钥复制到远程主机的 authorized_keys 文件上，ssh-copy-id 命令也会给远程主机的用户主目录（home）和~/.ssh, 和~/.ssh/authorized_keys设置合适的权限。 语法 1ssh-copy-id [-i [identity_file]] [user@]machine 选项 1-i：指定公钥文件 实例 1234567891011121314$ ssh-copy-id -i ~/.ssh/id_rsa.pub user1@192.168.1.158/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/user1/.ssh/id_rsa.pub&quot;The authenticity of host &apos;192.168.1.158 (192.168.1.158)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:Gx0RFwV4EiROc6ymV/BmSLE2zNT9AswZvzq9pc6Srwo.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysuser1@192.168.1.158&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &apos;user1@192.168.1.158&apos;&quot;and check to make sure that only the key(s) you wanted were added. root 用户登录在一些系统上，比如 ubuntu，root 用户是默认禁用的，也就是说通常不允许 root 账号通过网络登录。但是通过适当的配置可以允许 root 用户登录。 设置 root 用户密码 1$ sudo passwd root 允许 root 用户远程登录 12345# vi /etc/ssh/sshd_config...PermitRootLogin yes #取消注释# service sshd restart #重启ssh服务 ssh 登录选项1. 无选项参数运行 ssh如： 1ssh 192.168.1.156 这会用你的用户名去 192.168.1.156 这台主机，相当于省略了。如果目标主机上没有和你的用户名同名的，则每次输入密码都是显示密码错误。 2. 指定登录用户有如下两种方式指定用户名： 123ssh -l user1 192.168.1.156ssh user1@192.168.1.156 3. 指定端口ssh 默认使用的端口号是 22。大多现代的 Linux 系统 22 端口都是开放的。如果你运行 ssh 程序而没有指定端口号，它直接就是通过 22 端口发送请求的。 当然也可以改变 ssh 的默认端口号，只需要修改 /etc/ssh/sshd_config 文件，找到此行： 1Port 22 修改成你想要的端口，如 1234。然后重启 ssh 服务： 1sudo service sshd restart 使用新的端口来访问 ssh 服务： 123ssh -p 1234 user1@192.168.1.156或ssh -oPort=1234 user1@192.168.1.156 4. 绑定源地址如果你的主机有多个 IP，如： 1234567891011121314151617181920$ ifconfigeth0 Link encap:Ethernet HWaddr d0:94:66:52:07:57 inet addr:192.168.1.10 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::d294:66ff:fe52:757/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3217625896 errors:0 dropped:1996 overruns:0 frame:0 TX packets:3417264542 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:2364841876835 (2.3 TB) TX bytes:2940832952714 (2.9 TB) Memory:93300000-933fffff eth1 Link encap:Ethernet HWaddr d0:94:66:52:07:58 inet addr:192.168.1.20 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::d294:66ff:fe52:758/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:2387702 errors:0 dropped:2071 overruns:0 frame:0 TX packets:1052 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:275109861 (275.1 MB) TX bytes:100254 (100.2 KB) Memory:93200000-932fffff 有时，你可能想指定以某个 IP 来登录别的 Linux 主机，则可以使用 -b 选项来指定一个 IP地址。这个 IP 将会用作建立连接的源地址。 1ssh -b 192.168.1.20 user1@192.168.1.156 在服务端，可以使用 netstat 命令来检查 ssh 服务的连接建立情况。可以看到 IP 为 192.168.1.20 的连接已经建立。 12$ netstat |grep sshtcp 0 0 192.168.1.156:ssh 192.168.1.20:43838 ESTABLISHED 5. 公钥检查什么是公钥检查： ssh 连接远程主机时，会检查主机的公钥。如果是第一次该主机，会显示该主机的公钥摘要，提示用户是否信任该主机： 1234$ ssh user1@192.168.1.57The authenticity of host &apos;192.168.1.57 (192.168.1.57)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:Gx0RFwV4EiROc6ymV/BmSLE2zNT9AswZvzq9pc6Srwo.Are you sure you want to continue connecting (yes/no)? 当选择接受，就会将该主机的公钥追加到文件 ~/.ssh/known_hosts 中。当再次连接该主机时，就不会再提示该问题了。 如果因为某种原因（服务器系统重装，服务器间IP地址交换，DHCP，虚拟机重建，中间人劫持），该IP地址的公钥改变了，当使用 SSH 连接的时候，会报错： 12345678910111213@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that the RSA host key has just been changed.The fingerprint for the RSA key sent by the remote host ise9:0c:36:89:7f:3c:07:71:09:5a:9f:28:8c:44:e9:05.Please contact your system administrator.Add correct host key in /home/jiangxin/.ssh/known_hosts to get rid of this message.Offending key in /home/jiangxin/.ssh/known_hosts:81RSA host key for 192.168.0.110 has changed and you have requested strict checking.Host key verification failed. 上面的警告信息说的是： 服务器公钥已经改变，新的公钥的摘要是：e9:0c:36:89:7f:3c:07:71:09:5a:9f:28:8c:44:e9:05. 该服务器原来的公钥记录在文件 ~/.ssh/known_hosts 中第 81 行。 如果确认不是中间人劫持，需要连接到该服务器，怎么办呢？最简单的就是用 vi 打开 ~/.ssh/known_hosts 文件，定位到 81 行，将该行删除。之后就可以使用 ssh 连接了。 我之前在项目中，基于 OpenStack 来搭建 SDN 网络时经常碰到上面的错误，因为经常会将一些云主机给删掉而将他们的 IP 分给新起的云主机。再用这个 IP 来登录新的主机时就会碰到这个问题，以前还不太明白咋回事，现在有种恍然大悟的感觉。 如何不进行公钥检查： 在首次连接服务器时，会弹出公钥确认的提示。这会导致某些自动化任务，由于初次连接服务器而导致自动化任务中断。或者由于 ~/.ssh/known_hosts 文件内容清空，导致自动化任务中断。 SSH 客户端的 StrictHostKeyChecking 配置指令，可以实现当第一次连接服务器时，自动接受新的公钥。只需要修改 /etc/ssh/ssh_config 文件，包含下列语句： 12Host * StrictHostKeyChecking no 或者在 ssh 命令行中用 -o 参数： 1ssh -o StrictHostKeyChecking=no user1@192.168.1.110 公钥检查确实很烦人，我以前在写自动化登录脚本时就觉得很棘手，导致自动化登录脚本经常出错。早知道是这个问题就好办多了，感觉写博客总结一下这些内容对于自己挺有帮助的。 ssh 登录本机的问题不试不知道，一试才知道 ssh 登录本机其实和登录其他的主机是一样的，也需要输入密码验证，免密登录也需要将自己的公钥记录在 ~/.ssh/authorized_keys 文件中。ssh 登录本机可以使用本机的任意一个 IP 或者也可以用回环地址 127.0.0.1 。 完结。。以上内容就是个人在学习工作中关于 ssh 的一些总结，感觉里面的学问真的是太大了，写了这么多其实还有很多很多相关的知识是我不知道不清楚的，不过学习就是这样的，总是由浅入深，不可能一下子就把所有的弄明白，剩下的就以后再慢慢总结啦！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[frp 内网穿透实现 ssh 访问内网主机]]></title>
    <url>%2F2018%2F09%2F14%2Ffrp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E5%AE%9E%E7%8E%B0ssh%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8%2F</url>
    <content type="text"><![CDATA[本文目的frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。 本文将基于 frp 来实现内网穿透，从而实现从外网 ssh 登录内网主机，而不对 frp 其他的应用做过多的说明。 frp 的作用 利用处于内网或防火墙后的机器，对外网环境提供 http 或 https 服务。 对于 http, https 服务支持基于域名的虚拟主机，支持自定义域名绑定，使多个域名可以共用一个80端口。 利用处于内网或防火墙后的机器，对外网环境提供 tcp 和 udp 服务，例如在家里通过 ssh 访问处于公司内网环境内的主机。 架构 在具有公网IP的服务器或VPS上安装运行 frp 的服务端程序frps，并在处于内网的目标主机上面安装运行 frp 的客户端程序 frpc ，然后 User 就可以通过公网服务器来实现内网穿透从而访问内网主机。 配置 ssh 访问内网机器根据不同的系统架构选择不同的安装包，从 frp下载链接 上下载合适的安装包。由于本人的公网服务器是 vultr 上面的一台 VPS，操作系统是 CentOS 7；内网机器也是一台 Linux 服务器，操作系统是 Ubuntu 16.04，所以我使用的安装包是 frp_0.21.0_linux_amd64.tar.gz 。配置步骤如下： 下载安装包 在公网服务器和内网机器上都要下载安装包并解压： 12$ wget https://github.com/fatedier/frp/releases/download/v0.21.0/frp_0.21.0_linux_amd64.tar.gz$ tar -xzvf frp_0.21.0_linux_amd64.tar.gz 解压之后的文件夹中既包含了服务端的文件又包括客户端的文件，所以可以分别在两个机器上删除掉不必要的文件，也可以不删，都没有影响。强迫症还是来删一下，在解压后的文件夹中： 在公网服务器上删除客户端相关的文件，只保留一下两个文件： 1frps frps.ini 在内网机器上删除服务端相关的文件，只保留以下两个文件： 1frpc frpc.ini 在公网服务器上配置并启动 修改配置文件 $ vi frps.ini，如下： 12[common]bind_port = 7000 #frp服务端端口（必须） 配置很简单，然后启动： 1$ nohup ./frps -c frps.ini &amp; 查看 nohup.out 的信息，success 12345678# tail -f nohup.out 2018/09/14 05:33:15 [I] [service.go:130] frps tcp listen on 0.0.0.0:70002018/09/14 05:33:15 [I] [root.go:207] Start frps success2018/09/14 05:49:47 [I] [service.go:130] frps tcp listen on 0.0.0.0:70002018/09/14 05:49:47 [I] [root.go:207] Start frps success2018/09/14 06:28:59 [I] [service.go:319] client login info: ip [125.71.219.33:37092] version [0.21.0] hostname [] os [linux] arch [amd64]2018/09/14 06:28:59 [I] [proxy.go:217] [93eec0dde173fc68] [ssh] tcp proxy listen port [6000]2018/09/14 06:28:59 [I] [control.go:335] [93eec0dde173fc68] new proxy [ssh] success 在内网机器上配置并启动 修改配置文件 $ vi frpc.ini，如下： 12345678910[common]server_addr = 0.0.0.0 #frp服务端地址，可以填ip或者域名，这里假设为0.0.0.0server_port = 7000 #frp服务端端口，即填写服务端配置中的 bind_port[ssh]type = tcp #连接类型，填tcp或udplocal_ip = 127.0.0.1 #填127.0.0.1或内网ip都可以local_port = 22 #需要转发到的端口，ssh端口是22remote_port = 6000 #frp服务端的远程监听端口，即你访问服务端的remote_port就相当于访 #问客户端的 local_port，如果填0则会随机分配一个端口 启动客户端程序： 1$ nohup ./frpc -c frpc.ini &amp; 查看 nohup.out 的信息，success 1234567$ tail -f nohup.out2018/09/14 14:28:58 [I] [proxy_manager.go:300] proxy removed: []2018/09/14 14:28:58 [I] [proxy_manager.go:310] proxy added: [ssh]2018/09/14 14:28:58 [I] [proxy_manager.go:333] visitor removed: []2018/09/14 14:28:58 [I] [proxy_manager.go:342] visitor added: []2018/09/14 14:28:59 [I] [control.go:246] [93eec0dde173fc68] login to server success, get run id [93eec0dde173fc68], server udp port [0]2018/09/14 14:29:00 [I] [control.go:169] [93eec0dde173fc68] [ssh] start proxy success 登录 完成前面三步的配置就可以登录对应的内网机器了，执行 ssh 命令： 123ssh -oPort=6000 username@server_addr或者ssh -p 6000 username@server_addr 上面登录使用的 username 是内网机器的用户名，server_addr是公网服务器的IP，port 6000就是设置的 remote_port，最后的登录密码是内网机器的密码，而不是公网机器的密码，这一点一定要注意。 问题解决在启动服务端和客户端程序之后，可能发现还是无法登录到内网内网机器，在内网机器上面执行 tail -f nohup.out 查看启动命令的执行结果，可以发现以下的问题： 12345$ tail -f nohup.out 2018/09/14 14:11:02 [I] [proxy_manager.go:333] visitor removed: []2018/09/14 14:11:02 [I] [proxy_manager.go:342] visitor added: []2018/09/14 14:13:09 [W] [control.go:113] login to server failed: dial tcp xxx.xxx.xxx.xxx:7000: connect: connection timed outdial tcp xxx.xxx.xxx.xxx:7000: connect: connection timed out 仔细检查了一下，发现是我公网服务器防火墙的原因，没有允许对应端口的流量通过，所以需要配置防火墙： 123# firewall-cmd --zone=public --add-port=7000/tcp --permanent# firewall-cmd --zone=public --add-port=6000/tcp --permanent# firewall-cmd --reload 上面配置防火墙的命令是针对 CentOS 的，如果是别的系统不太一样，如 Ubuntu 通过iptables命令来配置。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
        <tag>frp</tag>
        <tag>内网穿透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell中set指令的用法]]></title>
    <url>%2F2018%2F09%2F13%2Fshell%E4%B8%ADset%E6%8C%87%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[语法1set [-可选参数] [-o 选项] 功能说明set 指令可根据不同的需求来设置当前所使用 shell 的执行方式，同时也可以用来设置或显示 shell 变量的值。当指定某个单一的选项时将设置 shell 的常用特性，如果在选项后使用 -o 参数将打开特殊特性，若是 +o 将关闭相应的特殊特性。而不带任何参数的 set 指令将显示当前 shell 中的全部变量，且总是返回 true，除非遇到非法的选项。 参数说明可选参数及其说明如下： 参数 说明 -a 标示已修改的变量，以供输出至环境变量 -b 使被中止的后台程序立刻回报执行状态 -d Shell预设会用杂凑表记忆使用过的指令，以加速指令的执行。使用-d参数可取消 -e 若指令传回值不等于0，则立即退出shell -f 取消使用通配符 -h 自动记录函数的所在位置 -k 指令所给的参数都会被视为此指令的环境变量 -l 记录for循环的变量名称 -m 使用监视模式 -n 测试模式，只读取指令，而不实际执行 -p 启动优先顺序模式 -P 启动-P参数后，执行指令时，会以实际的文件或目录来取代符号连接 -t 执行完随后的指令，即退出shell -u 当执行时使用到未定义过的变量，则显示错误信息 -v 显示shell所读取的输入值 -H shell 可利用”!”加&lt;指令编号&gt;的方式来执行 history 中记录的指令 -x 执行指令后，会先显示该指令及所下的参数 +&lt;参数&gt; 取消某个set曾启动的参数。与-&lt;参数&gt;相反 -o option 特殊属性有很多，大部分与上面的可选参数功能相同，这里就不列了 重点参数最常用的两个参数就是 -e 与 -x ，一般写在 shell 代码逻辑之前，这两个组合在一起用，可以在 debug 的时候替你节省许多时间 。 set -x 会在执行每一行 shell 脚本时，把执行的内容输出来。它可以让你看到当前执行的情况，里面涉及的变量也会被替换成实际的值。 set -e 会在执行出错时结束程序，就像其他语言中的“抛出异常”一样。（准确说，不是所有出错的时候都会结束程序，见下面的注） 注：set -e结束程序的条件比较复杂，在man bash里面，足足用了一段话描述各种情景。大多数执行都会在出错时退出，除非 shell 命令位于以下情况： 一个 pipeline 的非结尾部分，比如error | ok 一个组合语句的非结尾部分，比如ok &amp;&amp; error || other 一连串语句的非结尾部分，比如error; ok 位于判断语句内，包括test、if、while等等。 其他用法 set：初始化位置参数 调用 set 是接一个或多个参数时，set 会把参数的值赋予位置参数，从 $1 开始赋值。如下例子： 1234567$ cat set-it.sh#!/bin/bashset first second thirdecho $3 $2 $1$ ./set-it.shthird second first 如上，在执行 set-it.sh 脚本时并没有输入参数，但是使用 set 指令后会对位置参数进行赋值。 set：显示 shell 变量 如果不带任何参数的使用 set 命令，set 指令就会显示一列已设置的 shell 变量，包括用户定义的变量和关键字变量。 12345$ set BASH_VERSION=&apos;4.2.24(1)-release&apos;COLORS=/etc/DIR_COLORSMAIL=/var/spool/mail/username...]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger Fabric 多机环境部署]]></title>
    <url>%2F2018%2F09%2F12%2FFabric%E5%A4%9A%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言本文主要基于 Hyperledger Fabric v1.0 的单机环境部署文档来搭建多机环境。其中对于Fabric 的基础环境的配置都是一样的，也就是前 6 步都是一样的。配置好基础环境之后将配置好的虚拟机镜像复制 4 份，作为其他节点的镜像，该环境包括 5 个节点，是 4 Peer + 1 Orderer的架构，如下表： 主机名 IP地址 orderer.example.com 192.168.1.130 peer0.org1.example.com 192.168.1.188 peer1.org1.example.com 192.168.1.186 peer0.org2.example.com 192.168.1.193 peer1.org2.example.com 192.168.1.112 环境部署1. 更换 apt 源先备份 sources.list 文件： 1$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak 再修改 sources.list 文件，换成阿里云的国内源： 123456789101112131415161718$ sudo vim /etc/apt/sources.listdeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 最后更新一下源： 1$ sudo apt-get update 2. 安装 curlUbuntu 16.04 一般默认是安装了 curl 的，可以通过以下命令验证： 1234$ curl -Vcurl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets 如果没有安装，则通过 apt-get 安装： 1$ sudo apt-get install curl 3. 安装 Docker 由于 apt 源使用HTTPS以确保软件下载过程中不被篡改。因此，我们首先需要添加使用HTTPS传输的软件包以及CA证书。 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 为了确认所下载软件包的合法性，需要添加软件源的 GPG 秘钥 1$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 然后，我们需要向 sources.list 中添加 Docker 软件源 1234$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 以上命令会添加稳定版本的Docker CE apt 镜像源。 更新 apt 软件包缓存，并安装 docker-ce： 123$ sudo apt-get update$ sudo apt-get install docker-ce 查看 Docker 版本： 12$ docker -vDocker version 18.06.0-ce, build 0ffa825 满足官方文档中 Docker version 17.06.2-ce or greater is required 的要求。 启动 Docker CE 12$ sudo systemctl enable docker$ sudo systemctl start docker 建立 docker 用户组 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 1$ sudo groupadd docker 其实一般按照上面的方法安装 Docker 后就已经创建好 docker 用户组了，可以使用 $ cat /etc/group | grep docker 命令来验证，所以就不需要再建立 docker 用户组了，再建立也会报错提示用户组已存在的。 将当前用户加入 docker 用户组： 1$ sudo usermod -aG docker $USER 下次登录时即可方便的使用 docker 命令。 测试 Docker 是否安装正确 123456789101112131415161718192021222324252627$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-world9db2ca6ccae0: Pull complete Digest: sha256:4b8ff392a12ed9ea17784bd3c9a8b1fa3299cac44aca35a85c90c5e3c7afacdcStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 配置镜像加速器 国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如： Docker 官方提供的中国 registry mirror https://registry.docker-cn.com 七牛云加速器 https://reg-mirror.qiniu.com/ 当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。 国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。 我们以 Docker 官方加速器 https://registry.docker-cn.com 为例进行介绍。 在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在则创建） 12345&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125; 之后重新启动服务 12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 4. 安装 Docker Compose 通过二进制包来安装，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。 123$ sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 查看 Docker compose 版本 12$ docker-compose --versiondocker-compose version 1.22.0, build f46880fe 满足官方文档中 Docker Compose version 1.14.0 or greater 的要求。 5. 安装 Go 语言环境Hyperledger Fabric 在很多组件中使用了 Go 语言，并且 Hyperledger fabric 1.2.0 要求使用的是 GO version 1.10.x ，所以需要在我们的环境中安装对应的 Go 语言。 从官网下载 1.10.x 版本的 Linux 平台的源码包 1$ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz 解压到指定目录 1$ sudo tar zxvf go1.10.3.linux-amd64.tar.gz -C /usr/local/ 先创建 Go 的工作目录 1$ mkdir ~/go 配置环境变量 12345$ vi ~/.bashrc添加export GOROOT=/usr/local/goexport GOPATH=/home/user1/goexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin 保存并使生效： 1$ source ~/.bashrc 测试 Go 的 demo 程序 123456789101112131415161718$ cd ~/go$ vi hello.gopackage mainimport &quot;fmt&quot;func main() &#123; fmt.Printf(&quot;hello world\n&quot;)&#125;$ go build hello.go$ lshello hello.go$ ./hellohello world 6. Fabric 源码下载 首先创建存放源码的文件夹： 1$ mkdir -p ~/go/src/github.com/hyperledger 使用 Git 下载完整源码： 1$ git clone https://github.com/hyperledger/fabric.git 进入 fabric 目录查看版本分支并切换分支： 1234$ cd fabric$ git branch* release-1.2$ git checkout v1.0.0 最后再解决 examples/e2e_cli/base/peer-base.yaml 文件中的一个小 bug： 将 CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE 环境变量的值修改为 e2e_cli_default，而不是原来的 e2ecli_default。 1- CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=e2e_cli_default 不然在启动 Fabric 网络时会出现问题。 7. 启动其他节点将刚才配置好的虚拟机镜像拷贝四份，并基于这些镜像启动 5 台虚拟机，主机名分别设置为： orderer.example.com peer0.org1.example.com peer1.org1.example.com peer0.org2.example.com peer1.org2.example.com 8. docker-compose 配置文件准备在 fabric 的源码中，提供了单机部署的 4 Peer+1 Orderer的示例，在 example/e2e_cli 文件夹下。我们将登录到 orderer.example.com 节点，生成公私钥，修改 docker-compose 配置文件，然后将 e2e_cli 整个文件夹分发到 Peer 节点，再分别登录到不同的 Peer 节点完成少部分的个性化配置工作。 8.1 生成公私钥、证书、创世区块等公私钥和证书是用于 Server 和 Server 之间的安全通信，另外要创建 Channel 并让其他节点加入 Channel 就需要创世区块，这些必备文件都可以一个命令生成，官方已经给出了脚本： 1$ ./generateArtifacts.sh mychannel 运行这个命令后，系统会创建 channel-artifacts 文件夹，里面包含了 mychannel 这个通道相关的文件，另外还有一个 crypto-config 文件夹，里面包含了各个节点的公私钥和证书的信息。 8.2 设置 Peer 节点的 docker-compose 文件e2e_cli 中提供了多个yaml文件，我们可以基于docker-compose-cli.yaml文件创建： 1$ cp docker-compose-cli.yaml docker-compose-peer.yaml 然后修改 docker-compose-peer.yaml，去掉 orderer 的配置，只保留一个 peer 和 cli，因为我们要多机部署，节点与节点之前又是通过主机名通讯，所以需要修改容器中的host文件，也就是 extra_hosts 设置，修改后的peer配置如下： 1234567peer0.org1.example.com: container_name: peer0.org1.example.com extends: file: base/docker-compose-base.yaml service: peer0.org1.example.com extra_hosts: - &quot;orderer.example.com:192.168.1.130&quot; 同样，cli也需要能够和各个节点通讯，所以cli下面也需要添加 extra_hosts 设置，去掉无效的依赖，并且去掉command 这一行，因为我们是每个 peer 都会有个对应的客户端，也就是 cli，所以我只需要去手动执行一次命令，而不是自动运行。修改后的cli配置如下： 12345678910111213141516171819202122232425262728293031cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer volumes: - /var/run/:/host/var/run/ - ../chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/examples/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts depends_on: - peer0.org1.example.com extra_hosts: - &quot;orderer.example.com:192.168.1.130&quot; - &quot;peer0.org1.example.com:192.168.1.188&quot; - &quot;peer1.org1.example.com:192.168.1.186&quot; - &quot;peer0.org2.example.com:192.168.1.193&quot; - &quot;peer1.org2.example.com:192.168.1.112&quot; 在单机模式下，4个peer会映射主机不同的端口，但是我们在多机部署的时候是不需要映射不同端口的，所以需要修改 base/docker-compose-base.yaml 文件，将所有 peer 的端口映射都改为相同的： 1234ports: - 7051:7051 - 7052:7052 - 7053:7053 8.3 设置 Orderer 节点的 docker-compose 文件与创建peer的配置文件类似，我们也复制一个yaml文件出来进行修改： 1$ cp docker-compose-cli.yaml docker-compose-orderer.yaml orderer 服务器上我们只需要保留 order 设置，其他 peer 和 cli 设置都可以删除。orderer 可以不设置 extra_hosts。 8.4 分发配置文件前面 3 步操作，都是在 orderer.example.com上完成的，接下来我们需要将这些文件分发到另外4台服务器上。Linux 之间的文件传输，我们可以使用 scp 命令。 peer 节点上已经存在了 e2e_cli 文件夹，使用 scp 命令发送时会直接覆盖。在 orderer.example.com 节点的 fabric 的根目录下的 examples 目录下执行以下命令： 1234$ scp -r e2e_cli/ user1@192.168.1.188:/home/user1/go/src/github.com/hyperledger/fabric/examples$ scp -r e2e_cli/ user1@192.168.1.186:/home/user1/go/src/github.com/hyperledger/fabric/examples$ scp -r e2e_cli/ user1@192.168.1.193:/home/user1/go/src/github.com/hyperledger/fabric/examples$ scp -r e2e_cli/ user1@192.168.1.112:/home/user1/go/src/github.com/hyperledger/fabric/examples 8.5 Peer 节点的个性化配置因为之前配置的就是peer0.org1.example.com 节点，所以复制到peer0.org1.example.com 后不需要做任何修改。 复制到 peer1.org1.example.com 上，我们需要对 docker-compose-peer.yaml 做一个小小的修改，将启动的容器改为 peer1.org1.example.com，并且添加 peer0.org1.example.com 的 IP 映射，对应的 cli 中也改成对 peer1.org1.example.com 的依赖。这是修改后的 peer1.org1.example.com 上的配置文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344version: &apos;2&apos;services: peer1.org1.example.com: container_name: peer1.org1.example.com extends: file: base/docker-compose-base.yaml service: peer1.org1.example.com extra_hosts: - &quot;orderer.example.com:192.168.1.130&quot; - &quot;peer0.org1.example.com:192.168.1.188&quot; cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer volumes: - /var/run/:/host/var/run/ - ../chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/examples/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts depends_on: - peer1.org1.example.com extra_hosts: - &quot;orderer.example.com:192.168.1.130&quot; - &quot;peer0.org1.example.com:192.168.1.188&quot; - &quot;peer1.org1.example.com:192.168.1.186&quot; - &quot;peer0.org2.example.com:192.168.1.193&quot; - &quot;peer1.org2.example.com:192.168.1.112&quot; peer0.org2.example.com 和 peer0.org2.example.com 节点上面的 docker-compose-peer.yaml 也是同理修改。 注意需要修改组织相关的信息。 9. 启动 Fabric现在所有文件都已经准备完毕，我们可以启动我们的Fabric网络了。 9.1 启动 Orderer让我们首先来启动orderer节点，在orderer服务器上运行： 1$ docker-compose -f docker-compose-orderer.yaml up –d 运行完毕后我们可以使用 docker ps 看到运行了一个名字为 orderer.example.com 的节点。 9.2 启动 peer然后我们切换到 peer0.org1.example.com 服务器，启动本服务器的 peer 节点和 cli，命令为： 1$ docker-compose -f docker-compose-peer.yaml up –d 运行完毕后我们使用docker ps应该可以看到2个正在运行的容器。 接下来依次在另外 3 台服务器运行启动peer节点容器的命令。 现在我们整个Fabric4+1服务器网络已经成型，接下来是创建channel和运行ChainCode。 9.3 创建 Channel 测试 Chaincode我们切换到 peer0.org1.example.com 服务器上，使用该服务器上的 cli 来运行创建 Channel 和运行 ChainCode 的操作。首先进入cli 容器： 1$ docker exec -it cli bash 进入容器后我们可以看到命令提示变为： root@ad739321be2a:/opt/gopath/src/github.com/hyperledger/fabric/peer# 说明我们已经以 root 的身份进入到 cli 容器内部。官方已经提供了完整的创建 Channel 和测试 ChainCode 的脚本，并且已经映射到 cli 容器内部，所以我们只需要在 cli 内运行如下命令： 1./scripts/script.sh mychannel 那么该脚本就可以一步一步的完成创建通道，将其他节点加入通道，更新锚节点，创建 ChainCode，初始化账户，查询，转账，再次查询等链上代码的各个操作都可以自动化实现。直到最后，系统提示： 1===================== All GOOD, End-2-End execution completed ===================== 说明我们的 4+1 的 Fabric 多机部署成功了。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Hyperledger fabric</tag>
        <tag>Docker</tag>
        <tag>Docker Compose</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric e2e_cli 案例流程分析]]></title>
    <url>%2F2018%2F08%2F30%2FFabric%E6%A1%88%E4%BE%8B%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[0. Fabric e2e_cli 案例的运行流程在部署好 Hyperledger Fabric v1.0.0 的环境之后，我们通常会运行其 e2e_cli 的案例。在 fabric/examples/e2e_cli 目录下有一个 shell 脚本：network_setup.sh，这就是 e2e_cli 项目的入口，运行 ./network_setup.sh up 即可启动 fabric 的网络并完成相关的测试，然后运行 ./network_setup.sh down 可以删除相关的容器以及文件来结束测试。下面我们就来分析 e2e_cli 案例的运行流程。 首先看看 network_setup.sh 的具体内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#!/bin/bash## Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#UP_DOWN=&quot;$1&quot;CH_NAME=&quot;$2&quot;CLI_TIMEOUT=&quot;$3&quot;IF_COUCHDB=&quot;$4&quot;: $&#123;CLI_TIMEOUT:=&quot;10000&quot;&#125;COMPOSE_FILE=docker-compose-cli.yamlCOMPOSE_FILE_COUCH=docker-compose-couch.yaml#COMPOSE_FILE=docker-compose-e2e.yamlfunction printHelp () &#123; echo &quot;Usage: ./network_setup &lt;up|down&gt; &lt;\$channel-name&gt; &lt;\$cli_timeout&gt; &lt;couchdb&gt;.\nThe arguments must be in order.&quot;&#125;function validateArgs () &#123; if [ -z &quot;$&#123;UP_DOWN&#125;&quot; ]; then echo &quot;Option up / down / restart not mentioned&quot; printHelp exit 1 fi if [ -z &quot;$&#123;CH_NAME&#125;&quot; ]; then echo &quot;setting to default channel &apos;mychannel&apos;&quot; CH_NAME=mychannel fi&#125;function clearContainers () &#123; CONTAINER_IDS=$(docker ps -aq) if [ -z &quot;$CONTAINER_IDS&quot; -o &quot;$CONTAINER_IDS&quot; = &quot; &quot; ]; then echo &quot;---- No containers available for deletion ----&quot; else docker rm -f $CONTAINER_IDS fi&#125;function removeUnwantedImages() &#123; DOCKER_IMAGE_IDS=$(docker images | grep &quot;dev\|none\|test-vp\|peer[0-9]-&quot; | awk &apos;&#123;print $3&#125;&apos;) if [ -z &quot;$DOCKER_IMAGE_IDS&quot; -o &quot;$DOCKER_IMAGE_IDS&quot; = &quot; &quot; ]; then echo &quot;---- No images available for deletion ----&quot; else docker rmi -f $DOCKER_IMAGE_IDS fi&#125;function networkUp () &#123; if [ -f &quot;./crypto-config&quot; ]; then echo &quot;crypto-config directory already exists.&quot; else #Generate all the artifacts that includes org certs, orderer genesis block, # channel configuration transaction source generateArtifacts.sh $CH_NAME fi if [ &quot;$&#123;IF_COUCHDB&#125;&quot; == &quot;couchdb&quot; ]; then CHANNEL_NAME=$CH_NAME TIMEOUT=$CLI_TIMEOUT docker-compose -f $COMPOSE_FILE -f $COMPOSE_FILE_COUCH up -d 2&gt;&amp;1 else CHANNEL_NAME=$CH_NAME TIMEOUT=$CLI_TIMEOUT docker-compose -f $COMPOSE_FILE up -d 2&gt;&amp;1 fi if [ $? -ne 0 ]; then echo &quot;ERROR !!!! Unable to pull the images &quot; exit 1 fi docker logs -f cli&#125;function networkDown () &#123; docker-compose -f $COMPOSE_FILE down #Cleanup the chaincode containers clearContainers #Cleanup images removeUnwantedImages # remove orderer block and other channel configuration transactions and certs rm -rf channel-artifacts/*.block channel-artifacts/*.tx crypto-config&#125;validateArgs#Create the network using docker composeif [ &quot;$&#123;UP_DOWN&#125;&quot; == &quot;up&quot; ]; then networkUpelif [ &quot;$&#123;UP_DOWN&#125;&quot; == &quot;down&quot; ]; then ## Clear the network networkDownelif [ &quot;$&#123;UP_DOWN&#125;&quot; == &quot;restart&quot; ]; then ## Restart the network networkDown networkUpelse printHelp exit 1fi 1. 启动过程1.1 network_setup.sh 参数设置 传入参数 在运行 ./network_setup.sh up 启动的时候，我们传入了 up 这个参数，使得内置的 UP_DOWN 参数设置成了 up，也就是确定执行类型为启动 fabric 网络，但其实该脚本还允许传入 3 个参数，分别是 CH_NAME（通道名称，默认设为 ”mychannel“）、CLI_TIMEOUT（客户端超时设置，默认设为 10000）和 IF_COUCHDB（是否启动 CouchDB 版本 yaml 文件，默认为不启用）。 验证参数：validateArgs 在 network_setup.sh 脚本 88 行左右，调用了 validateArgs 函数来验证参数是否合法，如果不合法，也就是没有指定 up/down/restart，会打印帮助信息并退出脚本；如果合法，$UP_DOWN 设置合法，则判断是否指定 $CH_NAME，没有指定则设置为默认的 mychannel。 1.2 启动网络：networkUp 判断 crypto-config 目录是否存在 如果该目录不存在，会调用 generateArtifacts.sh 脚本创建 crypto-config 目录及所需的区块链网络整数等文件，命令如下： 1source generateArtifacts.sh $CH_NAME 详细请看 1.3 节 判断是否启用 CouchDB 如果启用了 CouchDB，会执行以下 docker-compose 命令： 1CHANNEL_NAME=$CH_NAME TIMEOUT=$CLI_TIMEOUT docker-compose -f $COMPOSE_FILE -f $COMPOSE_FILE_COUCH up -d 2&gt;&amp;1 否则执行： 1CHANNEL_NAME=$CH_NAME TIMEOUT=$CLI_TIMEOUT docker-compose -f $COMPOSE_FILE up -d 2&gt;&amp;1 详细请看 1.4 节 最后会查看 cli 容器的日志： 1docker logs -f cli 详细请看 1.5 节 1.3 调用 generateArtifacts.sh先看看 generateArtifacts.sh 脚本的内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#!/bin/bash +x## Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0##set -eCHANNEL_NAME=$1: $&#123;CHANNEL_NAME:=&quot;mychannel&quot;&#125;echo $CHANNEL_NAMEexport FABRIC_ROOT=$PWD/../..export FABRIC_CFG_PATH=$PWDechoOS_ARCH=$(echo &quot;$(uname -s|tr &apos;[:upper:]&apos; &apos;[:lower:]&apos;|sed &apos;s/mingw64_nt.*/windows/&apos;)-$(uname -m | sed &apos;s/x86_64/amd64/g&apos;)&quot; | awk &apos;&#123;print tolower($0)&#125;&apos;)## Using docker-compose template replace private key file names with constantsfunction replacePrivateKey () &#123; ARCH=`uname -s | grep Darwin` if [ &quot;$ARCH&quot; == &quot;Darwin&quot; ]; then OPTS=&quot;-it&quot; else OPTS=&quot;-i&quot; fi cp docker-compose-e2e-template.yaml docker-compose-e2e.yaml CURRENT_DIR=$PWD cd crypto-config/peerOrganizations/org1.example.com/ca/ PRIV_KEY=$(ls *_sk) cd $CURRENT_DIR sed $OPTS &quot;s/CA1_PRIVATE_KEY/$&#123;PRIV_KEY&#125;/g&quot; docker-compose-e2e.yaml cd crypto-config/peerOrganizations/org2.example.com/ca/ PRIV_KEY=$(ls *_sk) cd $CURRENT_DIR sed $OPTS &quot;s/CA2_PRIVATE_KEY/$&#123;PRIV_KEY&#125;/g&quot; docker-compose-e2e.yaml&#125;## Generates Org certs using cryptogen toolfunction generateCerts ()&#123; CRYPTOGEN=$FABRIC_ROOT/release/$OS_ARCH/bin/cryptogen if [ -f &quot;$CRYPTOGEN&quot; ]; then echo &quot;Using cryptogen -&gt; $CRYPTOGEN&quot; else echo &quot;Building cryptogen&quot; make -C $FABRIC_ROOT release fi echo echo &quot;##########################################################&quot; echo &quot;##### Generate certificates using cryptogen tool #########&quot; echo &quot;##########################################################&quot; $CRYPTOGEN generate --config=./crypto-config.yaml echo&#125;## Generate orderer genesis block , channel configuration transaction and anchor peer update transactionsfunction generateChannelArtifacts() &#123; CONFIGTXGEN=$FABRIC_ROOT/release/$OS_ARCH/bin/configtxgen if [ -f &quot;$CONFIGTXGEN&quot; ]; then echo &quot;Using configtxgen -&gt; $CONFIGTXGEN&quot; else echo &quot;Building configtxgen&quot; make -C $FABRIC_ROOT release fi echo &quot;##########################################################&quot; echo &quot;######### Generating Orderer Genesis block ##############&quot; echo &quot;##########################################################&quot; # Note: For some unknown reason (at least for now) the block file can&apos;t be # named orderer.genesis.block or the orderer will fail to launch! $CONFIGTXGEN -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block echo echo &quot;#################################################################&quot; echo &quot;### Generating channel configuration transaction &apos;channel.tx&apos; ###&quot; echo &quot;#################################################################&quot; $CONFIGTXGEN -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID $CHANNEL_NAME echo echo &quot;#################################################################&quot; echo &quot;####### Generating anchor peer update for Org1MSP ##########&quot; echo &quot;#################################################################&quot; $CONFIGTXGEN -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSP echo echo &quot;#################################################################&quot; echo &quot;####### Generating anchor peer update for Org2MSP ##########&quot; echo &quot;#################################################################&quot; $CONFIGTXGEN -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org2MSP echo&#125;generateCertsreplacePrivateKeygenerateChannelArtifacts 脚本最后调用了三个函数： generateCerts 该函数首先判断 fabric/release/linux-amd64/bin/cryptogen 是否存在，如果不存在则先编译生成，这个是 Fabric 基于 Go 语言的 crypto 库提供的工具，通过 cryptogen 可以快速地根据配置自动批量生成所需要的密钥和证书文件。cryptogen 根据 crypto-config.yaml 文件读入网络的拓扑结构，执行的命令如下： 1fabric/release/linux-amd64/bin/cryptogen generate --config=./crypto-config.yaml crypto-config.yaml 配置文件可以指定两类组织的信息： OrdererOrgs：构成 Orderer 集群的节点所属组织； PeerOrgs：构成 Peer 集群的节点所属的组织 每个组织拥有： 名称（name）：组织的名称； 组织域（Domain）：组织的命名域； CA：组织的 CA 地址，包括 Hostname 域； 若干节点（Node）：一个节点包括 Hostname、CommonName、SANS 等域，可以用 Specs 字段指定一组节点，或者用 Template 字段指定自动生成节点的个数； 用户（User）模板：自动生成除 admin 外的用户个数。 示例如下： 123456789101112131415161718OrdererOrgs: - Name: Orderer Domain: example.com Specs: - Hostname: ordererPeerOrgs: - Name: Org1 Domain: org1.example.com Template: Count: 2 Users: Count: 1 - Name: Org2 Domain: org2.example.com Template: Count: 2 Users: Count: 1 上面的示例配置中，Orderer 组织通过 Specs 字段指定了一个主机 order.example.com；而两个 Peer 组织则采用 Template 来自动生成了 Count 个数的主机。 同样，Users 字段下的 Count 字段值会让 cryptogen 工具以自动顺序生成指定个数的普通用户（除默认的 Admin 用户外）。 replacePrivatekey 在这个函数中，会替换 docker-compose-e2e-template.yaml 文件中的 CA1_PRIVATE_KEY 为当前目录 crypto-config/peerOrganizationsorg1.example.com/ca/ 下的以 _sk 结尾的文件名，同时替换 CA2_PRIVATE_KEY 为当前目录crypto-config/peerOrganizationsorg2.example.com/ca/ 下的以 _sk 结尾的文件名。 最终生成的新文件被创建在当前文件夹下并命名为 docker-compose-e2e.yaml，在该文件中定义了 CA 的 CERTFILE 及 KEYFILE，同时也通过 command 内置参数显示启动了 Fabric-CA 服务端。 其实在后续的过程中并没有使用这里生成的这个 docker-compose-e2e.yaml 配置文件，而使用的是 docker-compose-cli.yaml，在 network_setup.sh 脚本中的 COMPOSE_FILE 变量指定了要使用的配置文件，代码如下： 123&gt; COMPOSE_FILE=docker-compose-cli.yaml&gt; #COMPOSE_FILE=docker-compose-e2e.yaml&gt; generateChannelArtifacts 这个函数首先判断 fabric/release/linux-amd64/bin/configtxgen 是否存在，如果不存在则先编译生成。configtxgen 可以配合 cryptogen 生成的组织结构身份文件使用，离线生成跟通道有关的配置信息，其主要功能有如下三个： 生成启动 Orderer 需要的初始化区块，并支持检查区块内容； 生成创建应用通道需要的配置交易，并支持检查交易内容； 生成 2 个锚节点 Peer 的更新配置交易。 configtxgen 调用的配置文件为 configtx.yaml ，该配置文件一般包括四个部分： Profiles：一系列通道配置模板，包括 Orderer 系统通道模板和应用通道类型模板； Organization：一系列组织结构定义，被其他部分引用； Orderer：Orderer 系统通道相关配置，包括 Orderer 服务配置和参与 Ordering 服务的可用组织信息； Application：应用通道相关配置，主要包括参与应用网络的可用组织信息。 调用该函数最终在 channel-artifacts 目录下生成了四个文件，对应于上述的三个功能： 1channel.tx genesis.block Org1MSPanchors.tx Org2MSPanchors.tx 1.4 docker-compose 启动容器服务如 1.2 节所述，默认情况下，会执行以下命令启动容器服务： 1CHANNEL_NAME=$CH_NAME TIMEOUT=$CLI_TIMEOUT docker-compose -f $COMPOSE_FILE up -d 2&gt;&amp;1 其中变量的值为： CHANNEL_NAME=mychannel，TIMEOUT=10000，COMPOSE_FILE=docker-compose-cli.yaml。 -d：指定在后台运行容器； 2&gt;&amp;1：指定同时重定向标准输出（stdout）与标准错误（stderr） 容器启动配置文件 docker-compose-cli.yaml 的内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: orderer.example.com: extends: file: base/docker-compose-base.yaml service: orderer.example.com container_name: orderer.example.com peer0.org1.example.com: container_name: peer0.org1.example.com extends: file: base/docker-compose-base.yaml service: peer0.org1.example.com peer1.org1.example.com: container_name: peer1.org1.example.com extends: file: base/docker-compose-base.yaml service: peer1.org1.example.com peer0.org2.example.com: container_name: peer0.org2.example.com extends: file: base/docker-compose-base.yaml service: peer0.org2.example.com peer1.org2.example.com: container_name: peer1.org2.example.com extends: file: base/docker-compose-base.yaml service: peer1.org2.example.com cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash -c &apos;./scripts/script.sh $&#123;CHANNEL_NAME&#125;; sleep $TIMEOUT&apos; volumes: - /var/run/:/host/var/run/ - ../chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/examples/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts depends_on: - orderer.example.com - peer0.org1.example.com - peer1.org1.example.com - peer0.org2.example.com - peer1.org2.example.com 从配置文件不难看出，排序服务节点继承了 base/docker-compose-base.yaml 中的 order.example.com 属性；而其他四个 Peer 节点继承了 base/docker-compose-base.yaml 中的与之容器名称对应的属性，并又都继承了 base/peer-base.yaml 中的属性。 在上述文件所继承的文件属性中，关键属性如下： environment：当前所配置的外界的环境变量 working_dir：当前容器启动后的工作路径 volumes：外界物理机路径挂载或指引到容器内的路径 ports：指定当前容器启动后映射到物理机上的端口号 depends_on：指定当前容器启动所依赖的启动容器对象 在 docker-compose-cli.yaml 文件中，官方的节点配置信息基本已经满足实际生产应用，而真正使用这些服务节点进行数据维护和管理的则是交由客户端或 SDK 来执行。这里也就是配置文件中的 cli 客户端容器。cli 容器的 command 属性指定当 cli 容器启动后会执行当前目录中的 scripts 目录下的 script.sh 脚本，也就是： 1/bin/bash -c &apos;./scripts/script.sh $&#123;CHANNEL_NAME&#125;; sleep $TIMEOUT&apos; 在 1.5 节中将会分析 cli 容器执行上述脚本的日志记录。 1.5 docker logs -f cli 查看容器日志script.sh 脚本的执行则是 e2e_cli 中真正地对 Peer 节点、频道以及合约的集合操作演示，先来看看该脚本内的函数执行顺序： 123456789101112131415161718192021222324252627282930313233343536373839## Create channelecho &quot;Creating channel...&quot;createChannel## Join all the peers to the channelecho &quot;Having all peers join the channel...&quot;joinChannel## Set the anchor peers for each org in the channelecho &quot;Updating anchor peers for org1...&quot;updateAnchorPeers 0echo &quot;Updating anchor peers for org2...&quot;updateAnchorPeers 2## Install chaincode on Peer0/Org1 and Peer2/Org2echo &quot;Installing chaincode on org1/peer0...&quot;installChaincode 0echo &quot;Install chaincode on org2/peer2...&quot;installChaincode 2#Instantiate chaincode on Peer2/Org2echo &quot;Instantiating chaincode on org2/peer2...&quot;instantiateChaincode 2#Query on chaincode on Peer0/Org1echo &quot;Querying chaincode on org1/peer0...&quot;chaincodeQuery 0 100#Invoke on chaincode on Peer0/Org1echo &quot;Sending invoke transaction on org1/peer0...&quot;chaincodeInvoke 0## Install chaincode on Peer2/Org2echo &quot;Installing chaincode on org2/peer3...&quot;installChaincode 3#Query on chaincode on Peer2/Org2, check if the result is 90echo &quot;Querying chaincode on org2/peer3...&quot;chaincodeQuery 3 90 该脚本设计 Peer 节点及排序服务节点的操作分别有如下九个步骤： createChannel：根据之前在 generateArtifacts.sh 脚本中通过 configtx.yaml 配置文件生成的频道文件创建频道； joinChannel：Peer 节点加入指定频道； updateAnchorPeers 0/2：为频道中的每个组织设置 Peer 节点； installChaincode 0/2：在 Peer0/Org1 和 Peer0/Org2 上安装智能合约； instantiateChaincode 2：在 Peer0/Org2 上对智能合约进行实例化操作； chaincodeQuery 0 100：在 Peer0/Org1 上执行智能合约中的查询方法，判断是否等于 100； chaincodeInvoke 0：在 Peer0/Org1 上执行智能合约中的交易方法； installChaincode 3：在 Peer1/Org2 上安装智能合约； chaincodeQuery 3 90：在 Peer1/Org2 上执行智能合约中的查询方法，判断是否等于 90； 注：这里需要说明的是脚本中最后两步的注释以及回显命令中的内容都有点小问题，”Peer2/Org2” 和 “org2/peer3” 都是不存在的，“3” 其实对应的节点是 Peer1/Org2。看到有的资料在分析这部分内容的时候也是按照错误的注释来解释的，让人很困惑。 上面的脚本会在 cli 客户端容器启动之后执行，在 network_setup.sh 脚本的最后会执行 docker logs -f cli 来查看 cli 容器的日志信息，在开始时会出现如下的字符图： 1234567echoecho &quot; ____ _____ _ ____ _____ _____ ____ _____ &quot;echo &quot;/ ___| |_ _| / \ | _ \ |_ _| | ____| |___ \ | ____|&quot;echo &quot;\___ \ | | / _ \ | |_) | | | _____ | _| __) | | _| &quot;echo &quot; ___) | | | / ___ \ | _ &lt; | | |_____| | |___ / __/ | |___ &quot;echo &quot;|____/ |_| /_/ \_\ |_| \_\ |_| |_____| |_____| |_____|&quot;echo 在结束时显示的字符图如下，并提示执行完成： 1234567891011echoecho &quot;===================== All GOOD, End-2-End execution completed ===================== &quot;echoechoecho &quot; _____ _ _ ____ _____ ____ _____ &quot;echo &quot;| ____| | \ | | | _ \ | ____| |___ \ | ____|&quot;echo &quot;| _| | \| | | | | | _____ | _| __) | | _| &quot;echo &quot;| |___ | |\ | | |_| | |_____| | |___ / __/ | |___ &quot;echo &quot;|_____| |_| \_| |____/ |_____| |_____| |_____|&quot;echo 以上就是e2e_cli 案例启动 fabric 网络的流程分析，接下来分析关闭 fabric 网络的流程。 2. 关闭过程2.1 network_setup.sh 参数设置当执行 ./network_setup.sh down 命令时，down 参数传入脚本作为变量 UP_DOWN 的值，表示关闭 fabric 网络的操作。脚本中判断 $UP_DOWN=down 后将执行 networkDown 函数，如 2.2 节。 2.2 关闭网络：networkDown在 networkDown 函数中，首先会执行 docker-compose 相关的命令来关闭容器服务，然后调用 clearContainers 和 removeUnwantedImages 函数删除相应的容器及镜像，最后会删除 generateArtifacts.sh 脚本创建的 crypto-config 目录和区块链网络证书等文件。如后面 3 节。 2.3 docker-compose 关闭容器服务执行的命令为： 1docker-compose -f $COMPOSE_FILE down $COMPOSE_FILE 就是 docker-compose-cli.yaml 文件。 该命令会将由 docker-compose-cli.yaml 文件创建的容器服务全部关闭。 2.4 删除容器及镜像在运行 e2e_cli 案例之后，环境中会启动总共 9 个容器，使用 docker ps -a 命令查看，这里只列出容器的名字： 123456789orderer.example.com &lt;==Orderer排序节点peer0.org1.example.com &lt;==Peer0/Org1peer1.org1.example.com &lt;==Peer1/Org2peer0.org2.example.com &lt;==Peer0/Org2peer1.org2.example.com &lt;==Peer1/Org2cli &lt;==客户端容器dev-peer0.org1.example.com-mycc-1.0 &lt;==运行智能合约的容器，基于Peer0/Org1安装智能合约后生成dev-peer0.org2.example.com-mycc-1.0 &lt;==运行智能合约的容器，基于Peer0/Org2安装智能合约后生成dev-peer1.org2.example.com-mycc-1.0 &lt;==运行智能合约的容器，基于Peer1/Org2安装智能合约后生成 另外也会生成上面运行智能合约的容器对应的镜像，使用 docker image ls 列出如下： 12345$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEdev-peer1.org2.example.com-mycc-1.0 latest 8c03b3cb4f3f 36 hours ago 173MBdev-peer0.org1.example.com-mycc-1.0 latest 9aa4e69a149f 36 hours ago 173MBdev-peer0.org2.example.com-mycc-1.0 latest f3fef07f5dcc 36 hours ago 173MB 以上的容器和镜像就是要被删除的，这里调用了两个函数：clearContainers 和 removeUnwantedImages 分别来删除，函数内容如下： 1234567891011121314151617function clearContainers () &#123; CONTAINER_IDS=$(docker ps -aq) if [ -z &quot;$CONTAINER_IDS&quot; -o &quot;$CONTAINER_IDS&quot; = &quot; &quot; ]; then echo &quot;---- No containers available for deletion ----&quot; else docker rm -f $CONTAINER_IDS fi &#125;function removeUnwantedImages() &#123; DOCKER_IMAGE_IDS=$(docker images | grep &quot;dev\|none\|test-vp\|peer[0-9]-&quot; | awk &apos;&#123;print $3&#125;&apos;) if [ -z &quot;$DOCKER_IMAGE_IDS&quot; -o &quot;$DOCKER_IMAGE_IDS&quot; = &quot; &quot; ]; then echo &quot;---- No images available for deletion ----&quot; else docker rmi -f $DOCKER_IMAGE_IDS fi &#125; 2.5 删除相关目录文件在 networkDown 函数的最后就是要删除由 generateArtifacts.sh 脚本创建的 crypto-config 目录和区块链网络证书等文件，命令如下： 12# remove orderer block and other channel configuration transactions and certsrm -rf channel-artifacts/*.block channel-artifacts/*.tx crypto-config 结束以上步骤之后就完成了关闭 fabric 网络的操作。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Hyperledger fabric</tag>
        <tag>e2e_cli</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger fabric v1.0.0 环境部署过程]]></title>
    <url>%2F2018%2F08%2F26%2FFabric%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言本文主要基于 Hyperledger fabric 的官方文档来搭建其实验环境，但官方文档对于很多步骤都有省略，所以本文将比较详细的在一台新安装的 Ubuntu 16.04 虚拟机上来介绍 fabric 的环境部署流程步骤。 环境部署1. 更换 apt 源先备份 sources.list 文件： 1$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak 再修改 sources.list 文件，换成阿里云的国内源： 123456789101112131415161718$ sudo vim /etc/apt/sources.listdeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 最后更新一下源： 1$ sudo apt-get update 2. 安装 curlUbuntu 16.04 一般默认是安装了 curl 的，可以通过以下命令验证： 1234$ curl -Vcurl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets 如果没有安装，则通过 apt-get 安装： 1$ sudo apt-get install curl 3. 安装 Docker 由于 apt 源使用HTTPS以确保软件下载过程中不被篡改。因此，我们首先需要添加使用HTTPS传输的软件包以及CA证书。 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 为了确认所下载软件包的合法性，需要添加软件源的 GPG 秘钥 1$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 然后，我们需要向 sources.list 中添加 Docker 软件源 1234$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 以上命令会添加稳定版本的Docker CE apt 镜像源。 更新 apt 软件包缓存，并安装 docker-ce： 123$ sudo apt-get update$ sudo apt-get install docker-ce 查看 Docker 版本： 12$ docker -vDocker version 18.06.0-ce, build 0ffa825 满足官方文档中 Docker version 17.06.2-ce or greater is required 的要求。 启动 Docker CE 12$ sudo systemctl enable docker$ sudo systemctl start docker 建立 docker 用户组 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 1$ sudo groupadd docker 其实一般按照上面的方法安装 Docker 后就已经创建好 docker 用户组了，可以使用 $ cat /etc/group | grep docker 命令来验证，所以就不需要再建立 docker 用户组了，再建立也会报错提示用户组已存在的。 将当前用户加入 docker 用户组： 1$ sudo usermod -aG docker $USER 下次登录时即可方便的使用 docker 命令。 测试 Docker 是否安装正确 123456789101112131415161718192021222324252627$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-world9db2ca6ccae0: Pull complete Digest: sha256:4b8ff392a12ed9ea17784bd3c9a8b1fa3299cac44aca35a85c90c5e3c7afacdcStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 配置镜像加速器 国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如： Docker 官方提供的中国 registry mirror https://registry.docker-cn.com 七牛云加速器 https://reg-mirror.qiniu.com/ 当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。 国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。 我们以 Docker 官方加速器 https://registry.docker-cn.com 为例进行介绍。 在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在则创建） 12345&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125; 之后重新启动服务 12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 4. 安装 Docker Compose 通过二进制包来安装，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。 123$ sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 查看 Docker compose 版本 12$ docker-compose --versiondocker-compose version 1.22.0, build f46880fe 满足官方文档中 Docker Compose version 1.14.0 or greater 的要求。 5. 安装 Go 语言环境Hyperledger Fabric 在很多组件中使用了 Go 语言，并且 Hyperledger fabric 1.2.0 要求使用的是 GO version 1.10.x ，所以需要在我们的环境中安装对应的 Go 语言。 从官网下载 1.10.x 版本的 Linux 平台的源码包 1$ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz 解压到指定目录 1$ sudo tar zxvf go1.10.3.linux-amd64.tar.gz -C /usr/local/ 先创建 Go 的工作目录 1$ mkdir ~/go 配置环境变量 12345$ vi ~/.bashrc添加export GOROOT=/usr/local/goexport GOPATH=/home/user1/goexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin 保存并使生效： 1$ source ~/.bashrc 测试 Go 的 demo 程序 123456789101112131415161718$ cd ~/go$ vi hello.gopackage mainimport &quot;fmt&quot;func main() &#123; fmt.Printf(&quot;hello world\n&quot;)&#125;$ go build hello.go$ lshello hello.go$ ./hellohello world 6. Fabric 源码下载 首先创建存放源码的文件夹： 1$ mkdir -p ~/go/src/github.com/hyperledger 使用 Git 下载完整源码： 1$ git clone https://github.com/hyperledger/fabric.git 进入 fabric 目录查看版本分支并切换分支： 1234$ cd fabric$ git branch* release-1.2$ git checkout v1.0.0 由于在 release-1.2 版本中碰到没有解决的问题，所以先切换到 v1.0.0 来完成搭建并测试的过程。 7. Fabric Docker 镜像下载进入 ~/go/src/github.com/hyperledger/fabrci/examples/e2e_cli/ 目录，完成镜像下载，执行命令： 12345678910111213141516171819202122232425262728$ cd ~/go/src/github.com/hyperledger/fabrci/examples/e2e_cli/$ lsbase docker-compose-cli.yaml download-dockerimages.sh scriptschannel-artifacts docker-compose-couch.yaml end-to-end.rstconfigtx.yaml docker-compose-e2e-template.yaml generateArtifacts.shcrypto-config.yaml docker-compose-e2e.yaml network_setup.sh$ source download-dockerimages.sh -c x86_64-1.0.0 -f x86_64-1.0.0$ docker image listREPOSITORY TAG IMAGE ID CREATED SIZEhyperledger/fabric-tools latest 0403fd1c72c7 13 months ago 1.32GBhyperledger/fabric-tools x86_64-1.0.0 0403fd1c72c7 13 months ago 1.32GBhyperledger/fabric-couchdb latest 2fbdbf3ab945 13 months ago 1.48GBhyperledger/fabric-couchdb x86_64-1.0.0 2fbdbf3ab945 13 months ago 1.48GBhyperledger/fabric-kafka latest dbd3f94de4b5 13 months ago 1.3GBhyperledger/fabric-kafka x86_64-1.0.0 dbd3f94de4b5 13 months ago 1.3GBhyperledger/fabric-zookeeper latest e545dbf1c6af 13 months ago 1.31GBhyperledger/fabric-zookeeper x86_64-1.0.0 e545dbf1c6af 13 months ago 1.31GBhyperledger/fabric-orderer latest e317ca5638ba 13 months ago 179MBhyperledger/fabric-orderer x86_64-1.0.0 e317ca5638ba 13 months ago 179MBhyperledger/fabric-peer latest 6830dcd7b9b5 13 months ago 182MBhyperledger/fabric-peer x86_64-1.0.0 6830dcd7b9b5 13 months ago 182MBhyperledger/fabric-javaenv latest 8948126f0935 13 months ago 1.42GBhyperledger/fabric-javaenv x86_64-1.0.0 8948126f0935 13 months ago 1.42GBhyperledger/fabric-ccenv latest 7182c260a5ca 13 months ago 1.29GBhyperledger/fabric-ccenv x86_64-1.0.0 7182c260a5ca 13 months ago 1.29GBhyperledger/fabric-ca latest a15c59ecda5b 13 months ago 238MBhyperledger/fabric-ca x86_64-1.0.0 a15c59ecda5b 13 months ago 238MBhyperledger/fabric-baseos x86_64-0.3.1 4b0cab202084 15 months ago 157MB 8. 启动 fabric 网络并完成 chaincode 测试还是在刚刚的 e2e_cli 文件加下，执行： 123456789$ ./network_setup.sh up............===================== All GOOD, End-2-End execution completed ===================== _____ _ _ ____ _____ ____ _____ | ____| | \ | | | _ \ | ____| |___ \ | ____|| _| | \| | | | | | _____ | _| __) | | _| | |___ | |\ | | |_| | |_____| | |___ / __/ | |___ |_____| |_| \_| |____/ |_____| |_____| |_____| 最后出现上面字符说明 fabric 网络已经启动并完成了 chaincode 的测试。 搭建过程中碰到的问题：v1.0.0 版本搭建过程中会碰到的问题： 12345678910111213141516171819202122232425262728293031323334353637383940414243===================== Chaincode is installed on remote peer PEER2 ===================== Instantiating chaincode on org2/peer2...CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtCORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.keyCORE_PEER_LOCALMSPID=Org2MSPCORE_VM_ENDPOINT=unix:///host/var/run/docker.sockCORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crtCORE_PEER_TLS_ENABLED=trueCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspCORE_PEER_ID=cliCORE_LOGGING_LEVEL=DEBUGCORE_PEER_ADDRESS=peer0.org2.example.com:70512018-08-21 13:24:09.596 UTC [msp] GetLocalMSP -&gt; DEBU 001 Returning existing local MSP2018-08-21 13:24:09.596 UTC [msp] GetDefaultSigningIdentity -&gt; DEBU 002 Obtaining default signing identity2018-08-21 13:24:09.602 UTC [chaincodeCmd] checkChaincodeCmdParams -&gt; INFO 003 Using default escc2018-08-21 13:24:09.602 UTC [chaincodeCmd] checkChaincodeCmdParams -&gt; INFO 004 Using default vscc2018-08-21 13:24:09.603 UTC [msp/identity] Sign -&gt; DEBU 005 Sign: plaintext: 0A95070A6708031A0C08F9A4F0DB0510...324D53500A04657363630A0476736363 2018-08-21 13:24:09.603 UTC [msp/identity] Sign -&gt; DEBU 006 Sign: digest: D73172E2164A0DD4FA9B64DBAFE980C3ABC412EB3CD32FEBF6EF7A7AFE3B6431 Error: Error endorsing chaincode: rpc error: code = Unknown desc = Error starting container: API error (404): &#123;&quot;message&quot;:&quot;network e2ecli_default not found&quot;&#125;Usage: peer chaincode instantiate [flags]Flags: -C, --channelID string The channel on which this command should be executed (default &quot;testchainid&quot;) -c, --ctor string Constructor message for the chaincode in JSON format (default &quot;&#123;&#125;&quot;) -E, --escc string The name of the endorsement system chaincode to be used for this chaincode -l, --lang string Language the chaincode is written in (default &quot;golang&quot;) -n, --name string Name of the chaincode -P, --policy string The endorsement policy associated to this chaincode -v, --version string Version of the chaincode specified in install/instantiate/upgrade commands -V, --vscc string The name of the verification system chaincode to be used for this chaincodeGlobal Flags: --cafile string Path to file containing PEM-encoded trusted certificate(s) for the ordering endpoint --logging-level string Default logging level and overrides, see core.yaml for full syntax -o, --orderer string Ordering service endpoint --test.coverprofile string Done (default &quot;coverage.cov&quot;) --tls Use TLS when communicating with the orderer endpoint!!!!!!!!!!!!!!! Chaincode instantiation on PEER2 on channel &apos;mychannel&apos; failed !!!!!!!!!!!!!!!!================== ERROR !!! FAILED to execute End-2-End Scenario ================== 解决方法： 这个主要是因为 e2e_cli/base目录下的 peer-base.yaml 文件中的网络名称打成了 e2ecli_default，将其改成 e2e_cli_default 即可。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Hyperledger fabric</tag>
        <tag>Docker</tag>
        <tag>Docker Compose</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迁移vim-python开发环境]]></title>
    <url>%2F2018%2F08%2F23%2F%E8%BF%81%E7%A7%BBvim-python%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[概述之前配置好了自己的 vim python 开发环境，为了方便在其他主机上面迁移之前的开发环境，所以将所有的插件包都打包压缩了一份，和 vim 配置文件 .vimrc 一起上传到了百度云盘（下载链接：vim插件包 密码：neio）。通过下载这个文件夹就可以在新的主机上部署 vim 的 python 开发环境了。 步骤 下载插件包与配置文件 先使用百度云盘下载这两个文件，然后再通过 ftp 上传到你的 Linux 主机上。我也试过直接在 Linux 里面通过 wget 来下载百度云盘的文件，但是这个要使用浏览器来生成下载链接，挺麻烦的，所以我就不在这里说明了。 将 .vimrc 放到用户主目录下，即 ~/ 目录 .vimrc 文件里面主要记录需要安装哪些插件，我的 vim python 开发环境安装的插件有： VundleVim/Vundle.vim Valloric/YouCompleteMe Lokaltog/vim-powerline scrooloose/nerdtree Yggdroot/indentLine jiangmiao/auto-pairs tell-k/vim-autopep8 scrooloose/nerdcommenter altercation/vim-colors-solarized w0rp/ale scrooloose/syntastic nvie/vim-flake8 以及一些常用的配置信息，具体如下所示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149$ vi ~/.vimrc&quot;去掉vi的一致性&quot;set nocompatiblefiletype offset rtp+=~/.vim/bundle/Vundle.vimcall vundle#begin()Plugin &apos;VundleVim/Vundle.vim&apos;Plugin &apos;Valloric/YouCompleteMe&apos;Plugin &apos;Lokaltog/vim-powerline&apos;Plugin &apos;scrooloose/nerdtree&apos;Plugin &apos;Yggdroot/indentLine&apos;Plugin &apos;jiangmiao/auto-pairs&apos;Plugin &apos;tell-k/vim-autopep8&apos;Plugin &apos;scrooloose/nerdcommenter&apos;Plugin &apos;altercation/vim-colors-solarized&apos;&quot;Plugin &apos;w0rp/ale&apos;Plugin &apos;scrooloose/syntastic&apos;Plugin &apos;nvie/vim-flake8&apos;call vundle#end()filetype plugin indent on&quot;显示行号&quot;set number&quot; 隐藏滚动条&quot; &quot;set guioptions-=r &quot;set guioptions-=L&quot;set guioptions-=b&quot;隐藏顶部标签栏&quot;&quot;set showtabline=0&quot;设置字体&quot;set guifont=Monaco:h13 set nowrap &quot;设置不折行&quot;&quot;set fileformat=unix &quot;设置以unix的格式保存文件&quot;&quot;set cindent &quot;设置C样式的缩进格式&quot;set tabstop=4 &quot;设置table长度&quot;set shiftwidth=4 &quot;同上&quot;set showmatch &quot;显示匹配的括号&quot;set scrolloff=5 &quot;距离顶部和底部5行&quot;set laststatus=2 &quot;命令行为两行&quot;set fenc=utf-8 &quot;文件编码&quot;set backspace=2set mouse=v &quot;启用鼠标&quot;set selection=exclusiveset selectmode=mouse,keyset matchtime=5set ignorecase &quot;忽略大小写&quot;set incsearchset hlsearch &quot;高亮搜索项&quot;set noexpandtab &quot;不允许扩展table&quot;set whichwrap+=&lt;,&gt;,h,lset autoreadset cursorline &quot;突出显示当前行&quot;&quot;set cursorcolumn &quot;突出显示当前列&quot;syntax on &quot;开启语法高亮&quot;&quot;set background=dark &quot;设置背景色&quot;&quot;colorscheme solarized&quot;let g:solarized_termcolors=256 &quot;solarized主题设置在终端下的设置&quot;&quot;syntasticlet python_highlight_all=1&quot;设置error和warning的标志let g:syntastic_enable_signs=1let g:syntastic_error_symbol=&apos;✗&apos;let g:syntastic_warning_symbol=&apos;►&apos;&quot;总是打开Location&quot;List（相当于QuickFix）窗口，如果你发现syntastic因为与其他插件冲突而经常崩溃，将下面选项置0let g:syntastic_always_populate_loc_list = 0&quot;自动打开LocatonList，默认值为2，表示发现错误时不自动打开，当修正以后没有再发现错误时自动关闭，置1表示自动打开自动关闭，0表示关闭自动打开和自动关闭，3表示自动打开，但不自动关闭let g:syntastic_auto_loc_list = 2&quot;修改Locaton List窗口高度let g:syntastic_loc_list_height = 3&quot;打开文件时自动进行检查let g:syntastic_check_on_open = 1let g:syntastic_check_on_wq = 1&quot;自动跳转到发现的第一个错误或警告处let g:syntastic_auto_jump = 1&quot;高亮错误let g:syntastic_enable_highlighting=0&quot;设置pyflakes为默认的python语法检查工具let g:syntastic_python_checkers = [&apos;pyflakes&apos;]&quot;按F5运行python&quot;map &lt;F5&gt; :call RunPython()&lt;CR&gt;function RunPython() exec &quot;W&quot; if &amp;filetype == &apos;python&apos; exec &quot;!time python %&quot; endifendfunction&quot;默认配置文件路径&quot;let g:ycm_global_ycm_extra_conf = &apos;~/.ycm_extra_conf.py&apos;&quot;打开vim时不再询问是否加载ycm_extra_conf.py配置&quot;let g:ycm_confirm_extra_conf=0set completeopt=longest,menu&quot;python解释器路径&quot;let g:ycm_path_to_python_interpreter=&apos;/usr/bin/python&apos;&quot;是否开启语义补全&quot;let g:ycm_seed_identifiers_with_syntax=1 &quot;是否在注释中也开启补全&quot; let g:ycm_complete_in_comments=1 let g:ycm_collect_identifiers_from_comments_and_strings = 0&quot;开始补全的字符数&quot;let g:ycm_min_num_of_chars_for_completion=2&quot;补全后自动关机预览窗口&quot;let g:ycm_autoclose_preview_window_after_completion=1&quot; 禁止缓存匹配项,每次都重新生成匹配项&quot;let g:ycm_cache_omnifunc=0&quot;字符串中也开启补全&quot;let g:ycm_complete_in_strings = 1&quot;离开插入模式后自动关闭预览窗口&quot;autocmd InsertLeave * if pumvisible() == 0|pclose|endif&quot;回车即选中当前项&quot;&quot;inoremap &lt;expr&gt; &lt;CR&gt; pumvisible() ? &apos;&lt;C-y&gt;&apos; : &apos;\&lt;CR&gt;&apos;&quot;上下左右键行为&quot;inoremap &lt;expr&gt; &lt;Down&gt; pumvisible() ? &apos;\&lt;C-n&gt;&apos; : &apos;\&lt;Down&gt;&apos;inoremap &lt;expr&gt; &lt;Up&gt; pumvisible() ? &apos;\&lt;C-p&gt;&apos; : &apos;\&lt;Up&gt;&apos;inoremap &lt;expr&gt; &lt;PageDown&gt; pumvisible() ? &apos;\&lt;PageDown&gt;\&lt;C-p&gt;\&lt;C-n&gt;&apos; : &apos;\&lt;PageDown&gt;&apos;inoremap &lt;expr&gt; &lt;PageUp&gt; pumvisible() ? &apos;\&lt;PageUp&gt;\&lt;C-p&gt;\&lt;C-n&gt;&apos; : &apos;\&lt;PageUp&gt;&apos;&quot;F2开启和关闭树&quot;map &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;let NERDTreeChDirMode=1&quot;显示书签&quot;let NERDTreeShowBookmarks=1&quot;设置忽略文件类型&quot;let NERDTreeIgnore=[&apos;\~$&apos;, &apos;\.pyc$&apos;, &apos;\.swp$&apos;]&quot;窗口大小&quot;let NERDTreeWinSize=25&quot;split navigationsnnoremap &lt;C-J&gt; &lt;C-W&gt;&lt;C-J&gt;nnoremap &lt;C-K&gt; &lt;C-W&gt;&lt;C-K&gt;nnoremap &lt;C-L&gt; &lt;C-W&gt;&lt;C-L&gt;nnoremap &lt;C-H&gt; &lt;C-W&gt;&lt;C-H&gt;&quot;缩进指示线&quot;let g:indentLine_char=&apos;┆&apos;let g:indentLine_enabled = 1&quot;autopep8设置&quot;let g:autopep8_disable_show_diff=1let mapleader=&apos;,&apos;map &lt;F4&gt; &lt;leader&gt;ci &lt;CR&gt; 在用户主目录下新建一个 .vim 文件夹，并将插件包解压缩至该文件夹 12$ mkdir ~/.vim$ tar -jxv -f bundle.tar.bz2 -C ~/.vim 到这里，除了自动补全的插件 YouCompleteMe ，其实大部分的插件都已经起作用了，我们的插件包有几百兆主要就是因为 YouCompleteMe 这个插件比较大，这也是因为这个插件的功能太强大了，这个插件在下载完成后还需要编译安装，接下来就来完成这个步骤。 安装 python 和 python 库 1$ sudo apt install python python-dev 这一步没有完成在安装的时候可能会碰到下面的问题： 123WARNING: this script is deprecated. Use the install.py script instead.Searching Python 2.7 libraries...ERROR: unable to find an appropriate Python library. 安装编译环境 1$ sudo apt install cmake gcc build-essential 未完成这步可能会遇到的问题： 12WARNING: this script is deprecated. Use the install.py script instead.ERROR: Unable to find executable &apos;cmake&apos;. CMake is required to build ycmd 1No CMAKE_CXX_COMPILER could be found. 执行 YouCompleteMe 安装脚本 12$ cd ~/.vim/bundle/YouCompleteMe$ ./install.sh 完成上面的过程就实现了 YouCompleteMe 的安装，接下来就可以体验 vim 强大的功能啦！]]></content>
      <categories>
        <category>VIM</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>vim</tag>
        <tag>YouCompleteMe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 配置 shadowsocks 客户端]]></title>
    <url>%2F2018%2F08%2F21%2Fubuntu%E9%85%8D%E7%BD%AEshadowsocks%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[前言在日常的工作学习中，经常需要搭建各种环境，而很多环境都是由国外开发并开源的，有一些软件或源码必须要到墙外面才能够下载，所以需要在自己的环境中配置 shadowsocks 客户端来连接国外的网络。在这里将介绍如何在 Ubuntu 系统下搭建 shadowsocks 客户端。 安装 shadowsocks 安装 python pip工具 1$ sudo apt install python-pip 安装 shadowsocks 1$ sudo pip install shadowsocks 配置 shadowsocks 1$ sudo vi /etc/shadowsocks.json 输入以下 json 格式的代码： 1234567&#123; &quot;server&quot;: &quot;服务器ip&quot;, &quot;server_port&quot;: 服务器端口, &quot;password&quot;: &quot;你的密码&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;, &quot;timeout&quot;: 300&#125; 启动 shadowsocks 服务 1$ sslocal -c /etc/shadowsocks.json &amp; 加上 &amp; 以让 shadowsocks 进程在后台运行 设置 shadowsocks 开机自启动 将启动服务的命令添加到 /etc/rc.local 文件中的 exit 0 之前 1234$ sudo vi /etc/rc.local……sslocal -c /etc/shadowsocks.json &amp;exit 0 以上就是SS的搭建了，这个时候我们发现上网时并不可以翻墙，原因是需要将sock5代理映射为http代理。代理的软件很多，我选择了推荐度比较高的privoxy，下面是privoxy的配置。 安装 privoxy 安装 privoxy 1$ sudo apt install privoxy 配置 privoxy 打开 /etc/privoxy/config 1$ sudo vi /etc/privoxy/config 找到其中的4.1节，看一下有没有一句listen-address localhost:8118的代码，如果被注释了，取消注释。因为版本不一样这句的状态可能会不一样。 然后再将 localhost 改成 127.0.0.1（这一步很重要，反正我因为这一步的设置问题搞了很久都不知道为什么连不上外网），如图所示： 接着找到5.2节，在本节末尾加入代码 forward-socks5t / 127.0.0.1:1080 .，注意最后有一个点号，如图： 重启 privoxy 服务 1$ sudo /etc/init.d/privoxy restart 设置开机自启动 privoxy 服务 将启动服务的命令添加到 /etc/rc.local 文件中的 exit 0 之前： 12345$ sudo vi /etc/rc.local……sslocal -c /etc/shadowsocks.json &amp;/etc/init.d/privoxy startexit 0 代理配置 12345$ sudo vi /etc/profileexport http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118$ source /etc/profile 测试1curl www.google.com 或 1wget www.google.com]]></content>
      <categories>
        <category>VPN</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>shadowsocks</tag>
        <tag>privoxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vultr CentOS搭建shadowsocks服务端并开启BBR加速]]></title>
    <url>%2F2018%2F08%2F21%2FVPS%E6%90%AD%E5%BB%BAshadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B9%B6%E5%BC%80%E5%90%AFBBR%E5%8A%A0%E9%80%9F%2F</url>
    <content type="text"><![CDATA[前言最近教研室很多同学来问有没有公用的 VPN，教研室以前有师兄去买过一些 VPN，但现在师兄也毕业了就用不了了。为了同学们的方便，作为网管应该尽力满足大家日常查阅资料的需求，于是向老师申请了经费去购买了一台 VPS 来搭建教研室公用的 VPN。 购买 VPS我这里购买的是 vultr 的 VPS，在新加坡的节点，每个月5美元，其实是按小时计费，每小时0.007美元，如果出问题了可以方便的停止购买，不像其他厂商按年一次性付费的话，万一不能用了就很亏。操作系统选择 CentOS 7 64位。 安装过程安装 Shadowsocks 服务shadowsocks有很多版本，如Python，node.js，libev，Go等，Python版本用的人是最多的，但很久没有更新了，这里选择 Go 版本的shadowsocks。 在安装shadowsocks之前需要先安装 Go 语言的环境： 从官网下载 Linux 平台的源码包 1# wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz 解压到指定目录 1# sudo tar zxvf go1.10.3.linux-amd64.tar.gz -C /usr/local/ 配置环境变量 123# vi .bashrc添加export PATH=$PATH:/usr/local/go/bin 保存并使生效： 1# source .bashrc 安装 shadowsocks，使用一键安装脚本（https://github.com/iMeiji/shadowsocks_install） 123# wget --no-check-certificate https://raw.githubusercontent.com/iMeiji/shadowsocks_install/master/shadowsocks-go.sh# chmod +x shadowsocks-go.sh# ./shadowsocks-go.sh 2&gt;&amp;1 | tee shadowsocks-go.log 卸载 shadowsocks 方法： 1# ./shadowsocks-go.sh uninstall shadowsocks 常用命令： 启动：/etc/init.d/shadowsocks start 停止：/etc/init.d/shadowsocks stop 重启：/etc/init.d/shadowsocks restart 状态：/etc/init.d/shadowsocks status 执行完前面的安装脚本后，查看 shadowsocks 的运行状态： 12# /etc/init.d/shadowsocks statusshadowsocks-go running with PID 1629 能看到进程 ID 说明 shadowsocks 服务已经在运行了。 配置 shadowsocks 开机自启动： 123# vi /etc/rc.local添加/etc/init.d/shadowsocks restart 这样在系统重启后就可以自动加载 shadowsocks 服务了。 配置防火墙： 检查防火墙是否允许你设定的端口进行通信 1# iptables -vnL | grep 8989 如果没有信息的话，就是防火墙不允许该端口进行通信。 需设置： 12# firewall-cmd --zone=public --add-port=8989/tcp --permanent# firewall-cmd --reload 由于 CentOS 7 默认安装的是 firewalld，并没有安装 iptables-services，不能使用 iptables-save 来保存iptables 规则并在下次启动时自动加载，所以上面使用是 firewalld 来配置永久规则，这样在关机重启后规则也不会消失。 到这里，shadowsocks 服务器就基本上已经配置好了，你可以使用客户端来上外网了。但这时候的网络连接的速度可能只能够保证查查网页，如果要下载或者看 YouTube 速度很慢，所以后将进行配置TCP 加速。 TCP 加速在后面会升级系统内核，所以先查看一下服务器的内核版本： 12# uname -aLinux vultr.guest 3.10.0-862.3.2.el7.x86_64 #1 SMP Mon May 21 23:36:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 加速有锐速加速和 Google BBR 加速，这里使用 BBR 加速。TCP BBR是谷歌出品的 TCP 拥塞控制算法，目的是要尽量跑满带宽，并且尽量不要有排队的情况。BBR 可以起到单边加速 TCP 连接的效果。 Google提交到Linux主线并发表在ACM queue期刊上的TCP-BBR拥塞控制算法。继承了Google“先在生产环境上部署，再开源和发论文”的研究传统。TCP-BBR已经再YouTube服务器和Google跨数据中心的内部广域网(B4)上部署。由此可见出该算法的前途。 TCP-BBR的目标就是最大化利用网络上瓶颈链路的带宽。一条网络链路就像一条水管，要想最大化利用这条水管，最好的办法就是给这跟水管灌满水。 BBR解决了两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟，高带宽的网络链路。 降低网络链路上的buffer占用率，从而降低延迟。非常适合慢速接入网络的用户。 Google 在 2016年9月份开源了他们的优化网络拥堵算法BBR，最新版本的 Linux内核(4.9-rc8)中已经集成了该算法。 一键安装脚本： 123# wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh# chmod +x bbr.sh# ./bbr.sh 安装完成后会提示重启，重启完成后，查看内核： 12# uname -r4.18.3-1.el7.elrepo.x86_64 高于 4.9 就可以了 检查是否开启 BBR： 123456789101112# sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = reno cubic bbr# sysctl net.ipv4.tcp_congestion_controlnet.ipv4.tcp_congestion_control = bbr# sysctl net.core.default_qdiscnet.core.default_qdisc = fq# lsmod | grep bbrtcp_bbr 20480 7#返回值有 tcp_bbr 则说明已经启动 完成以上步骤，则 TCP 加速也已经配置好了，接下来就可以体验飞快的下载速度以及 1080p 的高清视屏啦！ shadowsocks 客户端下载连接：Shadowsocks - Clients]]></content>
      <categories>
        <category>VPN</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>shadowsocks</tag>
        <tag>BBR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM 虚拟机磁盘扩容]]></title>
    <url>%2F2018%2F07%2F24%2FKVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[概述在 KVM 环境中，通常通过创建一个各种格式的磁盘来安装虚拟机，可能创建时你觉得大小够用，但是可能用着用着到了某一天你发现你的磁盘容量不够用了，很多程序都打不开，你可能会很慌，其实并不需要慌，因为有很多方法可以给你的虚拟机扩容，哈哈哈，如： 通过virsh attach-disk添加一个新的磁盘 通过virsh attach-device添加一个新的存储设备 直接给原来用的磁盘扩容 上面的三种方法都能实现给你的虚拟机扩容，但是本文想介绍的是第三种方法，直接给安装了虚拟机的磁盘来扩容，这样的好处是在你的主机上看来一个虚拟机就是一个磁盘，管理方便，而且只通过这一个磁盘来分享迁移你的虚拟机，也是更方便。 #创建虚拟机 创建磁盘 1$ qemu-img create -f qcow2 centos.qcow2 50G 这里创建了一个50G的磁盘，格式为qcow2，这种格式的特点是分配给虚拟机的实际使用的磁盘的大小是动态增长的，并不是一下子把所有的空间都给配给虚拟机，可以通过命令查看磁盘信息： 1234567891011$ qemu-img info centos.qcow2image: centos.qcow2file format: qcow2virtual size: 50G (53687091200 bytes)disk size: 1.2Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false 可以看到50G的磁盘目前的实际大小只有1.2个G。 安装虚拟机 可以通过 virt-manager 通过图形界面安装，但是我一般都是通过远程连接服务器的，所以常用命令行安装虚拟机，命令如下： 123456$ virt-install --virt-type kvm --name test-centos --ram 2048 --vcpus=1 \--cdrom CentOS-7-x86_64-Minimal-1804.iso \--disk path=test-centos.qcow2,format=qcow2 \--network network=default,model=virtio \--graphics vnc,listen=0.0.0.0 --noautoconsole \--os-type=linux 系统安装的过程就不介绍了，但在安装系统的过程中，对于磁盘分区部分一般分两个区，根目录 / 和 交换分区 swap 。 查看虚拟机磁盘 查看磁盘空间信息 123456789$ df -lhFilesystem Size Used Avail Use% Mounted on/dev/sda2 46G 1.1G 45G 3% /devtmpfs 909M 0 909M 0% /devtmpfs 920M 0 920M 0% /dev/shmtmpfs 920M 8.5M 911M 1% /runtmpfs 920M 0 920M 0% /sys/fs/cgrouptmpfs 184M 0 184M 0% /run/user/1000tmpfs 184M 0 184M 0% /run/user/0 可以看到 /dev/sda2 被挂载到了系统根目录 / 查看系统磁盘分区 1234567891011# fdisk -lDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux 我这里系统磁盘被成了两个区：/dev/sda1 和 /dev/sda2，就是我们前面说的一个用于交换分区，另一个用于挂载到根目录。 注：磁盘分区信息中可以看到原来我们 50个G 的磁盘到这里怎么变成 53.7GB 了呢？其实只是单位不一样而已，前面我们用的是 GiB 为单位，这两个单位的计算方式不同，1 GiB = 1024*1024*1024 Bytes，而 1GB = 1000*1000*1000 Bytes 。 到这里，假如我们发现磁盘空间不够用了，接下来就来看看如何给虚拟机的磁盘扩容吧。 磁盘扩容 首先将我们的虚拟机关机，然后进行磁盘扩容，在服务器主机上操作： 12# qemu-img resize centos.qcow2 60GImage resized. 上面我们把磁盘大小扩成了 60G 重启虚拟机，在虚拟机内，使用 fdisk 指令对磁盘进行分区： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# fdisk /dev/sda #我们的整块磁盘是sda，对整个磁盘进行分区Command (m for help): pDisk /dev/sda: 64.4 GB, 64424509440 bytes, 125829120 sectors #磁盘大小变成64.4GB了，扩容成功Units = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux #结束sector号小于磁盘的总数Command (m for help): nPartition type: p primary (2 primary, 0 extended, 2 free) e extendedSelect (default p): e #添加扩展分区Partition number (3,4, default 3): 3 #分区号，填写默认的3First sector (104857600-125829119, default 104857600): #直接回车选择默认Using default value 104857600Last sector, +sectors or +size&#123;K,M,G&#125; (104857600-125829119, default 125829119): #回车选择默认 Using default value 125829119Partition 3 of type Extended and of size 10 GiB is setCommand (m for help): p #再次查看分区表Disk /dev/sda: 64.4 GB, 64424509440 bytes, 125829120 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux/dev/sda3 104857600 125829119 10485760 5 Extended #新添加的扩展分区Command (m for help): n #再添加逻辑分区，因为扩展分区是不能格式化后挂载的Partition type: p primary (2 primary, 1 extended, 1 free) l logical (numbered from 5)Select (default p): l #逻辑分区Adding logical partition 5First sector (104859648-125829119, default 104859648): #回车选择默认Using default value 104859648Last sector, +sectors or +size&#123;K,M,G&#125; (104859648-125829119, default 125829119): #回车选择默认Using default value 125829119Partition 5 of type Linux and of size 10 GiB is setCommand (m for help): pDisk /dev/sda: 64.4 GB, 64424509440 bytes, 125829120 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x00015299 Device Boot Start End Blocks Id System/dev/sda1 2048 8390655 4194304 82 Linux swap / Solaris/dev/sda2 * 8390656 104857599 48233472 83 Linux/dev/sda3 104857600 125829119 10485760 5 Extended/dev/sda5 104859648 125829119 10484736 83 Linux #新添加的逻辑分区Command (m for help): w #保存分区配置，并退出 The partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks. #需要重启或者重新获取分区表# 重新获取分区表： 1# partprobe 格式化分区： 可以先查看一下其他分区是什么格式的文件系统： 12# mount |grep /dev/sda/dev/sda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota) /dev/sda2 是 xfs 格式的，CentOS 7默认文件系统格式是xfs，在CentOS 6以及之前的版本，使用的是ext文件系统格式，CentOS 6是ext4格式、CentOS 5是ext3格式。那我们也把 /dev/sda5 格式化成一样格式的： 12345678910# mkfs -t xfs /dev/sda5 #-t 指定格式 meta-data=/dev/sda5 isize=512 agcount=4, agsize=655296 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0data = bsize=4096 blocks=2621184, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 挂载磁盘： 1# mount /dev/sda5 /mnt/newdisk 将 /dev/sda5 挂载到 /mnt/newdisk，需要注意的是，newdisk 最好是一个空文件夹，不然挂载之后文件夹内的文件将会暂时隐藏，直到你卸载到磁盘后才能恢复。 查看磁盘空间信息： 123456789# df -lhFilesystem Size Used Avail Use% Mounted on/dev/sda2 46G 1.1G 45G 3% /devtmpfs 909M 0 909M 0% /devtmpfs 920M 0 920M 0% /dev/shmtmpfs 920M 8.5M 911M 1% /runtmpfs 920M 0 920M 0% /sys/fs/cgrouptmpfs 184M 0 184M 0% /run/user/1000/dev/sda5 10G 33M 10G 1% /mnt/newdisk 到这里就基本上结束了，你的磁盘又多了 10个G 的容量，可以正常读写磁盘啦。 命令附录这里先介绍几个会用到的关于Linux磁盘操作指令： df 指令：磁盘信息命令 语法： 1$ df [OPTION]... [FILE]... 功能说明： 使用 df 指令查看磁盘空间的信息。指令可以查看指定文件系统的占用情况。如果指令中未指定文件名，将显示当前所有挂载的文件系统的可用空间。 参数说明： | 参数 | 说明 || :—————–: | :—————————————: || -a | 显示包括0区块在内的所有文件系统的情况 || -h | 以可读性较高的方式显示信息 || -H | 相当于“-h”但在计算时，1K=1000，而不是1024 || -i | 显示 inode 节点信息 || -k | 区块大小为1024字节 || -l | 尽显示本地文件系统 || –no-sync | 取得磁盘信息前，忽略 sync 指令 || -P | 输出 POSIX 格式 || –sync | 在取得磁盘信息前，先执行 sync 指令 || -T | 显示文件系统类型 || –block-size= | 指定区块大小 || -t filesystem-type | 只显示选定文件系统的磁盘信息 || -x filesystem-type | 不显示选定文件系统的磁盘信息 || –help | 帮助信息 || –version | 版本信息 | 常用命令：$ df -lh fdisk 指令：Linux磁盘分区命令 语法： 123fdisk [options] &lt;disk&gt; change partition tablefdisk [options] -l &lt;disk&gt; list partition table(s)fdisk -s &lt;partition&gt; give partition size(s) in blocks 功能说明： fdisk 指令是Linux下管理分区的程序。应用该程序不仅可以创建磁盘分区，还可以对磁盘进行维护，改变分区类型。 参数说明： | 参数 | 说明 || :———: | :—————————-: || -b | 指定各分区大小 || -l | 列出分区表情况 || -s | 输出指定分区大小到标准输出设备 || -u | 与“-l”参数搭配，显示分区数目 || -v | 版本信息 | 【fdisk 程序指令】 a：设置/删除可引导分区标记 d：删除指定分区 l：列出分区类型 m：显示 fdisk 程序指令 n：新建分区 p：列出当前分区信息 q：退出 fdisk 分区，对更改不保存 t：改变分区 ID v：检测当前分区信息 w：退出 fdisk 分区，保存更改 注：不同的版本略差别 mkfs 指令：建立各种文件系统 语法： 1# mkfs [options] [-t &lt;type&gt;] [fs-options] &lt;device&gt; [&lt;size&gt;] 功能说明： mkfs 指令可用来在指定的设备上建立各种文件系统，它通过调用相关的程序来执行文件系统的构建，本身并不执行系统构建。 参数说明： | 参数 | 说明 || :——————: | :—————————————————-: || -c | 在创建文件系统之前，检查是否有损坏的区块 || fs | 指定建立文件系统时的参数，针对不同的文件系统的参数不同 || -t | 指定要创建的文件系统的类型，默认为ext2 || -v | 显示详细的处理信息 || -V | 版本信息 | 常用命令 1# mkfs -t ext3 /dev/sda3 mount 指令：挂载文件系统 语法： 1# mount [options] &lt;source&gt; &lt;directory&gt; 功能说明： 使用 mount 指令可将指定设备挂载到已存在的目录。当文件系统挂载完成后，用户可通过该目录进行操作，来实现对指定设备的文件读写等操作。 必要参数说明： | 参数 | 说明 || :———: | :————–: || | 指定要挂载的设备 || | 指定要挂载的目录 | 常用示例： 1# mount /dev/sda3 /mnt]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>磁盘扩容</tag>
        <tag>df</tag>
        <tag>fdisk</tag>
        <tag>mkfs</tag>
        <tag>mount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Linux Shell新的收获]]></title>
    <url>%2F2018%2F07%2F21%2F%E5%85%B3%E4%BA%8EShell%E6%96%B0%E7%9A%84%E6%94%B6%E8%8E%B7%2F</url>
    <content type="text"><![CDATA[最近又翻阅了几本Linux相关的工具书，在这里记录一些关于Linux shell的新的小小的收获。 查看正在使用的shell有很多方法可以查看当前正在使用的shell： 使用特殊参数$0 12$ echo $0-bash 对于这里有点疑问的是有资料说在本地系统执行可能的结果会是/bin/bash，但是我尝试了一下发现结果还是bash或者-bash。不知道资料中说的本地系统指的是什么意思。 输入不存在的命令： 12$ asdf-bash: asdf: command not found 从shell提示中可以看出当前正在使用的shell是bash。 查看用户登录默认的shell 通过查看/etc/passwd文件找到用户对应的那一行，最后一列就是用户登录的shell 12user1@ubuntu:~$ cat /etc/passwd | grep user1user1:x:1000:1000:user1,,,:/home/user1:/bin/bash 最常用的方法： 12$ echo $SHELL/bin/bash 从环境变量中查看： 12$ env |grep SHELLSHELL=/bin/bash 切换用户正在使用的shell直接输入对应shell的命令即可切换： 12345$ echo $0-bash &lt;==当前正在使用的shell$ zsh &lt;==运行zsh$ echo $0zsh &lt;==当前正在使用的shell已经变成zsh 修改用户的登录shell 首先可以查看用户合法的shell： 123456789$ chsh -l/bin/sh/bin/bash/sbin/nologin &lt;==合法不可登录的shell/bin/tcsh/bin/csh/bin/mksh/bin/ksh/bin/zsh chsh -l命令并不总是可用，对于某些Linux发行版，chsh命令没有-l的选项，如ubuntu16.04，但是可以直接查看/etc/shells文件 123456789$ cat /etc/shells/bin/sh/bin/bash/sbin/nologin &lt;==合法不可登录的shell/bin/tcsh/bin/csh/bin/mksh/bin/ksh/bin/zsh 修改用户登录shell 123456$ chsh -s /bin/zshPassword:$ cat /etc/passwd | grep user1user1:x:1000:1000:user1,,,:/home/user1:/bin/zsh &lt;== /etc/passwd文件中已经发生变化$ echo $SHELL/bin/bash &lt;==但是再次查看$SHELL，发现并没有变，其实退出shell再次登录就会看到变化了]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>linux</tag>
        <tag>chsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化脚本与程序实现]]></title>
    <url>%2F2018%2F07%2F09%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC%E4%B8%8E%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[概述最近的项目中有远程登录虚拟机并执行相关命令的需求，所以尝试了远程免密码登录虚拟机（在另一篇博客中有介绍链接)。但是发现这还不够，因为登录远程虚拟机后执行的脚本可能会有需要交互的操作，比如sudo命令需要输入密码，所以就想实现一个完全自动化的脚本，包括登录时的密码自动输入以及登录后执行命令的自动交互。查阅相关资料后学习到，在Linux中可以用expect来实现自动化的交互，且在python中也有相应的一个模块pexpect具有此功能。本文将先介绍shell自动化交互脚本的实现，然后介绍其python程序实现。 shell脚本Expect介绍 简要介绍： Expect 是由Don Libes基于Tcl（Tool Command Language）语言开发的，主要应用于自动化交互式操作的场景，借助expect处理交互的命令，可以将交互过程如：ssh登录，ftp登录等写在一个脚本上，使之自动化完成。尤其适用于需要对多台服务器执行相同操作的环境中，可以大大提高系统管理人员的工作效率 。 主要命令： spawn：启动新的进程 spawn命令会fork一个子进程去执行command命令，然后在此子进程中执行后面的命令；spawn后的send和expect命令都是和spawn打开的进程进行交互的。如果没有spawn语句，整个Expect就无法进行下去，当然，如果真的不要spawn过程也没有关系，虽然这样就没有办法单独执行，但是这个脚本可以与任何调用它的进程进行交互。 使用方法：ssh自动登录脚本中，通过spawn ssh user_name@ip_addr，fork一个子进程执行ssh登录命令；连接远程ftp服务器，spawn ftp ftp.server.com。 expect：从进程接收字符串 expect命令是Expect解释器的关键命令，它的一般用法为 expect “string”，即期望获取到string字符串，可在在string字符串里使用 * 等通配符。 使用方法：在执行spawn命令ssh登录时，子进程会要求输入密码，因此可以使用expect命令检查子进程中的输出中是否包含“password”子字符串，命令为expect &quot;password&quot;。 send：用于向进程发送字符串 send命令的一般用法为 send “string”，它们会我们平常输入命令一样向命令行输入一条信息，当然不要忘了在string后面添加上 \r 表示输入回车 。 使用方法：在使用expect命令接收到字符串“password”后，就需要使用send命令来发送“PASSWORD”。 interact：允许用户交互 interact命令很简单，执行到此命令时，脚本fork的子进程会将操作权交给用户，允许用户与当前shell进行交互，让人在适当的时候干预这个过程了 。 使用方法：直接在脚本适当的位置加入一行interact。 安装方法： $ sudo apt-get install expect 脚本编写我需要实现一个脚本，其功能是ssh登录虚拟机，并在远程虚拟机用户目录下记录远程登录的日志文件，然后修改其iptables规则，禁止转发tcp 22号端口的报文。 ssh登录 12345spawn ssh $user@$ipexpect &#123; &quot;(yes/no)&quot; &#123;send &quot;yes\r&quot;; exp_continue&#125; &quot;password:&quot; &#123;send &quot;$password\r&quot;&#125;&#125; expect中可能会接收到两种字符串，”(yes/no)”表示你的主机还未登录过远程虚拟机，即你的用户目录下的文件~/.ssh/know_hosts中还未记录该远程虚拟机，问你是否需要将其添加到know_hosts中，回复”yes”，下次再登录就不会出现这个提醒了；然后就会收到&quot;password:&quot;，这时就需要将密码发送过去，这样就已经登录到远程虚拟机。 记录日志文件 1expect &quot;*$&quot; &#123;send &quot;echo &apos;login +1&apos; &gt;&gt; ~/remote_login.log\r&quot;&#125; 登录之后会收到”$”或”#”的命令行提示符，然后就可以发送需要执行的命令了。 修改iptables规则 12expect &quot;*$&quot; &#123;send &quot;sudo iptables -A FORWARD -p tcp --dport 22 -j REJECT\r&quot;&#125;expect &quot;password&quot; &#123;send &quot;$password\r&quot;&#125; 退出子程序 1expect &quot;*$&quot; &#123;send exit\r&#125; 完整的脚本： 12345678910111213141516171819#!/usr/bin/expectset ip 192.168.1.75set user openstackset passwd 123456set timeout 5spawn ssh $user@$ipexpect &#123; &quot;(yes/no)&quot; &#123;send &quot;yes\r&quot;; exp_continue&#125; &quot;password:&quot; &#123;send &quot;$password\r&quot;&#125;&#125;expect &quot;*$&quot; &#123;send &quot;echo &apos;login +1&apos; &gt;&gt; ~/remote_login.log\r&quot;&#125;;expect &quot;*$&quot; &#123;send &quot;sudo iptables -A FORWARD -p tcp --dport 22 -j REJECT\r&quot;&#125;;expect &quot;password&quot; &#123;send &quot;$passwd\r&quot;&#125;;expect &quot;*$&quot; &#123;send exit\r&#125;;expect eof; python程序Pexpect介绍Pexpect 是Expect 的一个 Python 实现，是一个用来启动子程序，并使用正则表达式对程序输出做出特定响应，以此实现与其自动交互的 Python 模块。 Pexpect 的使用范围很广，可以用来实现与 ssh、ftp 、telnet 等程序的自动交互；可以用来自动复制软件安装包并在不同机器自动安装；还可以用来实现软件测试中与命令行交互的自动化。 其依赖 pty module ，所以 Pexpect 还不能在 Windows 的标准 python 环境中执行，如果想在 Windows 平台使用，可以使用在 Windows 中运行 Cygwin 做为替代方案。 pexpect主要包含两个接口，一个是run()函数，另一个是spawn类。spawn类的功能很强大，run()函数要更简单，更适用于快速调用程序。 spawn class 使用这个类来开始和控制子程序。 spawn的构造函数 123class spawn: def __init__(self,command,args=[],timeout=30,maxread=2000,\ searchwindowsize=None, logfile=None, cwd=None, env=None) spawn是Pexpect模块主要的类，用以实现启动子程序，它有丰富的方法与子程序交互从而实现用户对子程序的控制。它主要使用 pty.fork() 生成子进程，并调用 exec() 系列函数执行 command 参数的内容。 使用示例： 12child = pexpect.spawn(&apos;/usr/bin/ftp&apos;)child = pexpect.spawn(&apos;/usr/bin/ssh user@example.com&apos;) 由于需要实现不断匹配子程序输出， searchwindowsize 指定了从输入缓冲区中进行模式匹配的位置，默认从开始匹配。 使用pexpect控制子程序 expect()定义 expect(self, pattern, timeout=-1, searchwindowsize=None) 在参数中： pattern 可以是正则表达式， pexpect.EOF ， pexpect.TIMEOUT ，或者由这些元素组成的列表。需要注意的是，当 pattern 的类型是一个列表时，且子程序输出结果中不止一个被匹配成功，则匹配返回的结果是缓冲区中最先出现的那个元素，或者是列表中最左边的元素。使用 timeout 可以指定等待结果的超时时间 ，该时间以秒为单位。当超过预订时间时， expect 匹配到pexpect.TIMEOUT。 expect 不断从读入缓冲区中匹配目标正则表达式，当匹配结束时 pexpect 的 before 成员中保存了缓冲区中匹配成功处之前的内容， pexpect 的 after 成员保存的是缓冲区中与目标正则表达式相匹配的内容。 12print child.beforeprint child.after send系列函数 123send(self, s) sendline(self, s=&apos;&apos;) sendcontrol(self, char) 这些方法用来向子程序发送命令，模拟输入命令的行为。 与 send() 不同的是 sendline() 会额外输入一个回车符 ，更加适合用来模拟对子程序进行输入命令的操作。 当需要模拟发送 “Ctrl+c” 的行为时，还可以使用 sendcontrol() 发送控制字符。 child.sendcontrol(&#39;c&#39;) 由于 send() 系列函数向子程序发送的命令会在终端显示，所以也会在子程序的输入缓冲区中出现，因此不建议使用 expect 匹配最近一次 sendline() 中包含的字符。否则可能会在造成不希望的匹配结果。 interact()定义 interact(self, escape_character = chr(29), input_filter = None, output_filter = None) Pexpect还可以调用interact() 让出控制权，用户可以继续当前的会话控制子程序。用户可以敲入特定的退出字符跳出，其默认值为“^]” 。 run() function run()的定义 12run(command,timeout=-1,withexitstatus=False,events=None,\ extra_args=None,logfile=None, cwd=None, env=None) 函数 run 可以用来运行命令，其作用与 Python os 模块中 system() 函数相似。run() 是通过 Pexpect spawn类实现的。 使用run()执行命令svn命令 12from pexpect import *run (&quot;svn ci -m &apos;automatic commit&apos; my_file.py&quot;) 与 os.system() 不同的是，使用 run() 可以方便地同时获得命令的输出结果与命令的退出状态 。 run()的返回值 12from pexpect import *(command_output, exitstatus) = run (&apos;ls -l /bin&apos;, withexitstatus=1) command_out 中保存的就是 /bin 目录下的内容。 更多关于pexpect的内容请看pexpect. 安装python pexpect模块 sudo pip install pexpect 程序编写还是一样的实现一个程序，其功能是ssh登录虚拟机，并在远程虚拟机用户目录下记录远程登录的日志文件，然后修改其iptables规则，禁止转发tcp 22号端口的报文。 完整程序： 12345678910111213141516171819202122232425262728import pexpectip = &quot;192.168.1.75&quot;user = &quot;chl&quot;passwd = &quot;123456&quot; ssh_newkey = &quot;Y|yes/no&quot;child = pexpect.spawn(&apos;ssh %s@%s&apos; % (user, ip))index = child.expect([pexpect.EOF, pexpect.TIMEOUT, ssh_newkey, &quot;password:&quot;])if index == 1: print &quot;TimeoutError!&quot;if index == 2: child.sendline(&quot;yes&quot;) child.expect(&quot;password&quot;) child.sendline(passwd)if index == 3: child.sendline(passwd)child.expect(&quot;chl@&quot;)child.sendline(&quot;echo &apos;login +1&apos; &gt;&gt; ~/remote_login.log&quot;)child.expect(&quot;chl@&quot;)child.sendline(&quot;sudo iptables -A FORWARD -p tcp --dport 22 -j REJECT&quot;)child.expect(&quot;password&quot;)child.sendline(passwd)child.expect(&quot;chl@&quot;)child.sendline(&quot;exit&quot;) 注：需要注意的是python中正则表达式与Linux中的通配符是有区别的，不能直接用通配符来编写python正则表达式。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>python</tag>
        <tag>Expect</tag>
        <tag>pexpect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程免密码登录Openstack实例]]></title>
    <url>%2F2018%2F07%2F07%2F%E8%BF%9C%E7%A8%8B%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95Openstack%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[背景一般情况下，可以通过Openstack Dashboard的控制台来访问用户创建的实例Instance，对于管理员来说，通过这种方法来访问会觉得很不方便，因为每次都需要打开浏览器来输入网址，每次点击都需要等待响应，登录到实例后控制台的响应也不是很及时且有卡顿。因此，本文介绍如何通过命名空间来实现免密码登录Openstack实例。 命名空间在Linux中，网络命名空间可以被认为是隔离的拥有单独网络栈（网卡、路由转发表、iptables）的环境。网络命名空间经常用来隔离网络设备和服务，只有拥有同样网络命名空间的设备，才能看到彼此。openstack中就采用命名空间来实现不同网络的隔离。 使用ip netns来查看已经存在的命名空间： 123$ ip netnsqrouter-e94975e8-4688-4858-8f64-86a18eea81edqdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 qdhcp开头的名字空间是dhcp服务器使用的，qrouter开头的则是router服务使用的。 查看openstack的网络： 1234567$ openstack network list+--------------------------------------+---------+--------------------------------------+| ID | Name | Subnets |+--------------------------------------+---------+--------------------------------------+| 34ba192f-ceea-4c86-addc-a5d14c6a34a8 | private | 74dbc6f4-ae59-4af5-b941-1f4d04918607 || fc9b502a-d472-46f5-8570-b0d3915759cf | public | de903618-fb31-412a-b46d-6ab593985b03 |+--------------------------------------+---------+--------------------------------------+ 可以看到private网络的dhcp服务器对应的命名空间qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8的名字中包含了private网络的ID。而本次测试的远程实例就是创建在private网络下的。 通过 ip netns exec namespace_id command 来在指定的网络名字空间中执行网络命令，记得加上sudo权限，例如 123456789101112131415161718$ sudo ip netns exec qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 ifconfiglo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:190 errors:0 dropped:0 overruns:0 frame:0 TX packets:190 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:65824 (65.8 KB) TX bytes:65824 (65.8 KB)tap4e2f68bb-84 Link encap:Ethernet HWaddr fa:16:3e:32:19:6b inet addr:10.0.0.2 Bcast:10.0.0.63 Mask:255.255.255.192 inet6 addr: fe80::f816:3eff:fe32:196b/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:110210 errors:0 dropped:0 overruns:0 frame:0 TX packets:107493 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:21335986 (21.3 MB) TX bytes:8426397 (8.4 MB) ssh远程登录实例： sudo ip netns exec namespace_id ssh $username@ip 通过该命令来实现从控制节点通过ssh服务远程访问Openstack实例。 发送公共秘钥要实现免密码远程登录实例，首先需要将控制节点root用户的ssh的公共秘钥发送到远程实例，也就是/root/.ssh/id_rsa.pub文件中的内容，远程实例收到后会将公共密钥保存到登录用户的.ssh/authorized_keys文件中，这样下次登录远程实例时就不再需要密码。 发送公共密钥： 1234567891011$ sudo ip netns exec qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 ssh-copy-id -i /root/.ssh/id_rsa.pub openstack@10.0.0.9/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysopenstack@10.0.0.9&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &apos;openstack@10.0.0.9&apos;&quot;and check to make sure that only the key(s) you wanted were added. 注意：发送的公共秘钥必须是控制节点root用户的，因为在进入命名空间执行命令时需要加上sudo权限，而sudo是用来以其他身份来执行命令的，预设的身份为root，这样在ssh登录远程实例时是以控制节点的root用户来登录远程实例的openstack用户，因此需要将控制节点root用户的公共密钥发送给远程实例，root用户的公共密钥的路径是/root/.ssh/id_rsa.pub。 查看远程实例的authorized_keys： 12$ vi ~/.ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCjXQQUtIcLLcvBXVudZDBbQFK8BT/hB67oOrs792sfCuMhxvxvFRbma5UmnxwxOhXUIRjdz4u7tWhR3VVhqqnlHGDKOQVje/t2QtTlXXcBI3kGnc0Epem2NRMgRKp/h/Y1EOwtPNHRDVfr8C2znilXpWW1ueigHuJF4TWT7vEjgbApmWhopZcOXKbLkSu5dxLGUO3TzGqkASgpLG2XyuUJVqoREr5wbAZytq7R2p5KCxUZ6T7sDUQG+xmFPsfPg3MUHQmatTvtSf+mImotTkNSqOp2Itct9afX7SPkRncrXVWJ0qutbrRrkjRJm1l/sCjFBOD0x6txcFBX30nPvkDx root@controller 查看控制节点root用户公共密钥： 12# vi /root/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCjXQQUtIcLLcvBXVudZDBbQFK8BT/hB67oOrs792sfCuMhxvxvFRbma5UmnxwxOhXUIRjdz4u7tWhR3VVhqqnlHGDKOQVje/t2QtTlXXcBI3kGnc0Epem2NRMgRKp/h/Y1EOwtPNHRDVfr8C2znilXpWW1ueigHuJF4TWT7vEjgbApmWhopZcOXKbLkSu5dxLGUO3TzGqkASgpLG2XyuUJVqoREr5wbAZytq7R2p5KCxUZ6T7sDUQG+xmFPsfPg3MUHQmatTvtSf+mImotTkNSqOp2Itct9afX7SPkRncrXVWJ0qutbrRrkjRJm1l/sCjFBOD0x6txcFBX30nPvkDx root@controller 可以看到，两者是一样的，说明控制节点的root用户已经被授权通过公共密钥来访问远程实例。 免密码登录 免密码登录远程实例： 1$ sudo ip netns exec qdhcp-34ba192f-ceea-4c86-addc-a5d14c6a34a8 ssh openstack@10.0.0.9 这样就通过ssh服务免密码远程登录Openstack实例，而不需要通过Dashboard的控制台来登录实例。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Namespace</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAT网关设计]]></title>
    <url>%2F2018%2F05%2F21%2FNAT%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[目的本文的目的是将Linux配置成NAT网关，具备NAT、DHCP、DNS功能，实现内网访问外网的通信、实现外网对特定主机的访问、实现内网主机的IP地址动态分配以及域名解析的功能。 环境实验环境介绍： 一台搭建了KVM环境的物理服务器，命名为Server，并在该服务器上创建两台虚拟机VM； 第一台虚拟机用作NAT网关，命名为Gateway，分配两张网卡，eth0连接内网，eth1连接外网； 第二台虚拟机作为内网主机，命名为Host，分配一张网卡eth0连接内网。 内网网段为：10.0.0.0/24 外网网段为：192.168.1.0/24 注：本实验其实也可以在VMware workstation或者virtualBox环境下进行，但是由于博主的电脑配置不高，不想在个人电脑上搭建该环境，且身边的服务器刚好有空闲的资源，所以就在服务器上搭建了该环境，至于如何在服务器上搭建KVM环境可以参照我的另一篇博客的部分内容 快速链接。 搭建实验环境： 创建内部网桥：服务器Server上创建一个虚拟网桥br-int，作为连接内网的交换机。 1234567$ sudo brctl addbr br-int$ brctl showbridge name bridge id STP enabled interfacesbr-int 8000.000000000000 no$ sudo ifconfig br-int up 注：在VMware workstation中搭建环境时需要在虚拟网络编辑器中将使用本地DHCP服务将IP分配给虚拟机选项取消勾选，因为我们需要实现NAT网关来给虚拟机分配IP。另外在创建虚拟网桥后一定要开启该网桥。 创建NAT网关虚拟机 其网络配置信息如下： eth0 eth1 通过ifconfig查看网络信息： 1234567891011121314151617$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:1c:83:53 inet6 addr: fe80::5054:ff:fe1c:8353/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...eth1 Link encap:Ethernet HWaddr 52:54:00:de:e8:51 inet addr:192.168.1.123 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::5054:ff:fede:e851/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 ... 可见NAT网关的eth0网卡因为没有DHCP服务器所以没有分配到IP，但外网网卡eth1已经有一个外网IP。 创建内部主机虚拟机 其网络配置信息如下： eth0 通过ifconfig查看网络信息： 1234567891011$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:1a:a1:78 inet6 addr: fe80::5054:ff:fe1a:a178/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 ... 可见其内网网卡eth0没有分配IP。 其实，在启动这两个虚拟机时，因为内部网络没有DHCP服务器，他们的内网网卡会一直等待DHCP服务器给它们分配IP，等待的时间大概是5分钟，最终还是没有获得IP。 修改两个虚拟机的网络配置文件 NAT网关：$ sudo vi /etc/network/interfaces 123456789101112131415source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet staticaddress 10.0.0.1netmask 255.255.255.0gateway 10.0.0.1auto eth1iface eth1 inet dhcp 因为NAT网关的eth0将做内网的网关，故将其IP设置为静态IP：10.0.0.1，这样更容易识别，且不会因为重启而改变。 内部主机：$ sudo vi /etc/network/interfaces 123456789source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet dhcp 在服务器Server上查看虚拟网桥的信息： 1234$ brctl show br-intbridge name bridge id STP enabled interfacesbr-int 8000.fe54001aa178 no vnet1 vnet3 可以看到虚拟网桥br-int上面已经有两个网络接口，对应两个虚拟机的内部网卡。 NAT 功能实现NAT网关的功能是通过Linux中自带的iptables来实现，NAT功能包括SNAT和DNAT。SNAT是源地址转换，在内网主机访问外网时发挥作用，可以将内网主机的ip地址转换为网关的ip地址。DNAT是目的地址转换，在外网的主机通过NAT网关的ip和端口对内网主机发起访问时发挥作用，可以将NAT网关的ip地址与端口转换为对应内网主机的ip，从而实现从外网对内网中某一特定主机的访问。当然要将Linux虚拟机配置成NAT网关，首先得要开启Linux虚拟机的网络转发功能。下面将介绍NAT功能的实现过程： 开启网络转发功能 临时开启网络转发功能，需要切换到root用户，命令如下： # echo 1 &gt; /proc/sys/net/ipv4/ip_forward 这样在下次虚拟机重启后该功能会自动关闭，因此可以修改配置文件的方式来开启该功能并永久生效。 永久开启网络转发功能： # vi /etc/sysctl.conf 在文件里面添加一行net.ipv4.ip_foward=1，在下次重启之后就不会还原了。 SNAT的实现 在iptables的nat表的POSTROUTING规则链中添加规则，使得从10.0.0.0/24网络发到NAT网关的数据包，从eth1转发出去，并将数据包的源ip地址修改为NAT网关的外网地址，命令如下 ： sudo iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -o eth1 -j SNAT --to-source 192.168.1.123 192.168.1.123是NAT网关的外网网卡eth1的IP。 DNAT的实现 在iptables的nat表的PREROUTING规则链中添加规则，使得从外网发往NAT网关固定端口（如：8080）的TCP（这里也可以添加其他网络协议）数据包转发到内网的某固定主机上，命令如下： sudo iptables -t nat -A PREROUTING -i eth1 -d 192.168.1.67 -p tcp --dport 8080 -j DNAT --to-destination 10.0.0.107:22 10.0.0.107是内网某主机的IP，这里说明一下：实际实现的过程中，最好先实现DHCP与DNS的功能。 保存iptables规则 sudo iptables-save | sudo tee /etc/iptables.sav 编辑/etc/rc.local文件，将下面的一行添加到&quot;exit 0&quot;之前： iptables-restore &lt; /etc/iptables.sav 这样每次重启机器时都会自动加载NAT相关的iptables规则。 DHCP与DNS功能实现实现DHCP服务器可以使用isc-dhcp-server工具包，实现DNS服务器可以使用bind9工具包，但是本实验考虑使用DNSmasq工具来同时实现DHCP服务器与DNS服务器的功能。DNSmasq是一个小巧且方便地用于配置DHCP和DNS的工具，适用于小型网络。作为域名解析服务器（DNS），DNSmasq可以通过缓存DNS请求来提高对访问过的网址的连接速度。作为DHCP服务器，DNSmasq可以用于为局域网电脑分配内网ip地址和提供路由。 安装DNSmasq工具： $ sudo apt-get install dnsmasq 编辑DNSmasq的配置文件/etc/dnsmasq.conf，添加下面两行： 12interface=eth0dhcp-range=10.0.0.100,10.0.0.200,72h 其中interface是用作内网网关的网卡，也就是NAT网关的eth0网卡；dhcp-range是动态分配的IP地址池；72h表示分配的IP的有效时间是72个小时，到时间后需要重新分配IP。 重启DNSmasq服务： 12$ sudo /etc/init.d/dnsmasq restart[ ok ] Restarting dnsmasq (via systemctl): dnsmasq.service. 到这里就配置内容就完成了。然后需要重启这两个虚拟机。 测试结果 DHCP服务 在完成DHCP服务器配置重启虚拟机后，会发现在启动过程中不再需要等待5分钟来获取IP了，进入内部主机查看网络信息： 123456789101112131415161718$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:1a:a1:78 inet addr:10.0.0.176 Bcast:10.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::5054:ff:fe1a:a178/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:17 errors:0 dropped:0 overruns:0 frame:0 TX packets:21 errors:0 dropped:0 overruns:0 carrier:0 collisions:36 txqueuelen:1000 RX bytes:1642 (1.6 KB) TX bytes:2182 (2.1 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:164 errors:0 dropped:0 overruns:0 frame:0 TX packets:164 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:12200 (12.2 KB) TX bytes:12200 (12.2 KB) 可以看到eth0分配到了一个在DHCP地址池内的一个IP：10.0.0.176。 DNS与NAT服务 这里将同时对DNS服务与NAT服务进行测试 内网到外网 由于我所在的网络所有的ping外网的报文都会被拦截，所以没有办法通过ping来测试网络的连通性，这里采用wget命令在内部主机测试： 1234567891011$ wget http://www.baidu.com--2018-07-08 16:30:37-- http://www.baidu.com/Resolving www.baidu.com (www.baidu.com)... 180.97.33.108, 180.97.33.107Connecting to www.baidu.com (www.baidu.com)|180.97.33.108|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 2381 (2.3K) [text/html]Saving to: ‘index.html’index.html 100%[================================&gt;] 2.33K --.-KB/s in 0s 2018-07-08 16:30:37 (61.1 MB/s) - ‘index.html’ saved [2381/2381] 上面的测试说明内网主机到外网的连通性，并且也实现了域名解析的功能。 外网到内网 在博主的个人Windows电脑上去访问内网主机，以此来测试外网到内网的连通性： 12345678910111213141516171819202122232425262728293031323334[c:\~]$ ssh openstack-image@192.168.1.152 8080Connecting to 192.168.1.152:8080...Connection established.To escape to local shell, press &apos;Ctrl+Alt+]&apos;.Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage134 packages can be updated.55 updates are security updates.Last login: Sun Jul 8 16:13:54 2018openstack-image@ubuntu:~$ ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:d0:51:38 inet addr:10.0.0.176 Bcast:10.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::5054:ff:fed0:5138/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:96 errors:0 dropped:0 overruns:0 frame:0 TX packets:125 errors:0 dropped:0 overruns:0 carrier:0 collisions:516 txqueuelen:1000 RX bytes:17842 (17.8 KB) TX bytes:15515 (15.5 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:176 errors:0 dropped:0 overruns:0 frame:0 TX packets:176 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:13392 (13.3 KB) TX bytes:13392 (13.3 KB) 可以访问，外网到内网也是连通的。到此，需要实现的功能都已经实现了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>NAT</tag>
        <tag>DHCP</tag>
        <tag>DNS</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟化技术与云计算平台报告]]></title>
    <url>%2F2018%2F03%2F22%2F%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97%E5%B9%B3%E5%8F%B0%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[前言从2006年谷歌首次提出“云计算”的概念到现在，云计算已经经历的十多年的发展，有众多厂商、组织和学者投入其中。一些云服务厂商有自己的云计算平台，但只是为客户提供云服务，外部开发者无法对这些厂商的云平台进行开发。另外也有一些组织机构或厂商开发的云计算平台是开源的，吸引大量开发者在这些开源平台上开展自己的工作。 目前，主流的开源云计算平台有很多种，本问将对这些平台进行调研以选择最适合于自己需求的平台。另外，虚拟化技术又是云计算的核心支撑技术，是将各种计算及存储资源充分整合和高效利用的关键技术，当前虚拟化技术也是多种多样，并且开源云计算平台往往又支持多种底层的虚拟化技术，因此也有必要对虚拟化技术进行调研，以选择最适合云平台和上层应用的虚拟化技术。综上，本报告的主要目标就是对多种虚拟化技术和多种开源云计算平台进行对比，以选出最满足需求的技术与平台。 本文将首先简单对比虚拟化与云计算。其后，由于虚拟化技术是云计算的基础，本报告将先对虚拟化技术展开论述，然后再对云计算平台进行论述。 虚拟化与云计算借助虚拟化技术，用户可以单个物理硬件系统为基础创建多个模拟环境或专用资源。并使用一款名为“Hypervisor”(虚拟机监控程序)的软件直接连接到硬件，从而将一个系统划分为不同的、单独安全环境，即虚拟机 (VM)。Hypervisor 能够将计算机资源与硬件分离并适当分配资源，而虚拟机则依赖这些功能运行。 云计算则由多种规则和方法组合而成，可以跨任何网络向用户按需提供计算、网络和存储基础架构资源、服务、平台和应用。这些基础架构资源、服务和应用来源于云。 简单来讲，云就是一系列管理及自动化软件编排而成的虚拟资源池，旨在帮助用户通过支持自动扩展和动态资源分配的自助服务门户，按需对这些资源进行访问。 下面对虚拟化与云计算做一个简单的对比： 虚拟化 云 定义 技术 方法论 目的 从一个物理硬件系统创建多个虚拟环境 汇聚并自动化分配虚拟资源以供按需使用 用途 针对具体用途为特定用户提供打包资源 针对多种用途为用户群组提供不同资源 使用寿命 数年（长期） 数小时至数月 成本 资本支出（CAPEX）高运营支出（OPEX）低 共有云：CAPEX高、OPEX低私有云：CAPEX低、OPEX高 可扩展性 纵向扩展 横向扩展 工作负载 有状态 无状态 租赁 单一租户 多个租户 虚拟化技术按照虚拟化的对象分类，虚拟化可分为服务器虚拟化、操作系统虚拟化、存储虚拟化、网络虚拟化等。其中服务器虚拟化对CPU、内存、设备与I/O这三种硬件资源的虚拟化技术已经相当成熟，但对GPU的虚拟化却还有很大的提升空间。下面将分别介绍服务器虚拟化中CPU虚拟化及GPU虚拟化相关的技术，然后对现在主流的虚拟化平台做一些比较。 一、CPU虚拟化目前，为了解决x86体系结构下的CPU虚拟化问题，业界提出了全虚拟化和半虚拟化两种不同的软件方案。除了通过软件的方式实现CPU虚拟化外，业界还提出了在硬件层添加支持功能的硬件辅助虚拟化方案来处理那些敏感的高级别指令。 全虚拟化在宿主机底层物理硬件与VM之间增加一个软件层，即虚拟机监控器（VMM或hypervisor），此时，VMM充当主机操作系统，用来管理不同的虚拟机，如图1所示。它隐藏了特定计算平台的实际物理特性，为用户提供抽象的、统一的、模拟的计算环境（称为虚拟机）。在VMM平台上，可以模拟出多套虚拟机，实现了在单机上运行多个不同类型操作系统的虚拟机。全虚拟化的优点是不需要修改客户机操作系统，因此支持多种操作系统，缺点是VMM层工作负荷较大，并占用一定的宿主机资源，性能不如裸机。主要代表有VMware vSphere，Microsoft公司的Virtual PC、Redhat公司的RED HAT ENTERPRISE VIRTUALIZATION等。 半虚拟化与全虚拟化类似，不同之处是需要修改客户机操作系统的核心代码，即增加一个专门的虚拟化应用程序接口，优化客户操作系统发出的指令，与VMM能够协同工作，以减轻VMM和宿主机的负担，进一步提升了虚拟机的性能，如图2所示。缺点是需要修改客户操作系统，影响了技术的普及。主要代表有使用开源虚拟化技术的Citrix的Xenserver、Microsoft的Hyper-V 。 为了更好地实现全、半虚拟化技术，Intel与AMD对传统X86架构进行改进，分别设计了Intel-VT和AMD-V CPU硬件辅助虚拟化技术。将原来的特权等级Ring 0、1、2、3 定义为Non-Root mode，新增了一个Root mode 特权等级（或称为Ring -1），这种情况下，OS 即可运行在原来Ring0 的等级，而VMM 则调整到更底层的Root Mode 等级，其架构如图3。硬件辅助虚拟化有效地解决了虚拟机效率低的问题，它使虚拟机可以运行ring 0 的指令，不用再进行操作系统的ring 切换，提高了虚拟机的整体效率。 现在主流的半、全虚拟化产品都支持硬件辅助虚拟化，代表有Oracle公司的VirtualBox、RHEV、VMware vSphere和Xneserver。 二、GPU虚拟化GPU虚拟化相关技术还垄断在少数厂商手中，并没有像CPU、内存、存储一样在开源社区推广普及。下面将介绍三大显卡厂商GPU虚拟化的发展。 NVIDIA，早在2013年，NVIDIA就推出了行业内第一款GPU虚拟化显卡GRID K1/K2，同期联合Citrix推出了商用的vGPU虚拟桌面解决方案，这比AMD提前了近3年。GPU虚拟化技术的出现，给一直被诟病性能不足的桌面虚拟化带来了转机。在2016年，NVIDIA推出第二款GPU虚拟化显卡，Maxwell架构的Tesla M6/M10/M60，新版的GRID将使用授权分为 3 个不同版本，依据版本不同收取额外软件授权使用费。 在2017年8月份NVIDIA推出了最新版GRID 5.0，虚拟化显卡新增Pascal架构的Tesla P4/P6/P40/P100，其中Quadro Virtual Datacenter Workstation版的授权支持vGPU在图形渲染模式和高性能计算模式之间切换，这是硬件厂商首次在vGPU层面将图形渲染和高性能计算进行了统一，又一次引领了行业趋势和市场需求。NVIDIA的虚拟化显卡只是硬件，还需要相应的服务器虚拟化系统的支持，这和Intel的CPU需要操作系统Windows和Linux来配合一样。现在只有Xen和ESXi能够支持GRID virtual GPU solution，被大量第三方厂商采用的KVM虚拟化平台还没有出现在GRID的支持列表中，因此可以说GPU另外一只脚还没能踏进云计算时代。2017年7月份NVIDIA和Nutanix宣布将合作在年底推出AHV版本的GRID，这算是KVM虚拟化走出了第 一步。在解决GPU虚拟化后，如何将虚拟机的画面传送到客户端，这是KVM虚拟化可以商用的第二步。KVM上默认配置的Spice协议对3D的支持并不好，Nutanix以及其他KVM方案解决商还需自行开发出可用的桌面传输协议，这才算是彻底完成了GRID在KVM上的应用。 AMD，AMD在NVIDIA推出GRID K1/K2 的两年半后才推出了自己的GPU虚拟化产品MxGPU，算是姗姗来迟。共有三款FirePro S系列的GPU支持MxGPU，一块GPU最多可以支持 32 个用户。当MxGPU上的虚拟机比较少时，能够达到图形工作站的性能。随着虚拟机数量增多，每个虚拟机获得的GPU性能逐步降低。有别于NVIDIA GRID通过软件实现的显卡虚拟化方式，AMD MxGPU是“全球首款基于硬件的虚拟化GPU解决方案”。MxGPU每个虚拟机能分得一定数量的独享流处理器和显存空间，这样可以避免不同虚拟机对GPU资源的抢夺，造成用户噪音。这种噪音问题直到今年8月底推出NVIDIA GRID 5. 0 才得到解决。MxGPU在定价上采用的是更符合买方逻辑的营销方法，只向用户收取需付硬件的购买费用。不过遗憾的是，目前只有VMware的ESXi支持MxGPU，Xen暂时只有技术验证版。没能同时支持两种主流化的虚拟化系统，一定程度上阻碍了MxGPU在市场的普及速度。Citrix的用户还是可以用过vSphere+XenDesktop的方案用上MxGPU，相比使用免费的XenServer，要多支付vSphere的费用。 Intel，Intel官方将不同的Intel GPU虚拟化技术分别命名为Intel GVT-s，Intel GVT-d和Intel GVT-g，分布对应API转发，直通，完全虚拟化。Intel GVT-g和NVIDIA vGPU类似，支持Xen/KVM平台，每个GPU最多能分享给7个用户同时使用。其中XenGT在 2016 年最早实现了业界的vGPU在线迁移，NVIDIA GRID直到这个季度才和Citrix合作完成了vGPU在线迁移。 2016 年的 2017 年2月份，Linux 4.10中加入Intel GVT-g for KVM。这是三大GPU厂商中，第一个支持KVM平台的完全虚拟化方案，意味着第三方采用KVM的云计算厂商终于有了一个可用的vGPU方案。不过Intel GPU虚拟化，由于核显性能的原因，只能满足图像密集型的用户体验，不能像GRID vGPU和MxGPU一样满足图形渲染的重度使用场景。Intel GPU虽然支持了大多数虚拟化桌面厂商使用的Xen/KVM两大类服务器虚拟化系统，可是硬件却和VDI高密度的使用场景不太搭。首先Intel的完全虚拟化只支持Broadwell架构以后的核芯显卡，作为VDI服务器中常用的Xeon E5/E7 v4 系列，以及第 一个的Xeon Scalable处理器，都没有核芯显卡。这在很大程度上限定了Intel GPU虚拟化在VDI的使用规模，有种落入有枪无弹的尴尬境地。 三、虚拟化平台比较服务器虚拟化技术日益成熟，并具有广泛的应用前景，目前有很多厂商进行虚拟化技术产品的开发和生产，包括：VMware、Microsoft、Citrix、IBM和RedHat等，其各自产品都有不同的特点，产品功能日益强大。下面将比较一下四种主流服务器虚拟化平台，如下表 ： VMware Xen KVM Hyper-V 厂商 VMware Citrix Red Hat Microsoft 是否免费 付费 开源免费 开源免费 付费 宿主机系统 WindowsLinux NetBSDLinuxSolaris Linux Windows server 2008及以上系统 客户机系统 Windows 2003、Windows 2008、RedHat、Debian、Ubuntu、Centos Xen-PV：纯Linux；Xen-HVM：支持Windows、Linux Linux、Windows Windows系列、Linux 支持技术 硬件辅助虚拟化（全虚拟化） 硬件辅助虚拟化（HVM全虚拟化、PV半虚拟化） 硬件辅助虚拟化（全虚拟化） 硬件辅助虚拟化（半虚拟化） 支持的vGPU产品 NVIDIA GRIDAMD MxGPU NVIDIA GRIDAMD MxGPU（技术验证版） Intel GVT-g for KVM 优点 相对成熟的商业软件，市场占有率较大 性能较好，支持半虚拟化 是内核本身的一部分，因此可以利用内核的优化和改进；高性能，稳定，无需修改客户机系统 对Windows的支持较好 缺点 不开源，费用较高 操作复杂，维护成本较高，目前已被RedHat抛弃 虚拟机性能比Xen略低 对Linux的支持较差，性能损失大 开源云计算平台Openstack对这四种虚拟化平台都有支持，默认使用的是KVM，Openstack与KVM结合的方案也已经相当成熟。另外，考虑到KVM是开源免费的虚拟化技术；宿主机系统支持绝大多数Linux系统，对于使用Linux系统的服务器都有很好的支持；而客户机操作系统不仅支持Linux，还支持Windows，可以满足绝大多数用户的需求，因此选择KVM作为Openstack底层的虚拟化技术的理由是很充分的。不过，KVM对于虚拟GPU的支持不是很好，只有Intel GVT-g for KVM可以支持KVM平台的全虚拟化方案，但是Intel GPU虚拟化由于核显性能的原因，只能满足图像密集型的用户体验，不能满足图形渲染等重度使用的场景。 云计算平台目前已经有多个云计算平台的开源实现，主要的开源云计算项目有Openstack、Eucalyptus、CloudStack和OpenNebula等，现比较如下： Openstack Eucalyptus CloudStack OpenNebula 发布时间 2010年7月 2008年5月 2010年5月 2008年7月 最新版本 Queens 4.4 4.11 5.4 授权协议 Apache v2.0 GPL v3.0 Apache v2.0 Apache v2.0 基本架构 Nova、Glance、Neutron、Keystone、Horizon、swift、Tacker等 Cloud Controller、Cluster Controller、Node Controller、Walrus、 Storage Controller 主要包括管理服务、云基础设施和网络三大部分 主要包括接口与API、用户与组、主机、网络、存储、集群6个部分 虚拟化技术支持 KVM、LXC、QEMU、UML、Vmware ESX/ESXi、Xen、Hyper-V Xen、KVM、ESXi KVM、Xen、ESXi、OVM、Baremetal Xen、KVM、Vmware 用户界面 Dashboard，较简单 web界面 Web Console，功能较完善 web界面 社区活跃程度 人数多，活跃用户数最多 人数多，但活跃用户数较少 人数少，但活跃用户数较多 人数较少，活跃用户数也少 兼容云平台 Amazon EC2，S3 Amazon EC2，S3 Amazon EC2，S3 Amazon EC2，S3 开发主导 开源社区 Eucalyptus System Inc Citrix公司 开源社区 主要支持厂商 160家左右，包括NASA、Rackspace、HP、Dell、UnitedStack等 亚马逊、戴尔、惠普、Intel、Redhat、Vmware等 不到60家，包括诺基亚、日本电话电报公司、阿尔卡特、迪士尼等 IBM、Akamai、Blackberry、Fuze、Telefonica、Indigital 官方文档 非常详细 不够详细 详细 详细 检测和审计 Telemetry Service Accounting system Event/Audit logs Accounting system、periodically-Monitoring 数据库 PostareSQL、MySQL、SQLite HyperSQL Database MySQL SQLite、MySQL 部署 私有云、共有云、混合云 私有云、混合云 私有云、共有云、混合云 私有云、共有云、混合云 操作系统 Debian 7.0、openSUSE、SUSE、Red Hat、CentOS、Fedora、Ubuntu CentOS、RHEL CentOS、RHEL6.3+、Ubuntu Red Hat、Ubuntu、SUSE、CentOS、Debian 开发语言 Python Java、C/C++ Java C、Ruby、shell 开源市场部署比例 69% 3% 14% 无统计数据 这四种主流的开源云计算平台都经过了近十年的发展，更新迭代了很多版本，能够从众多云计算平台的竞争中存活下来，都有相应的支持厂商和用户，说明它们各有各的特点，如在开发语言上就各有特色，Openstack使用的是Python语言，Eucalyptus使用Java、C/C++，CloudStack仅使用Java，而OpenNebula却显得比较奇怪，使用的是C语言、Ruby和shell，多种语言混杂而成。但不同平台之间还是有较大差距，从结果来看，在开源市场的部署比例，Openstack 69%的比例占据了绝对统治地位，Openstack能占据这样的地位有多方面的原因，如Openstack支持绝大多数的虚拟化技术，支持的操作系统也很多，使得Openstack具有广阔的应用范围；Openstack的官方文档非常详细，也降低了其学习成本；Openstack具有一个充满活力的开源社区，开发者不断为Openstack的发展作出贡献；OpenStack的支持厂商有160家左右，有如此多的厂商支持，给OpenStack的发展提供了根本保障。综合以上多方面的原因，本项目采用Openstack作为底层的云计算平台为上层提供基础设施资源也是理所当然的。另外，在2018年2月28日发布的Openstack Queens最新版本中，新引入的Marquee功能正是为了提供对vGPU的内置支持能力，这意味着用户能够将GPU添加至虚拟机中，为本项目的上层应用，如深度学习等需要强大GPU运算能力的应用提供了支持。 总结通过以上的分析，能够清楚的了解各种虚拟化技术及各种云计算平台的差异，对于要选择满足自己需求的技术与平台会有一些帮助。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>云计算</tag>
        <tag>虚拟化</tag>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack环境下手动安装Mistral]]></title>
    <url>%2F2017%2F12%2F23%2FOpenstack%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Mistral%2F</url>
    <content type="text"><![CDATA[概述在openstack平台中能够成功安装Tacker，但是安装的Tacker并不能用，因为在tacker中创建VIM时需要调用Mistral工作流组件。因此本文就来介绍在openstack环境中手动安装Mistral的过程。 注： 安装的openstack是在Ubuntu 16.04系统下的Ocata版本；本文中涉及的密码都统一设置成 “openstack”。 参考官方文档 一、安装必要组件12$ apt-get install python-dev python-setuptools python-pip libffi-dev \ libxslt1-dev libxml2-dev libyaml-dev libssl-dev 二、安装Mistral server1、下载Mistral源码，并进入下载目录$ git clone https://github.com/openstack/mistral.git $ cd mistral 2、安装Mistral环境依赖包$ pip install -r requirements.txt 3、安装Mistral$ python setup.py install 4、生成配置文件$ oslo-config-generator --config-file tools/config/config-generator.mistral.conf --output-file etc/mistral.conf 5、创建Mistral日志文件和配置文件夹# mkdir -p /etc/mistral /var/log/mistral 6、复制配置文件到配置文件夹# cp etc/* /etc/mistral/ 7、修改配置文件123456789101112131415# vi /etc/mistral/mistral.conf [keystone_authtoken]auth_uri = http://controller:5000auth_version = 3identity_uri = http://controller:35357/admin_user = adminadmin_password = openstackadmin_tenant_name = admin[database]connection = mysql+pymysql://mistral:openstack@controller/mistral [DEFAULT]transport_url = rabbit://openstack:openstack@controller 8、创建数据库123456# mysqlMariaDB [(none)]&gt; CREATE DATABASE mistral;MariaDB [mistral]&gt; GRANT ALL PRIVILEGES ON mistral.* TO &apos;mistral&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [mistral]&gt; GRANT ALL PRIVILEGES ON mistral.* TO &apos;mistral&apos;@&apos;%&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [mistral]&gt; flush privileges;MariaDB [mistral]&gt; exit; 9、创建服务和endpoint1234$ openstack service create --name mistral --description &quot;Openstack Workflow service&quot; workflow$ openstack endpoint create --region RegionOne workflow public http://controller:8989/v2$ openstack endpoint create --region RegionOne workflow internal http://controller:8989/v2$ openstack endpoint create --region RegionOne workflow admin http://controller:8989/v2 10、初始化数据库信息12345678910111213141516171819202122232425root@controller:/home/openstack# mistral-db-manage --config-file /etc/mistral/mistral.conf upgrade headINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; 001, Kilo releaseINFO [alembic.runtime.migration] Running upgrade 001 -&gt; 002, KiloINFO [alembic.runtime.migration] Running upgrade 002 -&gt; 003, cron_trigger_constraintsINFO [alembic.runtime.migration] Running upgrade 003 -&gt; 004, add description for executionINFO [alembic.runtime.migration] Running upgrade 004 -&gt; 005, Increase executions_v2 column size from JsonDictType to JsonLongDictTypeINFO [alembic.runtime.migration] Running upgrade 005 -&gt; 006, add a Boolean column &apos;processed&apos; to the table delayed_calls_v2INFO [alembic.runtime.migration] Running upgrade 006 -&gt; 007, Move system flag to base definitionINFO [alembic.runtime.migration] Running upgrade 007 -&gt; 008, Increase size of state_info column from String to TextINFO [alembic.runtime.migration] Running upgrade 008 -&gt; 009, Add database indicesINFO [alembic.runtime.migration] Running upgrade 009 -&gt; 010, add_resource_members_v2_tableINFO [alembic.runtime.migration] Running upgrade 010 -&gt; 011, add workflow id for executionINFO [alembic.runtime.migration] Running upgrade 011 -&gt; 012, add event triggers tableINFO [alembic.runtime.migration] Running upgrade 012 -&gt; 013, split_execution_table_increase_namesINFO [alembic.runtime.migration] Running upgrade 013 -&gt; 014, fix_past_scripts_discrepanciesINFO [alembic.runtime.migration] Running upgrade 014 -&gt; 015, add_unique_keys_for_non_locking_modelINFO [alembic.runtime.migration] Running upgrade 015 -&gt; 016, Increase size of task_executions_v2.unique_keyINFO [alembic.runtime.migration] Running upgrade 016 -&gt; 017, Add named lock tableINFO [alembic.runtime.migration] Running upgrade 017 -&gt; 018, increate_task_execution_unique_key_sizeINFO [alembic.runtime.migration] Running upgrade 018 -&gt; 019, Change scheduler schema.INFO [alembic.runtime.migration] Running upgrade 019 -&gt; 020, add type to task executionINFO [alembic.runtime.migration] Running upgrade 020 -&gt; 021, Increase environments_v2 column size from JsonDictType to JsonLongDictType 11、添加自带的action123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172root@controller:/home/openstack# mistral-db-manage --config-file /etc/mistral/mistral.conf populate*输出结果可能为：*No handlers could be found for logger &quot;mistral.actions.openstack.action_generator.base&quot;*也可能会出错：*……2017-12-22 22:21:24.486 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.551 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.761 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.878 16802 INFO mistral.actions.openstack.action_generator.base [-] Processing OpenStack action mapping from file: /usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/mapping.json2017-12-22 22:21:24.883 16802 WARNING oslo_config.cfg [-] Option &quot;auth_uri&quot; from group &quot;keystone_authtoken&quot; is deprecated for removal (The auth_uri option is deprecated in favor of www_authenticate_uri and will be removed in the S release.). Its value may be silently ignored in the future.^C2017-12-22 22:21:25.382 16802 CRITICAL Mistral [-] Unhandled error: KeyboardInterrupt2017-12-22 22:21:25.382 16802 ERROR Mistral Traceback (most recent call last):2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/bin/mistral-db-manage&quot;, line 10, in &lt;module&gt;2017-12-22 22:21:25.382 16802 ERROR Mistral sys.exit(main())2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/db/sqlalchemy/migration/cli.py&quot;, line 137, in main2017-12-22 22:21:25.382 16802 ERROR Mistral CONF.command.func(config, CONF.command.name)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/db/sqlalchemy/migration/cli.py&quot;, line 75, in do_populate2017-12-22 22:21:25.382 16802 ERROR Mistral action_manager.sync_db()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/services/action_manager.py&quot;, line 80, in sync_db2017-12-22 22:21:25.382 16802 ERROR Mistral register_action_classes()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/services/action_manager.py&quot;, line 126, in register_action_classes2017-12-22 22:21:25.382 16802 ERROR Mistral _register_dynamic_action_classes()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/services/action_manager.py&quot;, line 86, in _register_dynamic_action_classes2017-12-22 22:21:25.382 16802 ERROR Mistral actions = generator.create_actions()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/action_generator/base.py&quot;, line 143, in create_actions2017-12-22 22:21:25.382 16802 ERROR Mistral client_method = class_.get_fake_client_method()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/base.py&quot;, line 75, in get_fake_client_method2017-12-22 22:21:25.382 16802 ERROR Mistral return cls._get_client_method(cls._get_fake_client())2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/mistral/actions/openstack/actions.py&quot;, line 380, in _get_fake_client2017-12-22 22:21:25.382 16802 ERROR Mistral return cls._get_client_class()(session=sess)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/ironic_inspector_client/v1.py&quot;, line 88, in __init__2017-12-22 22:21:25.382 16802 ERROR Mistral super(ClientV1, self).__init__(**kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/ironic_inspector_client/common/http.py&quot;, line 134, in __init__2017-12-22 22:21:25.382 16802 ERROR Mistral region_name=region_name)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 856, in get_endpoint2017-12-22 22:21:25.382 16802 ERROR Mistral return auth.get_endpoint(self, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/base.py&quot;, line 212, in get_endpoint2017-12-22 22:21:25.382 16802 ERROR Mistral service_catalog = self.get_access(session).service_catalog2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/base.py&quot;, line 136, in get_access2017-12-22 22:21:25.382 16802 ERROR Mistral self.auth_ref = self.get_auth_ref(session)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/generic/base.py&quot;, line 198, in get_auth_ref2017-12-22 22:21:25.382 16802 ERROR Mistral return self._plugin.get_auth_ref(session, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/identity/v3/base.py&quot;, line 167, in get_auth_ref2017-12-22 22:21:25.382 16802 ERROR Mistral authenticated=False, log=False, **rkwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 766, in post2017-12-22 22:21:25.382 16802 ERROR Mistral return self.request(url, &apos;POST&apos;, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/positional/__init__.py&quot;, line 101, in inner2017-12-22 22:21:25.382 16802 ERROR Mistral return wrapped(*args, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 616, in request2017-12-22 22:21:25.382 16802 ERROR Mistral resp = send(**kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/dist-packages/keystoneauth1/session.py&quot;, line 674, in _send_request2017-12-22 22:21:25.382 16802 ERROR Mistral resp = self.session.request(method, url, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/requests/sessions.py&quot;, line 508, in request2017-12-22 22:21:25.382 16802 ERROR Mistral resp = self.send(prep, **send_kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/requests/sessions.py&quot;, line 618, in send2017-12-22 22:21:25.382 16802 ERROR Mistral r = adapter.send(request, **kwargs)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/requests/adapters.py&quot;, line 440, in send2017-12-22 22:21:25.382 16802 ERROR Mistral timeout=timeout2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py&quot;, line 601, in urlopen2017-12-22 22:21:25.382 16802 ERROR Mistral chunked=chunked)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py&quot;, line 380, in _make_request2017-12-22 22:21:25.382 16802 ERROR Mistral httplib_response = conn.getresponse(buffering=True)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/httplib.py&quot;, line 1136, in getresponse2017-12-22 22:21:25.382 16802 ERROR Mistral response.begin()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/httplib.py&quot;, line 453, in begin2017-12-22 22:21:25.382 16802 ERROR Mistral version, status, reason = self._read_status()2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/httplib.py&quot;, line 409, in _read_status2017-12-22 22:21:25.382 16802 ERROR Mistral line = self.fp.readline(_MAXLINE + 1)2017-12-22 22:21:25.382 16802 ERROR Mistral File &quot;/usr/lib/python2.7/socket.py&quot;, line 480, in readline2017-12-22 22:21:25.382 16802 ERROR Mistral data = self._sock.recv(self._rbufsize)2017-12-22 22:21:25.382 16802 ERROR Mistral KeyboardInterrupt2017-12-22 22:21:25.382 16802 ERROR Mistral 纠结了很久后发现这些都不用太在意，直接跳过，哈哈！ 三、安装Mistral client1、下载Mistral-client源码$ git clone git://git.openstack.org/openstack/python-mistralclient.git -b stable/ocata $ cd python-mistralclient 2、安装Mistral-client模块$ pip install -r requirements.txt $ python setup.py install 四、安装Mistral horizon1、下载Mistral-horizon源码$ git clone https://git.openstack.org/openstack/mistral-dashboard.git -b stable/ocata $ cd mistral-dashboard/ 2、安装Mistral-horizon模块$ pip install -r requirements.txt $ python setup.py install 3、复制一个文件# cp -b mistraldashboard/enabled/_50_mistral.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/_50_mistral.py 4、重启apache2服务# service apache2 restart 安装好Mistral-horizon后，admin用户登录dashboard界面就可以看到Mistral相关的workflow，如图： 五、运行Mistral server运行下面的第一条命令：12345678910111213141516171819202122232425262728root@controller:/home/openstack/mistral# python mistral/cmd/launch.py --server all --config-file /etc/mistral/mistral.conf|\\ //| || ||||\\ //|| __ || __ __ |||| \\// || || // |||||| || \\ // \\ |||| \/ || \\ || || || \\ |||| || || \\ || || || /\\ |||| || || __// ||_// || \\__// \\_ ||Mistral Workflow Service, version 6.0.0Launching server components [engine,event-engine,api,executor]...2017-12-22 22:42:58.373 16966 INFO mistral.event_engine.default_event_engine [-] Starting event notification task...2017-12-22 22:42:58.571 16966 INFO mistral.event_engine.default_event_engine [-] Found 0 event triggers./usr/local/lib/python2.7/dist-packages/oslo_messaging/server.py:341: FutureWarning: blocking executor is deprecated. Executor default will be removed. Use explicitly threading or eventlet instead in version &apos;pike&apos; and will be removed in version &apos;rocky&apos; category=FutureWarning)2017-12-22 22:42:58.913 16966 WARNING oslo_config.cfg [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Option &quot;auth_uri&quot; from group &quot;keystone_authtoken&quot; is deprecated for removal (The auth_uri option is deprecated in favor of www_authenticate_uri and will be removed in the S release.). Its value may be silently ignored in the future.2017-12-22 22:42:58.915 16966 WARNING oslo_config.cfg [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Option &quot;auth_uri&quot; from group &quot;keystone_authtoken&quot; is deprecated. Use option &quot;www_authenticate_uri&quot; from group &quot;keystone_authtoken&quot;.2017-12-22 22:42:58.925 16966 WARNING keystonemiddleware.auth_token [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] AuthToken middleware is set with keystone_authtoken.service_token_roles_required set to False. This is backwards compatible but deprecated behaviour. Please set this to True.2017-12-22 22:42:58.926 16966 WARNING keystonemiddleware.auth_token [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Use of the auth_admin_prefix, auth_host, auth_port, auth_protocol, identity_uri, admin_token, admin_user, admin_password, and admin_tenant_name configuration options was deprecated in the Mitaka release in favor of an auth_plugin and its related options. This class may be removed in a future release.2017-12-22 22:42:58.931 16966 INFO oslo.service.wsgi [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] mistral_api listening on 0.0.0.0:89892017-12-22 22:42:58.932 16966 INFO oslo_service.service [req-46885c1b-de53-4bba-958d-97484cd17783 - - - - -] Starting 4 workersAPI server started.API server started.API server started.API server started.Event engine server started.Executor server started.Engine server started. 六、测试一下Mistral是否可用12345678910111213openstack@controller:~/mistral/etc$ mistral workbook-list+--------+--------+------------+------------+| Name | Tags | Created at | Updated at |+--------+--------+------------+------------+| &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; |+--------+--------+------------+------------+openstack@controller:~/mistral/etc$ mistral action-list+--------+--------+-----------+--------+-------------+--------+------------+------------+| ID | Name | Is system | Input | Description | Tags | Created at | Updated at |+--------+--------+-----------+--------+-------------+--------+------------+------------+| &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; | &lt;none&gt; |+--------+--------+-----------+--------+-------------+--------+------------+------------+ OK，成功了，开心！！！]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Mistral</tag>
        <tag>Tacker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack环境下手动安装Tacker]]></title>
    <url>%2F2017%2F12%2F23%2FOpenstack%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Tacker%2F</url>
    <content type="text"><![CDATA[概述本文参考官方文档，在现有的openstack平台上，手动安装Tacker。基础的openstack平台包含了最核心的keystone、glance、nova、neutron、horizon这5个组件，但是Tacker还需要预先安装好Mistral和Barbican这两个组件，在安装好这两个组件后就可以开始按照以下步骤安装Tacker了。参考：官方文档链接 注： 本文涉及到的密码都统一设置成openstack。 一、安装Tacker server1、创建数据库12345# mysqlMariaDB [(none)]&gt; CREATE DATABASE tacker;MariaDB [tacker]&gt; GRANT ALL PRIVILEGES ON tacker.* TO &apos;tacker&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [tacker]&gt; GRANT ALL PRIVILEGES ON tacker.* TO &apos;tacker&apos;@&apos;%&apos; IDENTIFIED BY &apos;openstack&apos;;MariaDB [tacker]&gt; exit; 2、创建user、role、endpoints1)获得admin凭证# . admin-openrc 2)创建tacker用户，密码为openstack# openstack user create --domain default --password openstack tacker 3)给tacker用户赋予admin权限# openstack role add --project service --user tacker admin 4)创建tacker服务# openstack service create --name tacker \ --description &quot;Tacker Project&quot; nfv-orchestration 5)创建endpoints123456# openstack endpoint create --region RegionOne nfv-orchestration \ public http://controller:9890/# openstack endpoint create --region RegionOne nfv-orchestration \ internal http://controller:9890/# openstack endpoint create --region RegionOne nfv-orchestration \ admin http://controller:9890/ 3、下载Tacker源码# git clone https://github.com/openstack/tacker -b stable/ocata 4、安装Tacker环境依赖包# cd tacker # pip install -r requirements.txt 5、安装Tacker# python setup.py install 6、创建Tacker日志文件夹# mkdir -p /var/log/tacker 7、生成配置文件# ./tools/generate_config_file_sample.sh 这时生成的配置文件在etc/tacker/tacker.conf.sample，需要将其重命名为tacker.conf # mv etc/tacker/tacker.conf.sample etc/tacker/tacker.conf 8、修改配置文件1234567891011121314151617181920212223242526272829303132333435363738# vi etc/tacker/tacker.conf[DEFAULT]auth_strategy = keystonepolicy_file = /usr/local/etc/tacker/policy.jsondebug = Trueuse_syslog = Falsebind_host = 10.0.0.11bind_port = 9890service_plugins = nfvo,vnfmstate_path = /var/lib/tacker...[nfvo]vim_drivers = openstack[keystone_authtoken]memcached_servers = 11211region_name = RegionOneauth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultusername = tackerproject_name = servicepassword = openstackauth_url = http://controller:35357auth_uri = http://controller:5000...[agent]root_helper = sudo /usr/local/bin/tacker-rootwrap /usr/local/etc/tacker/rootwrap.conf[database]connection = mysql://tacker:openstack@controller:3306/tacker?charset=utf8[tacker]monitor_driver = ping,http_ping 9、复制配置文件到配置文件夹# cp etc/tacker/tacker.conf /usr/local/etc/tacker/ 10、初始化数据库信息# /usr/local/bin/tacker-db-manage --config-file /usr/local/etc/tacker/tacker.conf upgrade head 二、安装Tacker client1、下载Tacker-client源码# git clone https://github.com/openstack/python-tackerclient -b stable/ocata 2、安装Tacker-client模块# cd python-tackerclient # python setup.py install 三、安装Tacker horizon1、下载Tacker-horizon源码# git clone https://github.com/openstack/tacker-horizon -b stable/ocata 2、安装Tacker-horizon模块# cd tacker-horizon # python setup.py install 安装好tacker-horizon后，admin用户登录dashboard界面就可以看到Tacker相关的VNFM和NFVO，如图： 四、开启Tacker server打开一个新的终端，开启Tacker-server，因为Tacker-server的程序会独占这个终端。123sudo python /usr/local/bin/tacker-server \ --config-file /usr/local/etc/tacker/tacker.conf \ --log-file /var/log/tacker/tacker.log 需注意的一个问题： 在安装完Tacker而没有装Mistral时创建VIM的结果如下：1234root@controller:/home/openstack# tacker vim-register --is-default --config-file config.yaml test_vimThe resource could not be found.或者是这种错误：Expecting to find domain in project. The server could not comply with the request since it is either malformed or otherwise incorrect. The client is assumed to be in error. 经过查阅资料，知道这个问题是因为Tacker在创建VIM时要调用Mistral而造成的。所以在使用tacker之前需要先安装好Mistral（可以在安装tacker前安装Mistral，也可以在tacker安装之后安装Mistral，后续还需继续了解）。]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Tacker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM虚拟机部署openstack的网络配置]]></title>
    <url>%2F2017%2F12%2F03%2FKVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%83%A8%E7%BD%B2openstack%E7%9A%84%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[概述这篇文章记录的是按照官方文档在KVM环境下部署双节点openstack过程中，前期准备KVM环境和网络配置相关的内容，在完成这篇博客涉及到的工作之后就可以按照官方文档手动安装openstack了。本文涉及的主要工作，首先是在服务器的ubuntu 16.04 desktop版系统上搭建kvm环境，然后在服务器上安装VNC远程桌面，最后在KVM环境中开启两台虚拟机，分别两张网卡，第一张网卡使用桥接模式，第二张网卡使用NAT模式。下面开始介绍一下这个过程。 服务器搭建KVM环境查看CPU是否支持KVM$ egrep -c &quot;(svm|vmx)&quot; /proc/cpuinfo 输出结果大于0证明CPU支持KVM虚拟化 安装KVM及相关依赖包$ sudo apt-get install qemu-kvm qemu virt-manager virt-viewer libvirt-bin bridge-utils 启用桥接网络在服务器上启用桥接网络需要配置一个桥接设备br0，配置br0有两种方式，通过手动配置和通过修改文件配置。 通过手动配置 创建br0网桥 # brctl addbr br0 将eth0网卡添加到br0上，此时可能会断网 # brctl addif br0 eth0 删除eth0上的IP地址 # ip addr del dev eth0 192.168.1.25/24 配置br0的IP地址并启动br0网桥设备 # ifconfig br0 192.168.1.25/24 up 重新加入默认网关 # route add default gw 192.168.1.1 查看配置是否生效 12345# route //查看默认网关，输出结果如下Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 192.168.1.1 0.0.0.0 UG 0 0 0 br0192.168.1.0 * 255.255.255.0 U 0 0 0 br0 123456789101112131415161718192021222324252627# ifconfig //查看eth0和br0的IP信息，输出结果如下，可以发现现在br0有IP而eth0没有IP了br0 Link encap:Ethernet HWaddr 00:e0:81:e2:3c:3d inet addr:192.168.1.25 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::2e0:81ff:fee2:3c3d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1316822 errors:0 dropped:5787 overruns:0 frame:0 TX packets:365475 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:581279124 (581.2 MB) TX bytes:562586852 (562.5 MB) eth0 Link encap:Ethernet HWaddr 00:e0:81:e2:3c:3d inet6 addr: fe80::2e0:81ff:fee2:3c3d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6671034 errors:0 dropped:9627 overruns:0 frame:0 TX packets:840972 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1346523816 (1.3 GB) TX bytes:614510541 (614.5 MB) Memory:dfb80000-dfbfffff lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:1450290 errors:0 dropped:0 overruns:0 frame:0 TX packets:1450290 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:24027042487 (24.0 GB) TX bytes:24027042487 (24.0 GB) 这就是通过手动来配置桥接设备br0的方法，这种方法在配置好之后马上就生效了，但是在系统重启之后这些配置信息都会被清除，要想使配置永久生效则需要修改网络配置文件，也就是下面的方法。 通过修改文件配置 修改前先将网络配置文件进行备份 # cp /etc/network/interfaces /etc/network/interfaces.bak 修改网络配置文件/etc/network/interfaces # vi /etc/network/interfaces //修改结果如下 1234567891011121314auto loiface lo inet loopback # Enabing Bridge networking br0 interfaceauto br0iface br0 inet staticaddress 192.168.1.25network 192.168.1.0netmask 255.255.255.0broadcast 192.168.1.255gateway 192.168.1.1dns-nameservers 223.5.5.5bridge_ports eth0bridge_stp off 保存后退出，关机重启中配置文件就生效了。这种方法只需要修改配置文件然后重启就可以，比较简单，而且是永久生效，比较符合我们的需求，因为我们的虚拟机通过桥接模式连接外网的话都是连接到br0上的。 修改virbr0的网段在服务器上安装好虚拟化软件后，KVM会自动生成一个virbr0的桥接设备，它的作用是为连接其上的虚拟网卡提供NAT访问外网的功能，并提供DHCP服务。virbr0默认分配的IP是192.168.122.1，使用 ifconfig 命令查看得virbr0的信息如下： 12345678910$ ifconfig…… virbr0 Link encap:Ethernet HWaddr 52:54:00:f8:70:e3 inet addr:192.168.122.1 Bcast:192.168.122.255 Mask:255.255.255.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 在这种情况下，连接到virbr0上的虚拟机的虚拟网卡也是在192.168.122.0网段上的，如果让连接到virbr0上的虚拟网卡在自定义的网段上就需要修改virbr0的网段，修改方法如下： # virsh net-edit default 123456789101112&lt;network&gt; &lt;name&gt;default&lt;/name&gt; &lt;uuid&gt;91cc230a-bf53-487c-b296-10323705d7e8&lt;/uuid&gt; &lt;forward mode=&apos;nat&apos;/&gt; &lt;bridge name=&apos;virbr0&apos; stp=&apos;on&apos; delay=&apos;0&apos;/&gt; &lt;mac address=&apos;52:54:00:f8:70:e3&apos;/&gt; &lt;ip address=&apos;10.0.0.1&apos; netmask=&apos;255.255.255.0&apos;&gt; &lt;dhcp&gt; &lt;range start=&apos;10.0.0.2&apos; end=&apos;10.0.0.254&apos;/&gt; &lt;/dhcp&gt; &lt;/ip&gt;&lt;/network&gt; 这样就将virbr0的网段改成10.0.0.0/24，连接到virbr0的虚拟网卡的IP将会在10.0.0.2/24 - 10.0.0.254/24范围内自动分配一个。如果有需要可以自己手动给虚拟网卡配置IP并写到配置文件中去。 服务器安装VNC远程桌面因为服务器上安装的Ubuntu 16.04 LTS desktop版的系统，在后续的工作中需要远程登录到服务器，虽然可以通过SSH远程管理服务器，但是可视化的界面往往会给新手用户提供很大的便利，所以可以在服务器上安装VNC。开始在服务器上安装VNC试过很多方法，VNC服务器端也有多种选择，如VNC4server、tigervncserver，感觉很麻烦，而且还不一定能安装成功，我安装的VNC服务器端是x11VNC，按照步骤可以很顺利地完成安装，步骤如下： 安装x11VNC软件包$ sudo apt-get install x11vnc 配置访问密码$ sudo x11vnc -storepasswd /etc/x11vnc.pass 创建服务# vi /lib/systemd/system/x11vnc.service //粘贴一下代码，最后:wq 保存，请使用root用户，否则没有权限 12345678[Unit]Description=Start x11vnc at startup.After=multi-user.target[Service]Type=simpleExecStart=/usr/bin/x11vnc -auth guess -forever -loop -noxdamage -repeat -rfbauth /etc/x11vnc.pass -rfbport 5900 -shared[Install]WantedBy=multi-user.target 配置防火墙，配置和启动服务# ufw allow 5900 # systemctl enable x11vnc.service # systemctl daemon-reload # systemctl start x11vnc.service 完成这四个步骤然后重启就可以了。（这个VNC的安装过程可以参考http://blog.csdn.net/longhr/article/details/51657610） 最后在你自己的电脑需要有一个vnc viewer的软件，可以在这里下载（链接：https://pan.baidu.com/s/1o8kPqXG 密码：v5r2） 创建VM并配置相关信息在安装好VNC后就可以登录服务器的远程桌面，打开一个terminal，在terminal中输入下面的命令可以打开Virtual Machine Manager（注意，使用SSH远程登录服务器是无法打开virt-manager的界面的，一定要在登录了远程桌面后才能打开界面） 使用Virtual Machine Manager的界面可以很方便的创建虚拟机，当然也可以在命令行中使用命令创建虚拟机，这个我就不在这里说了。 按照Openstack官网安装文档的主机网络配置两台虚拟机Controller和Compute 控制节点和计算节点这两个虚拟机分别两张网卡，一张配置为桥接模式，另一张配置为NAT模式。创建虚拟机时默认是添加一张网卡的，后面可以在虚拟机的硬件信息中添加。两张虚拟网卡的配置信息如图： 上图显示的是桥接模式网卡的配置信息，Network source选择为Bridge br0：Host device eth0 上图显示的是NAT模式网卡的配置信息，Network source选择为Virtual network ‘default’：NAT 这样按照官方文档部署双节点Openstack的前期准备工作就已经做完，后面就可以按照官方文档开始安装openstack了，祝你成功。附上官方文档链接https://docs.openstack.org/ocata/zh_CN/install-guide-ubuntu/index.html （注：这个是在ubuntu系统下安装Ocata版本Openstack中文文档）]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>KVM</tag>
        <tag>vnc</tag>
        <tag>ubuntu</tag>
        <tag>Virtual Machine</tag>
      </tags>
  </entry>
</search>
